2023-02-26 23:56:16 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 1,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'imgaug',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\Pc-Ryzen\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_101.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_101',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': 'D:/DLC/CV-Mena-2023-02-26',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-27 00:00:34 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 1,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'imgaug',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\Pc-Ryzen\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_101.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_101',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': 'D:/DLC/CV-Mena-2023-02-26',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-27 00:02:46 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 1,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'imgaug',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\Pc-Ryzen\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': 'D:/DLC/CV-Mena-2023-02-26',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-27 00:25:10 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 1,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'imgaug',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\Pc-Ryzen\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': 'D:/DLC/CV-Mena-2023-02-26',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-27 00:46:28 Config:
{'Task': None,
 'TrainingFraction': None,
 'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'alphavalue': None,
 'apply_prob': 0.5,
 'batch_size': 1,
 'bodyparts': None,
 'colormap': None,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'corner2move2': None,
 'crop_pad': 0,
 'cropping': None,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'imgaug',
 'date': None,
 'decay_steps': 30000,
 'default_augmenter': None,
 'default_net_type': None,
 'deterministic': False,
 'display_iters': 1000,
 'dotsize': None,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'identity': None,
 'init_weights': 'C:\\Users\\Pc-Ryzen\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'iteration': None,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'move2corner': None,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'multianimalproject': None,
 'net_type': 'resnet_50',
 'num_joints': 17,
 'numframes2pick': None,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pcutoff': None,
 'pos_dist_thresh': 17,
 'project_path': 'D:/DLC/CV-Mena-2023-02-26/dlc-models/iteration-0/CVFeb26-trainset95shuffle1/train',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'scorer': None,
 'shuffle': True,
 'skeleton': [],
 'skeleton_color': 'black',
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'snapshotindex': None,
 'start': None,
 'stop': None,
 'stride': 8.0,
 'video_sets': None,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001,
 'x1': None,
 'x2': None,
 'y1': None,
 'y2': None}
2023-02-27 00:46:40 Config:
{'Task': None,
 'TrainingFraction': None,
 'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'alphavalue': None,
 'apply_prob': 0.5,
 'batch_size': 1,
 'bodyparts': None,
 'colormap': None,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'corner2move2': None,
 'crop_pad': 0,
 'cropping': None,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'imgaug',
 'date': None,
 'decay_steps': 30000,
 'default_augmenter': None,
 'default_net_type': None,
 'deterministic': False,
 'display_iters': 1000,
 'dotsize': None,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'identity': None,
 'init_weights': 'C:\\Users\\Pc-Ryzen\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'iteration': None,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'move2corner': None,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'multianimalproject': None,
 'net_type': 'resnet_50',
 'num_joints': 17,
 'numframes2pick': None,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pcutoff': None,
 'pos_dist_thresh': 17,
 'project_path': 'D:/DLC/CV-Mena-2023-02-26/dlc-models/iteration-0/CVFeb26-trainset95shuffle1/train',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'scorer': None,
 'shuffle': True,
 'skeleton': [],
 'skeleton_color': 'black',
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'snapshotindex': None,
 'start': None,
 'stop': None,
 'stride': 8.0,
 'video_sets': None,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001,
 'x1': None,
 'x2': None,
 'y1': None,
 'y2': None}
2023-02-27 09:48:38 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 1,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'imgaug',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\Pc-Ryzen\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': 'D:/DLC/CV-Mena-2023-02-26',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-27 10:16:28 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 1,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'tensorpack',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\Pc-Ryzen\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': 'D:/DLC/CV-Mena-2023-02-26',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-27 10:19:51 iteration: 100 loss: 2206301005.2830 lr: 0.005
2023-02-27 10:22:43 iteration: 200 loss: 67998171.9413 lr: 0.005
2023-02-27 10:25:24 iteration: 300 loss: 138295578166.8048 lr: 0.005
2023-02-27 10:27:55 iteration: 400 loss: 61583595186413.7188 lr: 0.005
2023-02-27 10:38:45 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 1,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'tensorpack',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\Pc-Ryzen\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': 'D:/DLC/CV-Mena-2023-02-26',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-27 10:43:00 iteration: 100 loss: 2833184530365.2896 lr: 0.005
2023-02-27 10:44:51 iteration: 200 loss: 1633659385.9927 lr: 0.005
2023-02-27 10:47:26 iteration: 300 loss: 786077722.5317 lr: 0.005
2023-02-27 10:49:02 iteration: 400 loss: 54264827421476.9922 lr: 0.005
2023-02-27 10:51:43 iteration: 500 loss: 1417060008420707.5000 lr: 0.005
2023-02-27 11:07:07 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 1,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'tensorpack',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\Pc-Ryzen\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': 'D:/DLC/CV-Mena-2023-02-26',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-27 11:10:53 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 1,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'tensorpack',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\Pc-Ryzen\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': 'D:/DLC/CV-Mena-2023-02-26',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-27 11:14:19 iteration: 100 loss: 119911853587.4479 lr: 0.005
2023-02-27 11:17:29 iteration: 200 loss: 259840715.8359 lr: 0.005
2023-02-27 11:19:56 iteration: 300 loss: 114725842409.2994 lr: 0.005
2023-02-27 11:22:58 iteration: 400 loss: 3179747101149.4102 lr: 0.005
2023-02-27 13:05:12 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 1,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'tensorpack',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\Pc-Ryzen\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': 'D:/DLC/CV-Mena-2023-02-26',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-27 13:17:02 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 1,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'imgaug',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\Pc-Ryzen\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': 'D:/DLC/CV-Mena-2023-02-26',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-27 23:34:15 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 1,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'imgaug',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\Pc-Ryzen\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': 'D:/DLC/CV-Mena-2023-02-26',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-27 23:34:55 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 1,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'imgaug',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\Pc-Ryzen\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': 'D:/DLC/CV-Mena-2023-02-26',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-27 23:36:20 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 1,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'imgaug',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\Pc-Ryzen\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': 'D:/DLC/CV-Mena-2023-02-26',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-27 23:47:37 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 1,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'imgaug',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\Pc-Ryzen\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': 'D:/DLC/CV-Mena-2023-02-26',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-27 23:47:46 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 1,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'imgaug',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\Pc-Ryzen\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': 'D:/DLC/CV-Mena-2023-02-26',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-27 23:48:29 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 1,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'imgaug',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\Pc-Ryzen\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': 'D:/DLC/CV-Mena-2023-02-26',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-27 23:49:39 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 1,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'tensorpack',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\mena_\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': 'D:/DLC/CV-Mena-2023-02-26',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-28 12:54:07 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 1,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'default',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\mena_\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': 'D:\\DLC\\CV-Mena-2023-02-26',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-28 13:02:03 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 1,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'default',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\mena_\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': 'D:\\DLC\\CV-Mena-2023-02-26',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-28 13:04:42 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 1,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'default',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\mena_\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': 'D:\\DLC\\CV-Mena-2023-02-26',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-28 13:16:32 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 1,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'default',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\mena_\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': 'D:\\DLC\\CV-Mena-2023-02-26',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-28 13:45:31 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 1,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'default',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\mena_\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': 'D:\\DLC\\CV-Mena-2023-02-26',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-28 13:49:32 iteration: 100 loss: 0.0702 lr: 0.005
2023-02-28 13:53:51 iteration: 200 loss: 0.0241 lr: 0.005
2023-02-28 13:58:29 iteration: 300 loss: 0.0218 lr: 0.005
2023-02-28 14:03:28 iteration: 400 loss: 0.0222 lr: 0.005
2023-02-28 19:34:13 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 1,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'default',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\mena_\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': 'D:\\DLC\\CV-Mena-2023-02-26',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-28 19:39:56 Use all bodyparts
2023-02-28 19:42:56 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 1,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'imgaug',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\Pc-Ryzen\\anaconda3\\envs\\DEEPLABCUT\\Lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': 'D:/DLC/CV-Mena-2023-02-26',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-28 19:45:21 iteration: 100 loss: 0.0856 lr: 0.005
2023-02-28 19:50:14 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 1,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'imgaug',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\Pc-Ryzen\\anaconda3\\envs\\DEEPLABCUT\\Lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': 'D:/DLC/CV-Mena-2023-02-26',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-28 19:52:36 iteration: 100 loss: 0.0780 lr: 0.005
2023-02-28 19:55:00 iteration: 200 loss: 0.0242 lr: 0.005
2023-02-28 19:57:28 iteration: 300 loss: 0.0222 lr: 0.005
2023-02-28 21:21:33 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 1,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'imgaug',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\Pc-Ryzen\\anaconda3\\envs\\DEEPLABCUT\\Lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': 'D:/DLC/CV-Mena-2023-02-26',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-28 21:22:43 iteration: 100 loss: 0.0777 lr: 0.005
2023-02-28 21:23:05 iteration: 200 loss: 0.0244 lr: 0.005
2023-02-28 21:23:21 iteration: 300 loss: 0.0224 lr: 0.005
2023-02-28 21:23:36 iteration: 400 loss: 0.0228 lr: 0.005
2023-02-28 21:23:48 iteration: 500 loss: 0.0209 lr: 0.005
2023-02-28 21:24:01 iteration: 600 loss: 0.0202 lr: 0.005
2023-02-28 21:24:12 iteration: 700 loss: 0.0188 lr: 0.005
2023-02-28 21:24:22 iteration: 800 loss: 0.0203 lr: 0.005
2023-02-28 21:24:32 iteration: 900 loss: 0.0192 lr: 0.005
2023-02-28 21:24:42 iteration: 1000 loss: 0.0192 lr: 0.005
2023-02-28 21:24:51 iteration: 1100 loss: 0.0180 lr: 0.005
2023-02-28 21:24:59 iteration: 1200 loss: 0.0189 lr: 0.005
2023-02-28 21:25:08 iteration: 1300 loss: 0.0188 lr: 0.005
2023-02-28 21:25:17 iteration: 1400 loss: 0.0178 lr: 0.005
2023-02-28 21:25:24 iteration: 1500 loss: 0.0174 lr: 0.005
2023-02-28 21:25:32 iteration: 1600 loss: 0.0170 lr: 0.005
2023-02-28 21:25:41 iteration: 1700 loss: 0.0165 lr: 0.005
2023-02-28 21:25:49 iteration: 1800 loss: 0.0173 lr: 0.005
2023-02-28 21:25:57 iteration: 1900 loss: 0.0169 lr: 0.005
2023-02-28 21:26:04 iteration: 2000 loss: 0.0162 lr: 0.005
2023-02-28 21:26:13 iteration: 2100 loss: 0.0173 lr: 0.005
2023-02-28 21:26:20 iteration: 2200 loss: 0.0160 lr: 0.005
2023-02-28 21:26:27 iteration: 2300 loss: 0.0155 lr: 0.005
2023-02-28 21:26:34 iteration: 2400 loss: 0.0163 lr: 0.005
2023-02-28 21:26:42 iteration: 2500 loss: 0.0161 lr: 0.005
2023-02-28 21:26:49 iteration: 2600 loss: 0.0157 lr: 0.005
2023-02-28 21:26:56 iteration: 2700 loss: 0.0155 lr: 0.005
2023-02-28 21:27:03 iteration: 2800 loss: 0.0152 lr: 0.005
2023-02-28 21:27:10 iteration: 2900 loss: 0.0148 lr: 0.005
2023-02-28 21:27:17 iteration: 3000 loss: 0.0160 lr: 0.005
2023-02-28 21:27:24 iteration: 3100 loss: 0.0151 lr: 0.005
2023-02-28 21:27:32 iteration: 3200 loss: 0.0146 lr: 0.005
2023-02-28 21:27:39 iteration: 3300 loss: 0.0152 lr: 0.005
2023-02-28 21:27:45 iteration: 3400 loss: 0.0143 lr: 0.005
2023-02-28 21:27:53 iteration: 3500 loss: 0.0146 lr: 0.005
2023-02-28 21:28:00 iteration: 3600 loss: 0.0147 lr: 0.005
2023-02-28 21:28:07 iteration: 3700 loss: 0.0140 lr: 0.005
2023-02-28 21:28:15 iteration: 3800 loss: 0.0141 lr: 0.005
2023-02-28 21:28:22 iteration: 3900 loss: 0.0147 lr: 0.005
2023-02-28 21:28:30 iteration: 4000 loss: 0.0146 lr: 0.005
2023-02-28 21:28:37 iteration: 4100 loss: 0.0143 lr: 0.005
2023-02-28 21:28:44 iteration: 4200 loss: 0.0140 lr: 0.005
2023-02-28 21:28:52 iteration: 4300 loss: 0.0140 lr: 0.005
2023-02-28 21:28:59 iteration: 4400 loss: 0.0133 lr: 0.005
2023-02-28 21:29:07 iteration: 4500 loss: 0.0136 lr: 0.005
2023-02-28 21:29:14 iteration: 4600 loss: 0.0133 lr: 0.005
2023-02-28 21:29:21 iteration: 4700 loss: 0.0137 lr: 0.005
2023-02-28 21:29:28 iteration: 4800 loss: 0.0130 lr: 0.005
2023-02-28 21:29:35 iteration: 4900 loss: 0.0125 lr: 0.005
2023-02-28 21:29:42 iteration: 5000 loss: 0.0132 lr: 0.005
2023-02-28 21:29:50 iteration: 5100 loss: 0.0134 lr: 0.005
2023-02-28 21:29:57 iteration: 5200 loss: 0.0133 lr: 0.005
2023-02-28 21:30:05 iteration: 5300 loss: 0.0122 lr: 0.005
2023-02-28 21:30:12 iteration: 5400 loss: 0.0122 lr: 0.005
2023-02-28 21:30:19 iteration: 5500 loss: 0.0130 lr: 0.005
2023-02-28 21:30:27 iteration: 5600 loss: 0.0131 lr: 0.005
2023-02-28 21:30:34 iteration: 5700 loss: 0.0135 lr: 0.005
2023-02-28 21:30:42 iteration: 5800 loss: 0.0123 lr: 0.005
2023-02-28 21:30:49 iteration: 5900 loss: 0.0126 lr: 0.005
2023-02-28 21:30:56 iteration: 6000 loss: 0.0133 lr: 0.005
2023-02-28 21:31:03 iteration: 6100 loss: 0.0129 lr: 0.005
2023-02-28 21:31:10 iteration: 6200 loss: 0.0129 lr: 0.005
2023-02-28 21:31:18 iteration: 6300 loss: 0.0125 lr: 0.005
2023-02-28 21:31:25 iteration: 6400 loss: 0.0126 lr: 0.005
2023-02-28 21:31:32 iteration: 6500 loss: 0.0123 lr: 0.005
2023-02-28 21:31:40 iteration: 6600 loss: 0.0127 lr: 0.005
2023-02-28 21:31:47 iteration: 6700 loss: 0.0127 lr: 0.005
2023-02-28 21:31:54 iteration: 6800 loss: 0.0124 lr: 0.005
2023-02-28 21:32:01 iteration: 6900 loss: 0.0125 lr: 0.005
2023-02-28 21:32:09 iteration: 7000 loss: 0.0128 lr: 0.005
2023-02-28 21:32:15 iteration: 7100 loss: 0.0122 lr: 0.005
2023-02-28 21:32:23 iteration: 7200 loss: 0.0123 lr: 0.005
2023-02-28 21:32:30 iteration: 7300 loss: 0.0119 lr: 0.005
2023-02-28 21:32:37 iteration: 7400 loss: 0.0121 lr: 0.005
2023-02-28 21:32:45 iteration: 7500 loss: 0.0117 lr: 0.005
2023-02-28 21:32:52 iteration: 7600 loss: 0.0117 lr: 0.005
2023-02-28 21:32:59 iteration: 7700 loss: 0.0117 lr: 0.005
2023-02-28 21:33:06 iteration: 7800 loss: 0.0113 lr: 0.005
2023-02-28 21:33:13 iteration: 7900 loss: 0.0112 lr: 0.005
2023-02-28 21:33:20 iteration: 8000 loss: 0.0122 lr: 0.005
2023-02-28 21:33:27 iteration: 8100 loss: 0.0111 lr: 0.005
2023-02-28 21:33:35 iteration: 8200 loss: 0.0121 lr: 0.005
2023-02-28 21:33:42 iteration: 8300 loss: 0.0120 lr: 0.005
2023-02-28 21:33:49 iteration: 8400 loss: 0.0112 lr: 0.005
2023-02-28 21:33:57 iteration: 8500 loss: 0.0116 lr: 0.005
2023-02-28 21:34:04 iteration: 8600 loss: 0.0112 lr: 0.005
2023-02-28 21:34:11 iteration: 8700 loss: 0.0115 lr: 0.005
2023-02-28 21:34:18 iteration: 8800 loss: 0.0115 lr: 0.005
2023-02-28 21:34:26 iteration: 8900 loss: 0.0110 lr: 0.005
2023-02-28 21:34:33 iteration: 9000 loss: 0.0110 lr: 0.005
2023-02-28 21:34:40 iteration: 9100 loss: 0.0115 lr: 0.005
2023-02-28 21:34:47 iteration: 9200 loss: 0.0111 lr: 0.005
2023-02-28 21:34:55 iteration: 9300 loss: 0.0110 lr: 0.005
2023-02-28 21:35:03 iteration: 9400 loss: 0.0127 lr: 0.005
2023-02-28 21:35:09 iteration: 9500 loss: 0.0102 lr: 0.005
2023-02-28 21:35:16 iteration: 9600 loss: 0.0116 lr: 0.005
2023-02-28 21:35:24 iteration: 9700 loss: 0.0116 lr: 0.005
2023-02-28 21:35:31 iteration: 9800 loss: 0.0104 lr: 0.005
2023-02-28 21:35:38 iteration: 9900 loss: 0.0113 lr: 0.005
2023-02-28 21:35:46 iteration: 10000 loss: 0.0108 lr: 0.005
2023-02-28 21:35:54 iteration: 10100 loss: 0.0120 lr: 0.02
2023-02-28 21:36:01 iteration: 10200 loss: 0.0119 lr: 0.02
2023-02-28 21:36:09 iteration: 10300 loss: 0.0126 lr: 0.02
2023-02-28 21:36:16 iteration: 10400 loss: 0.0130 lr: 0.02
2023-02-28 21:36:23 iteration: 10500 loss: 0.0134 lr: 0.02
2023-02-28 21:36:31 iteration: 10600 loss: 0.0118 lr: 0.02
2023-02-28 21:36:38 iteration: 10700 loss: 0.0115 lr: 0.02
2023-02-28 21:36:45 iteration: 10800 loss: 0.0116 lr: 0.02
2023-02-28 21:36:53 iteration: 10900 loss: 0.0116 lr: 0.02
2023-02-28 21:37:00 iteration: 11000 loss: 0.0109 lr: 0.02
2023-02-28 21:37:07 iteration: 11100 loss: 0.0109 lr: 0.02
2023-02-28 21:37:15 iteration: 11200 loss: 0.0119 lr: 0.02
2023-02-28 21:37:22 iteration: 11300 loss: 0.0107 lr: 0.02
2023-02-28 21:37:30 iteration: 11400 loss: 0.0115 lr: 0.02
2023-02-28 21:37:37 iteration: 11500 loss: 0.0107 lr: 0.02
2023-02-28 21:37:44 iteration: 11600 loss: 0.0104 lr: 0.02
2023-02-28 21:37:51 iteration: 11700 loss: 0.0106 lr: 0.02
2023-02-28 21:37:59 iteration: 11800 loss: 0.0112 lr: 0.02
2023-02-28 21:38:07 iteration: 11900 loss: 0.0098 lr: 0.02
2023-02-28 21:38:14 iteration: 12000 loss: 0.0103 lr: 0.02
2023-02-28 21:38:22 iteration: 12100 loss: 0.0112 lr: 0.02
2023-02-28 21:38:29 iteration: 12200 loss: 0.0103 lr: 0.02
2023-02-28 21:38:36 iteration: 12300 loss: 0.0099 lr: 0.02
2023-02-28 21:38:43 iteration: 12400 loss: 0.0118 lr: 0.02
2023-02-28 21:38:51 iteration: 12500 loss: 0.0103 lr: 0.02
2023-02-28 21:38:58 iteration: 12600 loss: 0.0097 lr: 0.02
2023-02-28 21:39:05 iteration: 12700 loss: 0.0098 lr: 0.02
2023-02-28 21:39:12 iteration: 12800 loss: 0.0093 lr: 0.02
2023-02-28 21:39:20 iteration: 12900 loss: 0.0096 lr: 0.02
2023-02-28 21:39:27 iteration: 13000 loss: 0.0100 lr: 0.02
2023-02-28 21:39:34 iteration: 13100 loss: 0.0097 lr: 0.02
2023-02-28 21:39:42 iteration: 13200 loss: 0.0097 lr: 0.02
2023-02-28 21:39:49 iteration: 13300 loss: 0.0101 lr: 0.02
2023-02-28 21:39:57 iteration: 13400 loss: 0.0090 lr: 0.02
2023-02-28 21:40:04 iteration: 13500 loss: 0.0090 lr: 0.02
2023-02-28 21:40:11 iteration: 13600 loss: 0.0093 lr: 0.02
2023-02-28 21:40:18 iteration: 13700 loss: 0.0093 lr: 0.02
2023-02-28 21:40:25 iteration: 13800 loss: 0.0092 lr: 0.02
2023-02-28 21:40:33 iteration: 13900 loss: 0.0087 lr: 0.02
2023-02-28 21:40:40 iteration: 14000 loss: 0.0091 lr: 0.02
2023-02-28 21:40:48 iteration: 14100 loss: 0.0098 lr: 0.02
2023-02-28 21:40:55 iteration: 14200 loss: 0.0092 lr: 0.02
2023-02-28 21:41:02 iteration: 14300 loss: 0.0088 lr: 0.02
2023-02-28 21:41:10 iteration: 14400 loss: 0.0089 lr: 0.02
2023-02-28 21:41:18 iteration: 14500 loss: 0.0084 lr: 0.02
2023-02-28 21:41:25 iteration: 14600 loss: 0.0086 lr: 0.02
2023-02-28 21:41:32 iteration: 14700 loss: 0.0089 lr: 0.02
2023-02-28 21:41:40 iteration: 14800 loss: 0.0089 lr: 0.02
2023-02-28 21:41:47 iteration: 14900 loss: 0.0086 lr: 0.02
2023-02-28 21:41:54 iteration: 15000 loss: 0.0089 lr: 0.02
2023-02-28 21:42:03 iteration: 15100 loss: 0.0088 lr: 0.02
2023-02-28 21:42:10 iteration: 15200 loss: 0.0088 lr: 0.02
2023-02-28 21:42:18 iteration: 15300 loss: 0.0090 lr: 0.02
2023-02-28 21:42:25 iteration: 15400 loss: 0.0090 lr: 0.02
2023-02-28 21:42:32 iteration: 15500 loss: 0.0082 lr: 0.02
2023-02-28 21:42:39 iteration: 15600 loss: 0.0090 lr: 0.02
2023-02-28 21:42:47 iteration: 15700 loss: 0.0081 lr: 0.02
2023-02-28 21:42:54 iteration: 15800 loss: 0.0087 lr: 0.02
2023-02-28 21:43:01 iteration: 15900 loss: 0.0084 lr: 0.02
2023-02-28 21:43:09 iteration: 16000 loss: 0.0084 lr: 0.02
2023-02-28 21:43:16 iteration: 16100 loss: 0.0082 lr: 0.02
2023-02-28 21:43:24 iteration: 16200 loss: 0.0080 lr: 0.02
2023-02-28 21:43:31 iteration: 16300 loss: 0.0074 lr: 0.02
2023-02-28 21:43:39 iteration: 16400 loss: 0.0081 lr: 0.02
2023-02-28 21:43:47 iteration: 16500 loss: 0.0080 lr: 0.02
2023-02-28 21:43:54 iteration: 16600 loss: 0.0081 lr: 0.02
2023-02-28 21:44:01 iteration: 16700 loss: 0.0078 lr: 0.02
2023-02-28 21:44:09 iteration: 16800 loss: 0.0080 lr: 0.02
2023-02-28 21:44:16 iteration: 16900 loss: 0.0074 lr: 0.02
2023-02-28 21:44:24 iteration: 17000 loss: 0.0090 lr: 0.02
2023-02-28 21:44:31 iteration: 17100 loss: 0.0081 lr: 0.02
2023-02-28 21:44:38 iteration: 17200 loss: 0.0078 lr: 0.02
2023-02-28 21:44:46 iteration: 17300 loss: 0.0079 lr: 0.02
2023-02-28 21:44:53 iteration: 17400 loss: 0.0077 lr: 0.02
2023-02-28 21:45:01 iteration: 17500 loss: 0.0078 lr: 0.02
2023-02-28 21:45:08 iteration: 17600 loss: 0.0074 lr: 0.02
2023-02-28 21:45:16 iteration: 17700 loss: 0.0079 lr: 0.02
2023-02-28 21:45:23 iteration: 17800 loss: 0.0080 lr: 0.02
2023-02-28 21:45:30 iteration: 17900 loss: 0.0082 lr: 0.02
2023-02-28 21:45:38 iteration: 18000 loss: 0.0080 lr: 0.02
2023-02-28 21:45:45 iteration: 18100 loss: 0.0071 lr: 0.02
2023-02-28 21:45:53 iteration: 18200 loss: 0.0074 lr: 0.02
2023-02-28 21:46:00 iteration: 18300 loss: 0.0079 lr: 0.02
2023-02-28 21:46:07 iteration: 18400 loss: 0.0071 lr: 0.02
2023-02-28 21:46:14 iteration: 18500 loss: 0.0073 lr: 0.02
2023-02-28 21:46:21 iteration: 18600 loss: 0.0073 lr: 0.02
2023-02-28 21:46:28 iteration: 18700 loss: 0.0076 lr: 0.02
2023-02-28 21:46:35 iteration: 18800 loss: 0.0076 lr: 0.02
2023-02-28 21:46:42 iteration: 18900 loss: 0.0072 lr: 0.02
2023-02-28 21:46:49 iteration: 19000 loss: 0.0074 lr: 0.02
2023-02-28 21:46:56 iteration: 19100 loss: 0.0073 lr: 0.02
2023-02-28 21:47:04 iteration: 19200 loss: 0.0075 lr: 0.02
2023-02-28 21:47:11 iteration: 19300 loss: 0.0068 lr: 0.02
2023-02-28 21:47:18 iteration: 19400 loss: 0.0067 lr: 0.02
2023-02-28 21:47:25 iteration: 19500 loss: 0.0069 lr: 0.02
2023-02-28 21:47:32 iteration: 19600 loss: 0.0071 lr: 0.02
2023-02-28 21:47:39 iteration: 19700 loss: 0.0068 lr: 0.02
2023-02-28 21:47:46 iteration: 19800 loss: 0.0068 lr: 0.02
2023-02-28 21:47:53 iteration: 19900 loss: 0.0068 lr: 0.02
2023-02-28 21:48:01 iteration: 20000 loss: 0.0070 lr: 0.02
2023-02-28 21:48:08 iteration: 20100 loss: 0.0068 lr: 0.02
2023-02-28 21:48:16 iteration: 20200 loss: 0.0067 lr: 0.02
2023-02-28 21:48:23 iteration: 20300 loss: 0.0067 lr: 0.02
2023-02-28 21:48:30 iteration: 20400 loss: 0.0068 lr: 0.02
2023-02-28 21:48:37 iteration: 20500 loss: 0.0072 lr: 0.02
2023-02-28 21:48:44 iteration: 20600 loss: 0.0064 lr: 0.02
2023-02-28 21:48:52 iteration: 20700 loss: 0.0063 lr: 0.02
2023-02-28 21:48:59 iteration: 20800 loss: 0.0068 lr: 0.02
2023-02-28 21:49:06 iteration: 20900 loss: 0.0068 lr: 0.02
2023-02-28 21:49:13 iteration: 21000 loss: 0.0069 lr: 0.02
2023-02-28 21:49:20 iteration: 21100 loss: 0.0063 lr: 0.02
2023-02-28 21:49:27 iteration: 21200 loss: 0.0068 lr: 0.02
2023-02-28 21:49:34 iteration: 21300 loss: 0.0062 lr: 0.02
2023-02-28 21:49:42 iteration: 21400 loss: 0.0068 lr: 0.02
2023-02-28 21:49:49 iteration: 21500 loss: 0.0068 lr: 0.02
2023-02-28 21:49:56 iteration: 21600 loss: 0.0066 lr: 0.02
2023-02-28 21:50:03 iteration: 21700 loss: 0.0068 lr: 0.02
2023-02-28 21:50:11 iteration: 21800 loss: 0.0064 lr: 0.02
2023-02-28 21:50:18 iteration: 21900 loss: 0.0061 lr: 0.02
2023-02-28 21:50:25 iteration: 22000 loss: 0.0066 lr: 0.02
2023-02-28 21:50:32 iteration: 22100 loss: 0.0064 lr: 0.02
2023-02-28 21:50:39 iteration: 22200 loss: 0.0065 lr: 0.02
2023-02-28 21:50:47 iteration: 22300 loss: 0.0067 lr: 0.02
2023-02-28 21:50:54 iteration: 22400 loss: 0.0064 lr: 0.02
2023-02-28 21:51:01 iteration: 22500 loss: 0.0063 lr: 0.02
2023-02-28 21:51:08 iteration: 22600 loss: 0.0060 lr: 0.02
2023-02-28 21:51:15 iteration: 22700 loss: 0.0068 lr: 0.02
2023-02-28 21:51:22 iteration: 22800 loss: 0.0064 lr: 0.02
2023-02-28 21:51:30 iteration: 22900 loss: 0.0065 lr: 0.02
2023-02-28 21:51:37 iteration: 23000 loss: 0.0064 lr: 0.02
2023-02-28 21:51:44 iteration: 23100 loss: 0.0063 lr: 0.02
2023-02-28 21:51:52 iteration: 23200 loss: 0.0067 lr: 0.02
2023-02-28 21:51:59 iteration: 23300 loss: 0.0060 lr: 0.02
2023-02-28 21:52:06 iteration: 23400 loss: 0.0058 lr: 0.02
2023-02-28 21:52:14 iteration: 23500 loss: 0.0062 lr: 0.02
2023-02-28 21:52:21 iteration: 23600 loss: 0.0063 lr: 0.02
2023-02-28 21:52:28 iteration: 23700 loss: 0.0062 lr: 0.02
2023-02-28 21:52:35 iteration: 23800 loss: 0.0063 lr: 0.02
2023-02-28 21:52:42 iteration: 23900 loss: 0.0060 lr: 0.02
2023-02-28 21:52:49 iteration: 24000 loss: 0.0057 lr: 0.02
2023-02-28 21:52:56 iteration: 24100 loss: 0.0058 lr: 0.02
2023-02-28 21:53:03 iteration: 24200 loss: 0.0058 lr: 0.02
2023-02-28 21:53:11 iteration: 24300 loss: 0.0060 lr: 0.02
2023-02-28 21:53:18 iteration: 24400 loss: 0.0062 lr: 0.02
2023-02-28 21:53:25 iteration: 24500 loss: 0.0059 lr: 0.02
2023-02-28 21:53:32 iteration: 24600 loss: 0.0064 lr: 0.02
2023-02-28 21:53:40 iteration: 24700 loss: 0.0064 lr: 0.02
2023-02-28 21:53:47 iteration: 24800 loss: 0.0059 lr: 0.02
2023-02-28 21:53:54 iteration: 24900 loss: 0.0062 lr: 0.02
2023-02-28 21:54:01 iteration: 25000 loss: 0.0060 lr: 0.02
2023-02-28 21:54:09 iteration: 25100 loss: 0.0056 lr: 0.02
2023-02-28 21:54:17 iteration: 25200 loss: 0.0065 lr: 0.02
2023-02-28 21:54:24 iteration: 25300 loss: 0.0058 lr: 0.02
2023-02-28 21:54:30 iteration: 25400 loss: 0.0058 lr: 0.02
2023-02-28 21:54:38 iteration: 25500 loss: 0.0063 lr: 0.02
2023-02-28 21:54:45 iteration: 25600 loss: 0.0061 lr: 0.02
2023-02-28 21:54:53 iteration: 25700 loss: 0.0055 lr: 0.02
2023-02-28 21:55:01 iteration: 25800 loss: 0.0057 lr: 0.02
2023-02-28 21:55:09 iteration: 25900 loss: 0.0058 lr: 0.02
2023-02-28 21:55:17 iteration: 26000 loss: 0.0062 lr: 0.02
2023-02-28 21:55:24 iteration: 26100 loss: 0.0061 lr: 0.02
2023-02-28 21:55:32 iteration: 26200 loss: 0.0056 lr: 0.02
2023-02-28 21:55:40 iteration: 26300 loss: 0.0058 lr: 0.02
2023-02-28 21:55:48 iteration: 26400 loss: 0.0060 lr: 0.02
2023-02-28 21:55:56 iteration: 26500 loss: 0.0059 lr: 0.02
2023-02-28 21:56:04 iteration: 26600 loss: 0.0063 lr: 0.02
2023-02-28 21:56:12 iteration: 26700 loss: 0.0055 lr: 0.02
2023-02-28 21:56:19 iteration: 26800 loss: 0.0054 lr: 0.02
2023-02-28 21:56:27 iteration: 26900 loss: 0.0058 lr: 0.02
2023-02-28 21:56:35 iteration: 27000 loss: 0.0057 lr: 0.02
2023-02-28 21:56:43 iteration: 27100 loss: 0.0059 lr: 0.02
2023-02-28 21:56:50 iteration: 27200 loss: 0.0060 lr: 0.02
2023-02-28 21:56:59 iteration: 27300 loss: 0.0056 lr: 0.02
2023-02-28 21:57:06 iteration: 27400 loss: 0.0054 lr: 0.02
2023-02-28 21:57:14 iteration: 27500 loss: 0.0055 lr: 0.02
2023-02-28 21:57:22 iteration: 27600 loss: 0.0062 lr: 0.02
2023-02-28 21:57:29 iteration: 27700 loss: 0.0059 lr: 0.02
2023-02-28 21:57:37 iteration: 27800 loss: 0.0053 lr: 0.02
2023-02-28 21:57:45 iteration: 27900 loss: 0.0061 lr: 0.02
2023-02-28 21:57:52 iteration: 28000 loss: 0.0053 lr: 0.02
2023-02-28 21:58:00 iteration: 28100 loss: 0.0052 lr: 0.02
2023-02-28 21:58:08 iteration: 28200 loss: 0.0054 lr: 0.02
2023-02-28 21:58:16 iteration: 28300 loss: 0.0055 lr: 0.02
2023-02-28 21:58:24 iteration: 28400 loss: 0.0058 lr: 0.02
2023-02-28 21:58:31 iteration: 28500 loss: 0.0051 lr: 0.02
2023-02-28 21:58:39 iteration: 28600 loss: 0.0050 lr: 0.02
2023-02-28 21:58:46 iteration: 28700 loss: 0.0052 lr: 0.02
2023-02-28 21:58:54 iteration: 28800 loss: 0.0061 lr: 0.02
2023-02-28 21:59:02 iteration: 28900 loss: 0.0053 lr: 0.02
2023-02-28 21:59:10 iteration: 29000 loss: 0.0052 lr: 0.02
2023-02-28 21:59:17 iteration: 29100 loss: 0.0057 lr: 0.02
2023-02-28 21:59:25 iteration: 29200 loss: 0.0053 lr: 0.02
2023-02-28 21:59:33 iteration: 29300 loss: 0.0055 lr: 0.02
2023-02-28 21:59:41 iteration: 29400 loss: 0.0053 lr: 0.02
2023-02-28 21:59:49 iteration: 29500 loss: 0.0052 lr: 0.02
2023-02-28 21:59:57 iteration: 29600 loss: 0.0051 lr: 0.02
2023-02-28 22:00:05 iteration: 29700 loss: 0.0052 lr: 0.02
2023-02-28 22:00:12 iteration: 29800 loss: 0.0054 lr: 0.02
2023-02-28 22:00:20 iteration: 29900 loss: 0.0053 lr: 0.02
2023-02-28 22:00:28 iteration: 30000 loss: 0.0052 lr: 0.02
2023-02-28 22:17:46 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 1,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'imgaug',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\Pc-Ryzen\\anaconda3\\envs\\DEEPLABCUT\\Lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': 'D:/DLC/CV-Mena-2023-02-26',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-28 22:18:08 iteration: 10 loss: 0.4704 lr: 0.005
2023-02-28 22:18:13 iteration: 20 loss: 0.0596 lr: 0.005
2023-02-28 22:18:18 iteration: 30 loss: 0.0438 lr: 0.005
2023-02-28 22:18:22 iteration: 40 loss: 0.0318 lr: 0.005
2023-02-28 22:18:27 iteration: 50 loss: 0.0280 lr: 0.005
2023-02-28 22:18:31 iteration: 60 loss: 0.0257 lr: 0.005
2023-02-28 22:18:36 iteration: 70 loss: 0.0274 lr: 0.005
2023-02-28 22:18:39 iteration: 80 loss: 0.0251 lr: 0.005
2023-02-28 22:18:43 iteration: 90 loss: 0.0262 lr: 0.005
2023-02-28 22:18:55 iteration: 100 loss: 0.0245 lr: 0.005
2023-02-28 22:19:02 iteration: 110 loss: 0.0227 lr: 0.005
2023-02-28 22:19:06 iteration: 120 loss: 0.0263 lr: 0.005
2023-02-28 22:19:09 iteration: 130 loss: 0.0234 lr: 0.005
2023-02-28 22:19:13 iteration: 140 loss: 0.0247 lr: 0.005
2023-02-28 22:19:15 iteration: 150 loss: 0.0241 lr: 0.005
2023-02-28 22:19:17 iteration: 160 loss: 0.0259 lr: 0.005
2023-02-28 22:19:20 iteration: 170 loss: 0.0224 lr: 0.005
2023-02-28 22:19:23 iteration: 180 loss: 0.0262 lr: 0.005
2023-02-28 22:19:26 iteration: 190 loss: 0.0234 lr: 0.005
2023-02-28 22:19:29 iteration: 200 loss: 0.0253 lr: 0.005
2023-02-28 22:19:34 iteration: 210 loss: 0.0238 lr: 0.005
2023-02-28 22:19:37 iteration: 220 loss: 0.0258 lr: 0.005
2023-02-28 22:19:39 iteration: 230 loss: 0.0216 lr: 0.005
2023-02-28 22:19:43 iteration: 240 loss: 0.0255 lr: 0.005
2023-02-28 22:19:45 iteration: 250 loss: 0.0193 lr: 0.005
2023-02-28 22:19:47 iteration: 260 loss: 0.0193 lr: 0.005
2023-02-28 22:19:49 iteration: 270 loss: 0.0222 lr: 0.005
2023-02-28 22:19:51 iteration: 280 loss: 0.0218 lr: 0.005
2023-02-28 22:19:53 iteration: 290 loss: 0.0210 lr: 0.005
2023-02-28 22:19:55 iteration: 300 loss: 0.0244 lr: 0.005
2023-02-28 22:20:01 iteration: 310 loss: 0.0219 lr: 0.005
2023-02-28 22:20:03 iteration: 320 loss: 0.0245 lr: 0.005
2023-02-28 22:20:06 iteration: 330 loss: 0.0240 lr: 0.005
2023-02-28 22:20:08 iteration: 340 loss: 0.0230 lr: 0.005
2023-02-28 22:20:10 iteration: 350 loss: 0.0187 lr: 0.005
2023-02-28 22:20:13 iteration: 360 loss: 0.0241 lr: 0.005
2023-02-28 22:20:14 iteration: 370 loss: 0.0207 lr: 0.005
2023-02-28 22:20:16 iteration: 380 loss: 0.0262 lr: 0.005
2023-02-28 22:20:18 iteration: 390 loss: 0.0234 lr: 0.005
2023-02-28 22:20:20 iteration: 400 loss: 0.0209 lr: 0.005
2023-02-28 22:20:23 iteration: 410 loss: 0.0220 lr: 0.005
2023-02-28 22:20:25 iteration: 420 loss: 0.0220 lr: 0.005
2023-02-28 22:20:28 iteration: 430 loss: 0.0215 lr: 0.005
2023-02-28 22:20:30 iteration: 440 loss: 0.0235 lr: 0.005
2023-02-28 22:20:32 iteration: 450 loss: 0.0225 lr: 0.005
2023-02-28 22:20:34 iteration: 460 loss: 0.0214 lr: 0.005
2023-02-28 22:20:35 iteration: 470 loss: 0.0186 lr: 0.005
2023-02-28 22:20:38 iteration: 480 loss: 0.0223 lr: 0.005
2023-02-28 22:20:40 iteration: 490 loss: 0.0220 lr: 0.005
2023-02-28 22:20:42 iteration: 500 loss: 0.0205 lr: 0.005
2023-02-28 22:21:38 Use all bodyparts
2023-02-28 22:22:12 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'batch_size': 1,
 'crop_pad': 0,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'imgaug',
 'deterministic': False,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\Pc-Ryzen\\anaconda3\\envs\\DEEPLABCUT\\Lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 1.0,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'mean_pixel': [123.68, 116.779, 103.939],
 'mirror': False,
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': True,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'regularize': False,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\test\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-28 22:22:56 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'batch_size': 1,
 'crop_pad': 0,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'imgaug',
 'deterministic': False,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\Pc-Ryzen\\anaconda3\\envs\\DEEPLABCUT\\Lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 1.0,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'mean_pixel': [123.68, 116.779, 103.939],
 'mirror': False,
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': True,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'regularize': False,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\test\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-28 22:23:25 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'batch_size': 1,
 'crop_pad': 0,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'imgaug',
 'deterministic': False,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\Pc-Ryzen\\anaconda3\\envs\\DEEPLABCUT\\Lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 1.0,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'mean_pixel': [123.68, 116.779, 103.939],
 'mirror': False,
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': True,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'regularize': False,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\test\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-28 22:23:39 Videos selected to analyze:
{'D:/DLC/CV-Mena-2023-02-26/videos/Grupo2R4S1.mp4'}
2023-02-28 22:23:45 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'batch_size': 1,
 'crop_pad': 0,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'imgaug',
 'deterministic': False,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\Pc-Ryzen\\anaconda3\\envs\\DEEPLABCUT\\Lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 1.0,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'mean_pixel': [123.68, 116.779, 103.939],
 'mirror': False,
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': True,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'regularize': False,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\test\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-28 22:33:31 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 1,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'imgaug',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\Pc-Ryzen\\anaconda3\\envs\\DEEPLABCUT\\Lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': 'D:/DLC/CV-Mena-2023-02-26',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-28 22:33:44 iteration: 1 loss: 1.7039 lr: 0.005
2023-02-28 22:33:45 iteration: 2 loss: 0.7124 lr: 0.005
2023-02-28 22:33:46 iteration: 3 loss: 0.4843 lr: 0.005
2023-02-28 22:33:46 iteration: 4 loss: 0.5882 lr: 0.005
2023-02-28 22:33:46 iteration: 5 loss: 0.3443 lr: 0.005
2023-02-28 22:33:47 iteration: 6 loss: 0.1510 lr: 0.005
2023-02-28 22:33:47 iteration: 7 loss: 0.2152 lr: 0.005
2023-02-28 22:33:48 iteration: 8 loss: 0.1038 lr: 0.005
2023-02-28 22:33:49 iteration: 9 loss: 0.1307 lr: 0.005
2023-02-28 22:33:49 iteration: 10 loss: 0.0949 lr: 0.005
2023-02-28 22:33:52 iteration: 11 loss: 0.1069 lr: 0.005
2023-02-28 22:33:54 iteration: 12 loss: 0.0654 lr: 0.005
2023-02-28 22:33:55 iteration: 13 loss: 0.1184 lr: 0.005
2023-02-28 22:33:55 iteration: 14 loss: 0.0546 lr: 0.005
2023-02-28 22:33:56 iteration: 15 loss: 0.0716 lr: 0.005
2023-02-28 22:33:57 iteration: 16 loss: 0.0332 lr: 0.005
2023-02-28 22:33:57 iteration: 17 loss: 0.0508 lr: 0.005
2023-02-28 22:33:57 iteration: 18 loss: 0.0518 lr: 0.005
2023-02-28 22:33:57 iteration: 19 loss: 0.0486 lr: 0.005
2023-02-28 22:33:58 iteration: 20 loss: 0.0441 lr: 0.005
2023-02-28 22:34:01 iteration: 21 loss: 0.0396 lr: 0.005
2023-02-28 22:34:02 iteration: 22 loss: 0.0483 lr: 0.005
2023-02-28 22:34:03 iteration: 23 loss: 0.0576 lr: 0.005
2023-02-28 22:34:04 iteration: 24 loss: 0.0484 lr: 0.005
2023-02-28 22:34:05 iteration: 25 loss: 0.0429 lr: 0.005
2023-02-28 22:34:05 iteration: 26 loss: 0.0347 lr: 0.005
2023-02-28 22:34:05 iteration: 27 loss: 0.0437 lr: 0.005
2023-02-28 22:34:05 iteration: 28 loss: 0.0519 lr: 0.005
2023-02-28 22:34:06 iteration: 29 loss: 0.0376 lr: 0.005
2023-02-28 22:34:06 iteration: 30 loss: 0.0247 lr: 0.005
2023-02-28 22:34:09 iteration: 31 loss: 0.0348 lr: 0.005
2023-02-28 22:34:09 iteration: 32 loss: 0.0444 lr: 0.005
2023-02-28 22:34:11 iteration: 33 loss: 0.0426 lr: 0.005
2023-02-28 22:34:11 iteration: 34 loss: 0.0404 lr: 0.005
2023-02-28 22:34:12 iteration: 35 loss: 0.0289 lr: 0.005
2023-02-28 22:34:12 iteration: 36 loss: 0.0232 lr: 0.005
2023-02-28 22:34:13 iteration: 37 loss: 0.0317 lr: 0.005
2023-02-28 22:34:13 iteration: 38 loss: 0.0313 lr: 0.005
2023-02-28 22:34:14 iteration: 39 loss: 0.0285 lr: 0.005
2023-02-28 22:34:14 iteration: 40 loss: 0.0351 lr: 0.005
2023-02-28 22:34:17 iteration: 41 loss: 0.0366 lr: 0.005
2023-02-28 22:34:18 iteration: 42 loss: 0.0277 lr: 0.005
2023-02-28 22:34:18 iteration: 43 loss: 0.0278 lr: 0.005
2023-02-28 22:34:19 iteration: 44 loss: 0.0272 lr: 0.005
2023-02-28 22:34:19 iteration: 45 loss: 0.0273 lr: 0.005
2023-02-28 22:34:20 iteration: 46 loss: 0.0266 lr: 0.005
2023-02-28 22:34:20 iteration: 47 loss: 0.0270 lr: 0.005
2023-02-28 22:34:20 iteration: 48 loss: 0.0227 lr: 0.005
2023-02-28 22:34:22 iteration: 49 loss: 0.0376 lr: 0.005
2023-02-28 22:34:22 iteration: 50 loss: 0.0201 lr: 0.005
2023-02-28 22:34:25 iteration: 51 loss: 0.0364 lr: 0.005
2023-02-28 22:34:25 iteration: 52 loss: 0.0187 lr: 0.005
2023-02-28 22:34:26 iteration: 53 loss: 0.0171 lr: 0.005
2023-02-28 22:34:27 iteration: 54 loss: 0.0359 lr: 0.005
2023-02-28 22:34:27 iteration: 55 loss: 0.0247 lr: 0.005
2023-02-28 22:34:28 iteration: 56 loss: 0.0344 lr: 0.005
2023-02-28 22:34:28 iteration: 57 loss: 0.0250 lr: 0.005
2023-02-28 22:34:29 iteration: 58 loss: 0.0219 lr: 0.005
2023-02-28 22:34:30 iteration: 59 loss: 0.0318 lr: 0.005
2023-02-28 22:34:30 iteration: 60 loss: 0.0223 lr: 0.005
2023-02-28 22:34:34 iteration: 61 loss: 0.0286 lr: 0.005
2023-02-28 22:34:42 iteration: 62 loss: 0.0306 lr: 0.005
2023-02-28 22:34:43 iteration: 63 loss: 0.0286 lr: 0.005
2023-02-28 22:34:44 iteration: 64 loss: 0.0360 lr: 0.005
2023-02-28 22:34:45 iteration: 65 loss: 0.0309 lr: 0.005
2023-02-28 22:34:46 iteration: 66 loss: 0.0314 lr: 0.005
2023-02-28 22:34:46 iteration: 67 loss: 0.0228 lr: 0.005
2023-02-28 22:34:46 iteration: 68 loss: 0.0165 lr: 0.005
2023-02-28 22:34:46 iteration: 69 loss: 0.0238 lr: 0.005
2023-02-28 22:34:46 iteration: 70 loss: 0.0273 lr: 0.005
2023-02-28 22:34:49 iteration: 71 loss: 0.0373 lr: 0.005
2023-02-28 22:34:49 iteration: 72 loss: 0.0157 lr: 0.005
2023-02-28 22:34:50 iteration: 73 loss: 0.0302 lr: 0.005
2023-02-28 22:34:50 iteration: 74 loss: 0.0277 lr: 0.005
2023-02-28 22:34:50 iteration: 75 loss: 0.0187 lr: 0.005
2023-02-28 22:34:50 iteration: 76 loss: 0.0369 lr: 0.005
2023-02-28 22:34:51 iteration: 77 loss: 0.0208 lr: 0.005
2023-02-28 22:34:51 iteration: 78 loss: 0.0207 lr: 0.005
2023-02-28 22:34:51 iteration: 79 loss: 0.0250 lr: 0.005
2023-02-28 22:34:52 iteration: 80 loss: 0.0204 lr: 0.005
2023-02-28 22:34:56 iteration: 81 loss: 0.0292 lr: 0.005
2023-02-28 22:34:56 iteration: 82 loss: 0.0364 lr: 0.005
2023-02-28 22:34:57 iteration: 83 loss: 0.0196 lr: 0.005
2023-02-28 22:34:57 iteration: 84 loss: 0.0239 lr: 0.005
2023-02-28 22:34:58 iteration: 85 loss: 0.0280 lr: 0.005
2023-02-28 22:34:58 iteration: 86 loss: 0.0222 lr: 0.005
2023-02-28 22:34:59 iteration: 87 loss: 0.0321 lr: 0.005
2023-02-28 22:35:00 iteration: 88 loss: 0.0176 lr: 0.005
2023-02-28 22:35:00 iteration: 89 loss: 0.0356 lr: 0.005
2023-02-28 22:35:00 iteration: 90 loss: 0.0203 lr: 0.005
2023-02-28 22:35:04 iteration: 91 loss: 0.0256 lr: 0.005
2023-02-28 22:35:04 iteration: 92 loss: 0.0198 lr: 0.005
2023-02-28 22:35:04 iteration: 93 loss: 0.0165 lr: 0.005
2023-02-28 22:35:04 iteration: 94 loss: 0.0150 lr: 0.005
2023-02-28 22:35:04 iteration: 95 loss: 0.0226 lr: 0.005
2023-02-28 22:35:05 iteration: 96 loss: 0.0334 lr: 0.005
2023-02-28 22:35:05 iteration: 97 loss: 0.0331 lr: 0.005
2023-02-28 22:35:06 iteration: 98 loss: 0.0311 lr: 0.005
2023-02-28 22:35:07 iteration: 99 loss: 0.0370 lr: 0.005
2023-02-28 22:35:07 iteration: 100 loss: 0.0158 lr: 0.005
2023-02-28 22:37:04 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'batch_size': 1,
 'crop_pad': 0,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'imgaug',
 'deterministic': False,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\Pc-Ryzen\\anaconda3\\envs\\DEEPLABCUT\\Lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 1.0,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'mean_pixel': [123.68, 116.779, 103.939],
 'mirror': False,
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': True,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'regularize': False,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\test\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-28 22:43:58 Use all bodyparts
2023-02-28 22:44:37 Videos selected to analyze:
{'D:/DLC/CV-Mena-2023-02-26/videos/Grupo2R4S41.mp4'}
2023-02-28 22:44:52 Save results as CSV ENABLED
2023-02-28 22:44:56 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'batch_size': 1,
 'crop_pad': 0,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'imgaug',
 'deterministic': False,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\Pc-Ryzen\\anaconda3\\envs\\DEEPLABCUT\\Lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 1.0,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'mean_pixel': [123.68, 116.779, 103.939],
 'mirror': False,
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': True,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'regularize': False,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\test\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-28 23:44:55 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 1,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'imgaug',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\Pc-Ryzen\\anaconda3\\envs\\DEEPLABCUT\\Lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': 'D:/DLC/CV-Mena-2023-02-26',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-02-28 23:46:03 iteration: 100 loss: 0.0745 lr: 0.005
2023-02-28 23:46:32 iteration: 200 loss: 0.0242 lr: 0.005
2023-02-28 23:46:54 iteration: 300 loss: 0.0223 lr: 0.005
2023-02-28 23:47:14 iteration: 400 loss: 0.0227 lr: 0.005
2023-02-28 23:47:33 iteration: 500 loss: 0.0211 lr: 0.005
2023-02-28 23:47:52 iteration: 600 loss: 0.0203 lr: 0.005
2023-02-28 23:48:07 iteration: 700 loss: 0.0189 lr: 0.005
2023-02-28 23:48:22 iteration: 800 loss: 0.0202 lr: 0.005
2023-02-28 23:48:37 iteration: 900 loss: 0.0191 lr: 0.005
2023-02-28 23:48:51 iteration: 1000 loss: 0.0193 lr: 0.005
2023-02-28 23:49:08 iteration: 1100 loss: 0.0178 lr: 0.005
2023-02-28 23:49:21 iteration: 1200 loss: 0.0190 lr: 0.005
2023-02-28 23:49:34 iteration: 1300 loss: 0.0189 lr: 0.005
2023-02-28 23:49:46 iteration: 1400 loss: 0.0181 lr: 0.005
2023-02-28 23:49:58 iteration: 1500 loss: 0.0174 lr: 0.005
2023-02-28 23:50:10 iteration: 1600 loss: 0.0168 lr: 0.005
2023-02-28 23:50:24 iteration: 1700 loss: 0.0165 lr: 0.005
2023-02-28 23:50:35 iteration: 1800 loss: 0.0174 lr: 0.005
2023-02-28 23:50:47 iteration: 1900 loss: 0.0180 lr: 0.005
2023-02-28 23:50:58 iteration: 2000 loss: 0.0163 lr: 0.005
2023-02-28 23:51:13 iteration: 2100 loss: 0.0171 lr: 0.005
2023-02-28 23:51:24 iteration: 2200 loss: 0.0160 lr: 0.005
2023-02-28 23:51:34 iteration: 2300 loss: 0.0154 lr: 0.005
2023-02-28 23:51:44 iteration: 2400 loss: 0.0162 lr: 0.005
2023-02-28 23:51:55 iteration: 2500 loss: 0.0161 lr: 0.005
2023-02-28 23:52:05 iteration: 2600 loss: 0.0159 lr: 0.005
2023-02-28 23:52:15 iteration: 2700 loss: 0.0156 lr: 0.005
2023-02-28 23:52:25 iteration: 2800 loss: 0.0153 lr: 0.005
2023-02-28 23:52:35 iteration: 2900 loss: 0.0147 lr: 0.005
2023-02-28 23:52:46 iteration: 3000 loss: 0.0160 lr: 0.005
2023-02-28 23:52:58 iteration: 3100 loss: 0.0151 lr: 0.005
2023-02-28 23:53:09 iteration: 3200 loss: 0.0148 lr: 0.005
2023-02-28 23:53:20 iteration: 3300 loss: 0.0151 lr: 0.005
2023-02-28 23:53:29 iteration: 3400 loss: 0.0144 lr: 0.005
2023-02-28 23:53:38 iteration: 3500 loss: 0.0145 lr: 0.005
2023-02-28 23:53:48 iteration: 3600 loss: 0.0147 lr: 0.005
2023-02-28 23:53:57 iteration: 3700 loss: 0.0142 lr: 0.005
2023-02-28 23:54:06 iteration: 3800 loss: 0.0143 lr: 0.005
2023-02-28 23:54:15 iteration: 3900 loss: 0.0147 lr: 0.005
2023-02-28 23:54:25 iteration: 4000 loss: 0.0147 lr: 0.005
2023-02-28 23:54:37 iteration: 4100 loss: 0.0144 lr: 0.005
2023-02-28 23:54:46 iteration: 4200 loss: 0.0141 lr: 0.005
2023-02-28 23:54:55 iteration: 4300 loss: 0.0142 lr: 0.005
2023-02-28 23:55:05 iteration: 4400 loss: 0.0133 lr: 0.005
2023-02-28 23:55:15 iteration: 4500 loss: 0.0137 lr: 0.005
2023-02-28 23:55:24 iteration: 4600 loss: 0.0133 lr: 0.005
2023-02-28 23:55:33 iteration: 4700 loss: 0.0135 lr: 0.005
2023-02-28 23:55:42 iteration: 4800 loss: 0.0130 lr: 0.005
2023-02-28 23:55:50 iteration: 4900 loss: 0.0125 lr: 0.005
2023-02-28 23:56:00 iteration: 5000 loss: 0.0129 lr: 0.005
2023-02-28 23:56:12 iteration: 5100 loss: 0.0133 lr: 0.005
2023-02-28 23:56:21 iteration: 5200 loss: 0.0134 lr: 0.005
2023-02-28 23:56:30 iteration: 5300 loss: 0.0120 lr: 0.005
2023-02-28 23:56:39 iteration: 5400 loss: 0.0121 lr: 0.005
2023-02-28 23:56:48 iteration: 5500 loss: 0.0129 lr: 0.005
2023-02-28 23:56:58 iteration: 5600 loss: 0.0131 lr: 0.005
2023-02-28 23:57:07 iteration: 5700 loss: 0.0133 lr: 0.005
2023-02-28 23:57:16 iteration: 5800 loss: 0.0121 lr: 0.005
2023-02-28 23:57:25 iteration: 5900 loss: 0.0127 lr: 0.005
2023-02-28 23:57:34 iteration: 6000 loss: 0.0133 lr: 0.005
2023-02-28 23:57:46 iteration: 6100 loss: 0.0129 lr: 0.005
2023-02-28 23:57:55 iteration: 6200 loss: 0.0128 lr: 0.005
2023-02-28 23:58:04 iteration: 6300 loss: 0.0124 lr: 0.005
2023-02-28 23:58:12 iteration: 6400 loss: 0.0126 lr: 0.005
2023-02-28 23:58:21 iteration: 6500 loss: 0.0123 lr: 0.005
2023-02-28 23:58:30 iteration: 6600 loss: 0.0126 lr: 0.005
2023-02-28 23:58:38 iteration: 6700 loss: 0.0126 lr: 0.005
2023-02-28 23:58:47 iteration: 6800 loss: 0.0124 lr: 0.005
2023-02-28 23:58:55 iteration: 6900 loss: 0.0124 lr: 0.005
2023-02-28 23:59:04 iteration: 7000 loss: 0.0127 lr: 0.005
2023-02-28 23:59:15 iteration: 7100 loss: 0.0120 lr: 0.005
2023-02-28 23:59:24 iteration: 7200 loss: 0.0125 lr: 0.005
2023-02-28 23:59:33 iteration: 7300 loss: 0.0118 lr: 0.005
2023-02-28 23:59:41 iteration: 7400 loss: 0.0121 lr: 0.005
2023-02-28 23:59:50 iteration: 7500 loss: 0.0117 lr: 0.005
2023-02-28 23:59:59 iteration: 7600 loss: 0.0118 lr: 0.005
2023-03-01 00:00:07 iteration: 7700 loss: 0.0117 lr: 0.005
2023-03-01 00:00:15 iteration: 7800 loss: 0.0111 lr: 0.005
2023-03-01 00:00:23 iteration: 7900 loss: 0.0113 lr: 0.005
2023-03-01 00:00:32 iteration: 8000 loss: 0.0119 lr: 0.005
2023-03-01 00:00:42 iteration: 8100 loss: 0.0110 lr: 0.005
2023-03-01 00:00:51 iteration: 8200 loss: 0.0120 lr: 0.005
2023-03-01 00:00:59 iteration: 8300 loss: 0.0117 lr: 0.005
2023-03-01 00:01:08 iteration: 8400 loss: 0.0110 lr: 0.005
2023-03-01 00:01:17 iteration: 8500 loss: 0.0117 lr: 0.005
2023-03-01 00:01:25 iteration: 8600 loss: 0.0112 lr: 0.005
2023-03-01 00:01:34 iteration: 8700 loss: 0.0114 lr: 0.005
2023-03-01 00:01:42 iteration: 8800 loss: 0.0115 lr: 0.005
2023-03-01 00:01:51 iteration: 8900 loss: 0.0109 lr: 0.005
2023-03-01 00:01:59 iteration: 9000 loss: 0.0109 lr: 0.005
2023-03-01 00:02:10 iteration: 9100 loss: 0.0117 lr: 0.005
2023-03-01 00:02:18 iteration: 9200 loss: 0.0110 lr: 0.005
2023-03-01 00:02:28 iteration: 9300 loss: 0.0109 lr: 0.005
2023-03-01 00:02:36 iteration: 9400 loss: 0.0125 lr: 0.005
2023-03-01 00:02:44 iteration: 9500 loss: 0.0100 lr: 0.005
2023-03-01 00:02:53 iteration: 9600 loss: 0.0113 lr: 0.005
2023-03-01 00:02:59 iteration: 9700 loss: 0.0112 lr: 0.005
2023-03-01 00:11:43 iteration: 9800 loss: 0.0104 lr: 0.005
2023-03-01 00:11:52 iteration: 9900 loss: 0.0112 lr: 0.005
2023-03-01 00:12:00 iteration: 10000 loss: 0.0107 lr: 0.005
2023-03-01 00:12:11 iteration: 10100 loss: 0.0118 lr: 0.02
2023-03-01 00:12:19 iteration: 10200 loss: 0.0120 lr: 0.02
2023-03-01 00:12:27 iteration: 10300 loss: 0.0127 lr: 0.02
2023-03-01 00:12:35 iteration: 10400 loss: 0.0142 lr: 0.02
2023-03-01 00:12:43 iteration: 10500 loss: 0.0138 lr: 0.02
2023-03-01 00:12:51 iteration: 10600 loss: 0.0118 lr: 0.02
2023-03-01 00:12:58 iteration: 10700 loss: 0.0117 lr: 0.02
2023-03-01 00:13:07 iteration: 10800 loss: 0.0116 lr: 0.02
2023-03-01 00:13:14 iteration: 10900 loss: 0.0115 lr: 0.02
2023-03-01 00:13:23 iteration: 11000 loss: 0.0107 lr: 0.02
2023-03-01 00:13:35 iteration: 11100 loss: 0.0110 lr: 0.02
2023-03-01 00:13:43 iteration: 11200 loss: 0.0120 lr: 0.02
2023-03-01 00:13:50 iteration: 11300 loss: 0.0105 lr: 0.02
2023-03-01 00:13:58 iteration: 11400 loss: 0.0109 lr: 0.02
2023-03-01 00:14:06 iteration: 11500 loss: 0.0107 lr: 0.02
2023-03-01 00:14:13 iteration: 11600 loss: 0.0103 lr: 0.02
2023-03-01 00:14:21 iteration: 11700 loss: 0.0103 lr: 0.02
2023-03-01 00:14:29 iteration: 11800 loss: 0.0113 lr: 0.02
2023-03-01 00:14:36 iteration: 11900 loss: 0.0098 lr: 0.02
2023-03-01 00:14:43 iteration: 12000 loss: 0.0101 lr: 0.02
2023-03-01 00:14:54 iteration: 12100 loss: 0.0114 lr: 0.02
2023-03-01 00:15:02 iteration: 12200 loss: 0.0103 lr: 0.02
2023-03-01 00:15:10 iteration: 12300 loss: 0.0101 lr: 0.02
2023-03-01 00:15:18 iteration: 12400 loss: 0.0109 lr: 0.02
2023-03-01 00:15:26 iteration: 12500 loss: 0.0103 lr: 0.02
2023-03-01 00:15:34 iteration: 12600 loss: 0.0096 lr: 0.02
2023-03-01 00:15:43 iteration: 12700 loss: 0.0098 lr: 0.02
2023-03-01 00:15:50 iteration: 12800 loss: 0.0094 lr: 0.02
2023-03-01 00:15:58 iteration: 12900 loss: 0.0094 lr: 0.02
2023-03-01 00:16:06 iteration: 13000 loss: 0.0101 lr: 0.02
2023-03-01 00:16:17 iteration: 13100 loss: 0.0097 lr: 0.02
2023-03-01 00:16:25 iteration: 13200 loss: 0.0096 lr: 0.02
2023-03-01 00:16:32 iteration: 13300 loss: 0.0095 lr: 0.02
2023-03-01 00:16:40 iteration: 13400 loss: 0.0089 lr: 0.02
2023-03-01 00:16:48 iteration: 13500 loss: 0.0090 lr: 0.02
2023-03-01 00:16:56 iteration: 13600 loss: 0.0093 lr: 0.02
2023-03-01 00:17:04 iteration: 13700 loss: 0.0092 lr: 0.02
2023-03-01 00:17:11 iteration: 13800 loss: 0.0091 lr: 0.02
2023-03-01 00:17:19 iteration: 13900 loss: 0.0087 lr: 0.02
2023-03-01 00:17:27 iteration: 14000 loss: 0.0090 lr: 0.02
2023-03-01 00:17:39 iteration: 14100 loss: 0.0099 lr: 0.02
2023-03-01 00:17:47 iteration: 14200 loss: 0.0092 lr: 0.02
2023-03-01 00:17:55 iteration: 14300 loss: 0.0089 lr: 0.02
2023-03-01 00:18:03 iteration: 14400 loss: 0.0089 lr: 0.02
2023-03-01 00:18:11 iteration: 14500 loss: 0.0084 lr: 0.02
2023-03-01 00:18:19 iteration: 14600 loss: 0.0083 lr: 0.02
2023-03-01 00:18:27 iteration: 14700 loss: 0.0090 lr: 0.02
2023-03-01 00:18:34 iteration: 14800 loss: 0.0090 lr: 0.02
2023-03-01 00:18:42 iteration: 14900 loss: 0.0085 lr: 0.02
2023-03-01 00:18:50 iteration: 15000 loss: 0.0087 lr: 0.02
2023-03-01 00:19:02 iteration: 15100 loss: 0.0087 lr: 0.02
2023-03-01 00:19:10 iteration: 15200 loss: 0.0088 lr: 0.02
2023-03-01 00:19:17 iteration: 15300 loss: 0.0090 lr: 0.02
2023-03-01 00:19:25 iteration: 15400 loss: 0.0094 lr: 0.02
2023-03-01 00:19:33 iteration: 15500 loss: 0.0091 lr: 0.02
2023-03-01 00:19:42 iteration: 15600 loss: 0.0095 lr: 0.02
2023-03-01 00:19:49 iteration: 15700 loss: 0.0086 lr: 0.02
2023-03-01 00:19:57 iteration: 15800 loss: 0.0088 lr: 0.02
2023-03-01 00:20:05 iteration: 15900 loss: 0.0083 lr: 0.02
2023-03-01 00:20:13 iteration: 16000 loss: 0.0084 lr: 0.02
2023-03-01 00:20:24 iteration: 16100 loss: 0.0084 lr: 0.02
2023-03-01 00:20:32 iteration: 16200 loss: 0.0082 lr: 0.02
2023-03-01 00:20:39 iteration: 16300 loss: 0.0074 lr: 0.02
2023-03-01 00:20:47 iteration: 16400 loss: 0.0080 lr: 0.02
2023-03-01 00:20:54 iteration: 16500 loss: 0.0078 lr: 0.02
2023-03-01 00:21:02 iteration: 16600 loss: 0.0084 lr: 0.02
2023-03-01 00:21:10 iteration: 16700 loss: 0.0077 lr: 0.02
2023-03-01 00:21:17 iteration: 16800 loss: 0.0080 lr: 0.02
2023-03-01 00:21:25 iteration: 16900 loss: 0.0075 lr: 0.02
2023-03-01 00:21:32 iteration: 17000 loss: 0.0083 lr: 0.02
2023-03-01 00:21:42 iteration: 17100 loss: 0.0078 lr: 0.02
2023-03-01 00:21:50 iteration: 17200 loss: 0.0076 lr: 0.02
2023-03-01 00:21:58 iteration: 17300 loss: 0.0079 lr: 0.02
2023-03-01 00:22:05 iteration: 17400 loss: 0.0077 lr: 0.02
2023-03-01 00:22:13 iteration: 17500 loss: 0.0077 lr: 0.02
2023-03-01 00:22:20 iteration: 17600 loss: 0.0073 lr: 0.02
2023-03-01 00:22:28 iteration: 17700 loss: 0.0079 lr: 0.02
2023-03-01 00:22:35 iteration: 17800 loss: 0.0080 lr: 0.02
2023-03-01 00:22:43 iteration: 17900 loss: 0.0084 lr: 0.02
2023-03-01 00:22:51 iteration: 18000 loss: 0.0078 lr: 0.02
2023-03-01 00:23:01 iteration: 18100 loss: 0.0071 lr: 0.02
2023-03-01 00:23:08 iteration: 18200 loss: 0.0075 lr: 0.02
2023-03-01 00:23:16 iteration: 18300 loss: 0.0079 lr: 0.02
2023-03-01 00:23:24 iteration: 18400 loss: 0.0072 lr: 0.02
2023-03-01 00:23:31 iteration: 18500 loss: 0.0076 lr: 0.02
2023-03-01 00:23:39 iteration: 18600 loss: 0.0074 lr: 0.02
2023-03-01 00:23:47 iteration: 18700 loss: 0.0075 lr: 0.02
2023-03-01 00:23:54 iteration: 18800 loss: 0.0076 lr: 0.02
2023-03-01 00:24:02 iteration: 18900 loss: 0.0072 lr: 0.02
2023-03-01 00:24:09 iteration: 19000 loss: 0.0073 lr: 0.02
2023-03-01 00:24:20 iteration: 19100 loss: 0.0074 lr: 0.02
2023-03-01 00:24:28 iteration: 19200 loss: 0.0074 lr: 0.02
2023-03-01 00:24:36 iteration: 19300 loss: 0.0068 lr: 0.02
2023-03-01 00:24:43 iteration: 19400 loss: 0.0065 lr: 0.02
2023-03-01 00:24:51 iteration: 19500 loss: 0.0068 lr: 0.02
2023-03-01 00:24:58 iteration: 19600 loss: 0.0071 lr: 0.02
2023-03-01 00:25:05 iteration: 19700 loss: 0.0068 lr: 0.02
2023-03-01 00:25:13 iteration: 19800 loss: 0.0070 lr: 0.02
2023-03-01 00:25:20 iteration: 19900 loss: 0.0070 lr: 0.02
2023-03-01 00:25:28 iteration: 20000 loss: 0.0069 lr: 0.02
2023-03-01 00:25:39 iteration: 20100 loss: 0.0068 lr: 0.02
2023-03-01 00:25:46 iteration: 20200 loss: 0.0070 lr: 0.02
2023-03-01 00:25:54 iteration: 20300 loss: 0.0069 lr: 0.02
2023-03-01 00:26:01 iteration: 20400 loss: 0.0067 lr: 0.02
2023-03-01 00:26:09 iteration: 20500 loss: 0.0072 lr: 0.02
2023-03-01 00:26:16 iteration: 20600 loss: 0.0064 lr: 0.02
2023-03-01 00:26:24 iteration: 20700 loss: 0.0064 lr: 0.02
2023-03-01 00:26:31 iteration: 20800 loss: 0.0070 lr: 0.02
2023-03-01 00:26:38 iteration: 20900 loss: 0.0068 lr: 0.02
2023-03-01 00:26:46 iteration: 21000 loss: 0.0068 lr: 0.02
2023-03-01 00:26:57 iteration: 21100 loss: 0.0064 lr: 0.02
2023-03-01 00:27:05 iteration: 21200 loss: 0.0066 lr: 0.02
2023-03-01 00:27:13 iteration: 21300 loss: 0.0062 lr: 0.02
2023-03-01 00:27:20 iteration: 21400 loss: 0.0067 lr: 0.02
2023-03-01 00:27:28 iteration: 21500 loss: 0.0069 lr: 0.02
2023-03-01 00:27:35 iteration: 21600 loss: 0.0065 lr: 0.02
2023-03-01 00:27:43 iteration: 21700 loss: 0.0068 lr: 0.02
2023-03-01 00:27:50 iteration: 21800 loss: 0.0063 lr: 0.02
2023-03-01 00:27:57 iteration: 21900 loss: 0.0063 lr: 0.02
2023-03-01 00:28:05 iteration: 22000 loss: 0.0065 lr: 0.02
2023-03-01 00:28:15 iteration: 22100 loss: 0.0065 lr: 0.02
2023-03-01 00:28:23 iteration: 22200 loss: 0.0066 lr: 0.02
2023-03-01 00:28:30 iteration: 22300 loss: 0.0067 lr: 0.02
2023-03-01 00:28:38 iteration: 22400 loss: 0.0063 lr: 0.02
2023-03-01 00:28:46 iteration: 22500 loss: 0.0063 lr: 0.02
2023-03-01 00:28:53 iteration: 22600 loss: 0.0059 lr: 0.02
2023-03-01 00:29:01 iteration: 22700 loss: 0.0069 lr: 0.02
2023-03-01 00:29:08 iteration: 22800 loss: 0.0062 lr: 0.02
2023-03-01 00:29:15 iteration: 22900 loss: 0.0066 lr: 0.02
2023-03-01 00:29:23 iteration: 23000 loss: 0.0065 lr: 0.02
2023-03-01 00:29:33 iteration: 23100 loss: 0.0064 lr: 0.02
2023-03-01 00:29:41 iteration: 23200 loss: 0.0067 lr: 0.02
2023-03-01 00:29:48 iteration: 23300 loss: 0.0061 lr: 0.02
2023-03-01 00:29:56 iteration: 23400 loss: 0.0057 lr: 0.02
2023-03-01 00:30:03 iteration: 23500 loss: 0.0062 lr: 0.02
2023-03-01 00:30:10 iteration: 23600 loss: 0.0064 lr: 0.02
2023-03-01 00:30:18 iteration: 23700 loss: 0.0060 lr: 0.02
2023-03-01 00:30:25 iteration: 23800 loss: 0.0062 lr: 0.02
2023-03-01 00:30:33 iteration: 23900 loss: 0.0059 lr: 0.02
2023-03-01 00:30:40 iteration: 24000 loss: 0.0058 lr: 0.02
2023-03-01 00:30:51 iteration: 24100 loss: 0.0058 lr: 0.02
2023-03-01 00:30:58 iteration: 24200 loss: 0.0058 lr: 0.02
2023-03-01 00:31:06 iteration: 24300 loss: 0.0062 lr: 0.02
2023-03-01 00:31:13 iteration: 24400 loss: 0.0061 lr: 0.02
2023-03-01 00:31:20 iteration: 24500 loss: 0.0060 lr: 0.02
2023-03-01 00:31:28 iteration: 24600 loss: 0.0063 lr: 0.02
2023-03-01 00:31:35 iteration: 24700 loss: 0.0063 lr: 0.02
2023-03-01 00:31:42 iteration: 24800 loss: 0.0059 lr: 0.02
2023-03-01 00:31:50 iteration: 24900 loss: 0.0061 lr: 0.02
2023-03-01 00:31:57 iteration: 25000 loss: 0.0060 lr: 0.02
2023-03-01 00:32:08 iteration: 25100 loss: 0.0055 lr: 0.02
2023-03-01 00:32:16 iteration: 25200 loss: 0.0065 lr: 0.02
2023-03-01 00:32:23 iteration: 25300 loss: 0.0060 lr: 0.02
2023-03-01 00:32:31 iteration: 25400 loss: 0.0058 lr: 0.02
2023-03-01 00:32:38 iteration: 25500 loss: 0.0063 lr: 0.02
2023-03-01 00:32:46 iteration: 25600 loss: 0.0061 lr: 0.02
2023-03-01 00:32:53 iteration: 25700 loss: 0.0055 lr: 0.02
2023-03-01 00:33:00 iteration: 25800 loss: 0.0056 lr: 0.02
2023-03-01 00:33:08 iteration: 25900 loss: 0.0057 lr: 0.02
2023-03-01 00:33:16 iteration: 26000 loss: 0.0060 lr: 0.02
2023-03-01 00:33:26 iteration: 26100 loss: 0.0061 lr: 0.02
2023-03-01 00:33:34 iteration: 26200 loss: 0.0057 lr: 0.02
2023-03-01 00:33:42 iteration: 26300 loss: 0.0057 lr: 0.02
2023-03-01 00:33:49 iteration: 26400 loss: 0.0060 lr: 0.02
2023-03-01 00:33:57 iteration: 26500 loss: 0.0057 lr: 0.02
2023-03-01 00:34:04 iteration: 26600 loss: 0.0060 lr: 0.02
2023-03-01 00:34:12 iteration: 26700 loss: 0.0053 lr: 0.02
2023-03-01 00:34:19 iteration: 26800 loss: 0.0056 lr: 0.02
2023-03-01 00:34:27 iteration: 26900 loss: 0.0058 lr: 0.02
2023-03-01 00:34:34 iteration: 27000 loss: 0.0055 lr: 0.02
2023-03-01 00:34:45 iteration: 27100 loss: 0.0060 lr: 0.02
2023-03-01 00:34:53 iteration: 27200 loss: 0.0060 lr: 0.02
2023-03-01 00:35:01 iteration: 27300 loss: 0.0056 lr: 0.02
2023-03-01 00:35:08 iteration: 27400 loss: 0.0053 lr: 0.02
2023-03-01 00:35:15 iteration: 27500 loss: 0.0056 lr: 0.02
2023-03-01 00:35:23 iteration: 27600 loss: 0.0062 lr: 0.02
2023-03-01 00:35:30 iteration: 27700 loss: 0.0059 lr: 0.02
2023-03-01 00:35:38 iteration: 27800 loss: 0.0054 lr: 0.02
2023-03-01 00:35:46 iteration: 27900 loss: 0.0061 lr: 0.02
2023-03-01 00:35:54 iteration: 28000 loss: 0.0053 lr: 0.02
2023-03-01 00:36:04 iteration: 28100 loss: 0.0051 lr: 0.02
2023-03-01 00:36:12 iteration: 28200 loss: 0.0055 lr: 0.02
2023-03-01 00:36:19 iteration: 28300 loss: 0.0053 lr: 0.02
2023-03-01 00:36:27 iteration: 28400 loss: 0.0058 lr: 0.02
2023-03-01 00:36:34 iteration: 28500 loss: 0.0052 lr: 0.02
2023-03-01 00:36:41 iteration: 28600 loss: 0.0050 lr: 0.02
2023-03-01 00:36:49 iteration: 28700 loss: 0.0053 lr: 0.02
2023-03-01 00:36:56 iteration: 28800 loss: 0.0062 lr: 0.02
2023-03-01 00:37:04 iteration: 28900 loss: 0.0053 lr: 0.02
2023-03-01 00:37:11 iteration: 29000 loss: 0.0054 lr: 0.02
2023-03-01 00:37:22 iteration: 29100 loss: 0.0058 lr: 0.02
2023-03-01 00:37:29 iteration: 29200 loss: 0.0051 lr: 0.02
2023-03-01 00:37:37 iteration: 29300 loss: 0.0056 lr: 0.02
2023-03-01 00:37:44 iteration: 29400 loss: 0.0054 lr: 0.02
2023-03-01 00:37:52 iteration: 29500 loss: 0.0054 lr: 0.02
2023-03-01 00:38:00 iteration: 29600 loss: 0.0053 lr: 0.02
2023-03-01 00:38:07 iteration: 29700 loss: 0.0052 lr: 0.02
2023-03-01 00:38:15 iteration: 29800 loss: 0.0055 lr: 0.02
2023-03-01 00:38:22 iteration: 29900 loss: 0.0053 lr: 0.02
2023-03-01 00:38:29 iteration: 30000 loss: 0.0051 lr: 0.02
2023-03-01 00:38:40 iteration: 30100 loss: 0.0056 lr: 0.02
2023-03-01 00:38:47 iteration: 30200 loss: 0.0053 lr: 0.02
2023-03-01 00:38:55 iteration: 30300 loss: 0.0051 lr: 0.02
2023-03-01 00:39:02 iteration: 30400 loss: 0.0061 lr: 0.02
2023-03-01 00:39:10 iteration: 30500 loss: 0.0054 lr: 0.02
2023-03-01 00:39:17 iteration: 30600 loss: 0.0053 lr: 0.02
2023-03-01 00:39:25 iteration: 30700 loss: 0.0057 lr: 0.02
2023-03-01 00:39:32 iteration: 30800 loss: 0.0059 lr: 0.02
2023-03-01 00:39:41 iteration: 30900 loss: 0.0055 lr: 0.02
2023-03-01 00:39:48 iteration: 31000 loss: 0.0054 lr: 0.02
2023-03-01 00:39:59 iteration: 31100 loss: 0.0056 lr: 0.02
2023-03-01 00:40:07 iteration: 31200 loss: 0.0050 lr: 0.02
2023-03-01 00:40:14 iteration: 31300 loss: 0.0051 lr: 0.02
2023-03-01 00:40:21 iteration: 31400 loss: 0.0053 lr: 0.02
2023-03-01 00:40:29 iteration: 31500 loss: 0.0055 lr: 0.02
2023-03-01 00:40:36 iteration: 31600 loss: 0.0057 lr: 0.02
2023-03-01 00:40:43 iteration: 31700 loss: 0.0054 lr: 0.02
2023-03-01 00:40:51 iteration: 31800 loss: 0.0053 lr: 0.02
2023-03-01 00:40:58 iteration: 31900 loss: 0.0052 lr: 0.02
2023-03-01 00:41:05 iteration: 32000 loss: 0.0053 lr: 0.02
2023-03-01 00:41:16 iteration: 32100 loss: 0.0051 lr: 0.02
2023-03-01 00:41:23 iteration: 32200 loss: 0.0053 lr: 0.02
2023-03-01 00:41:30 iteration: 32300 loss: 0.0053 lr: 0.02
2023-03-01 00:41:38 iteration: 32400 loss: 0.0052 lr: 0.02
2023-03-01 00:41:45 iteration: 32500 loss: 0.0051 lr: 0.02
2023-03-01 00:41:53 iteration: 32600 loss: 0.0052 lr: 0.02
2023-03-01 00:42:00 iteration: 32700 loss: 0.0051 lr: 0.02
2023-03-01 00:42:08 iteration: 32800 loss: 0.0051 lr: 0.02
2023-03-01 00:42:15 iteration: 32900 loss: 0.0049 lr: 0.02
2023-03-01 00:42:23 iteration: 33000 loss: 0.0051 lr: 0.02
2023-03-01 00:42:34 iteration: 33100 loss: 0.0049 lr: 0.02
2023-03-01 00:42:42 iteration: 33200 loss: 0.0050 lr: 0.02
2023-03-01 00:42:49 iteration: 33300 loss: 0.0050 lr: 0.02
2023-03-01 00:42:56 iteration: 33400 loss: 0.0045 lr: 0.02
2023-03-01 00:43:04 iteration: 33500 loss: 0.0049 lr: 0.02
2023-03-01 00:43:11 iteration: 33600 loss: 0.0048 lr: 0.02
2023-03-01 00:43:18 iteration: 33700 loss: 0.0052 lr: 0.02
2023-03-01 00:43:26 iteration: 33800 loss: 0.0051 lr: 0.02
2023-03-01 00:43:34 iteration: 33900 loss: 0.0048 lr: 0.02
2023-03-01 00:43:42 iteration: 34000 loss: 0.0048 lr: 0.02
2023-03-01 00:43:53 iteration: 34100 loss: 0.0049 lr: 0.02
2023-03-01 00:44:00 iteration: 34200 loss: 0.0045 lr: 0.02
2023-03-01 00:44:08 iteration: 34300 loss: 0.0052 lr: 0.02
2023-03-01 00:44:15 iteration: 34400 loss: 0.0046 lr: 0.02
2023-03-01 00:44:22 iteration: 34500 loss: 0.0048 lr: 0.02
2023-03-01 00:44:29 iteration: 34600 loss: 0.0052 lr: 0.02
2023-03-01 00:44:37 iteration: 34700 loss: 0.0048 lr: 0.02
2023-03-01 00:44:44 iteration: 34800 loss: 0.0051 lr: 0.02
2023-03-01 00:44:52 iteration: 34900 loss: 0.0051 lr: 0.02
2023-03-01 00:45:00 iteration: 35000 loss: 0.0051 lr: 0.02
2023-03-01 00:45:10 iteration: 35100 loss: 0.0049 lr: 0.02
2023-03-01 00:45:18 iteration: 35200 loss: 0.0048 lr: 0.02
2023-03-01 00:45:25 iteration: 35300 loss: 0.0047 lr: 0.02
2023-03-01 00:45:32 iteration: 35400 loss: 0.0054 lr: 0.02
2023-03-01 00:45:40 iteration: 35500 loss: 0.0049 lr: 0.02
2023-03-01 00:45:47 iteration: 35600 loss: 0.0048 lr: 0.02
2023-03-01 00:45:54 iteration: 35700 loss: 0.0045 lr: 0.02
2023-03-01 00:46:02 iteration: 35800 loss: 0.0046 lr: 0.02
2023-03-01 00:46:09 iteration: 35900 loss: 0.0047 lr: 0.02
2023-03-01 00:46:17 iteration: 36000 loss: 0.0043 lr: 0.02
2023-03-01 00:46:27 iteration: 36100 loss: 0.0046 lr: 0.02
2023-03-01 00:46:34 iteration: 36200 loss: 0.0045 lr: 0.02
2023-03-01 00:46:42 iteration: 36300 loss: 0.0047 lr: 0.02
2023-03-01 00:46:49 iteration: 36400 loss: 0.0048 lr: 0.02
2023-03-01 00:46:57 iteration: 36500 loss: 0.0047 lr: 0.02
2023-03-01 00:47:05 iteration: 36600 loss: 0.0046 lr: 0.02
2023-03-01 00:47:12 iteration: 36700 loss: 0.0048 lr: 0.02
2023-03-01 00:47:19 iteration: 36800 loss: 0.0050 lr: 0.02
2023-03-01 00:47:26 iteration: 36900 loss: 0.0045 lr: 0.02
2023-03-01 00:47:34 iteration: 37000 loss: 0.0046 lr: 0.02
2023-03-01 00:47:44 iteration: 37100 loss: 0.0047 lr: 0.02
2023-03-01 00:47:52 iteration: 37200 loss: 0.0049 lr: 0.02
2023-03-01 00:48:00 iteration: 37300 loss: 0.0046 lr: 0.02
2023-03-01 00:48:07 iteration: 37400 loss: 0.0048 lr: 0.02
2023-03-01 00:48:14 iteration: 37500 loss: 0.0048 lr: 0.02
2023-03-01 00:48:22 iteration: 37600 loss: 0.0045 lr: 0.02
2023-03-01 00:48:29 iteration: 37700 loss: 0.0045 lr: 0.02
2023-03-01 00:48:36 iteration: 37800 loss: 0.0050 lr: 0.02
2023-03-01 00:48:44 iteration: 37900 loss: 0.0048 lr: 0.02
2023-03-01 00:48:51 iteration: 38000 loss: 0.0045 lr: 0.02
2023-03-01 00:49:02 iteration: 38100 loss: 0.0048 lr: 0.02
2023-03-01 00:49:09 iteration: 38200 loss: 0.0048 lr: 0.02
2023-03-01 00:49:17 iteration: 38300 loss: 0.0042 lr: 0.02
2023-03-01 00:49:24 iteration: 38400 loss: 0.0045 lr: 0.02
2023-03-01 00:49:32 iteration: 38500 loss: 0.0049 lr: 0.02
2023-03-01 00:49:40 iteration: 38600 loss: 0.0049 lr: 0.02
2023-03-01 00:49:47 iteration: 38700 loss: 0.0043 lr: 0.02
2023-03-01 00:49:55 iteration: 38800 loss: 0.0047 lr: 0.02
2023-03-01 00:50:02 iteration: 38900 loss: 0.0043 lr: 0.02
2023-03-01 00:50:10 iteration: 39000 loss: 0.0045 lr: 0.02
2023-03-01 00:50:20 iteration: 39100 loss: 0.0047 lr: 0.02
2023-03-01 00:50:28 iteration: 39200 loss: 0.0044 lr: 0.02
2023-03-01 00:50:35 iteration: 39300 loss: 0.0044 lr: 0.02
2023-03-01 00:50:43 iteration: 39400 loss: 0.0049 lr: 0.02
2023-03-01 00:50:50 iteration: 39500 loss: 0.0045 lr: 0.02
2023-03-01 00:50:58 iteration: 39600 loss: 0.0043 lr: 0.02
2023-03-01 00:51:05 iteration: 39700 loss: 0.0048 lr: 0.02
2023-03-01 00:51:12 iteration: 39800 loss: 0.0043 lr: 0.02
2023-03-01 00:51:20 iteration: 39900 loss: 0.0045 lr: 0.02
2023-03-01 00:51:27 iteration: 40000 loss: 0.0044 lr: 0.02
2023-03-01 00:51:37 iteration: 40100 loss: 0.0045 lr: 0.02
2023-03-01 00:51:45 iteration: 40200 loss: 0.0047 lr: 0.02
2023-03-01 00:51:53 iteration: 40300 loss: 0.0050 lr: 0.02
2023-03-01 00:52:00 iteration: 40400 loss: 0.0042 lr: 0.02
2023-03-01 00:52:07 iteration: 40500 loss: 0.0045 lr: 0.02
2023-03-01 00:52:15 iteration: 40600 loss: 0.0047 lr: 0.02
2023-03-01 00:52:22 iteration: 40700 loss: 0.0047 lr: 0.02
2023-03-01 00:52:30 iteration: 40800 loss: 0.0047 lr: 0.02
2023-03-01 00:52:37 iteration: 40900 loss: 0.0045 lr: 0.02
2023-03-01 00:52:44 iteration: 41000 loss: 0.0046 lr: 0.02
2023-03-01 00:52:55 iteration: 41100 loss: 0.0045 lr: 0.02
2023-03-01 00:53:02 iteration: 41200 loss: 0.0044 lr: 0.02
2023-03-01 00:53:10 iteration: 41300 loss: 0.0045 lr: 0.02
2023-03-01 00:53:17 iteration: 41400 loss: 0.0048 lr: 0.02
2023-03-01 00:53:25 iteration: 41500 loss: 0.0046 lr: 0.02
2023-03-01 00:53:33 iteration: 41600 loss: 0.0045 lr: 0.02
2023-03-01 00:53:42 iteration: 41700 loss: 0.0044 lr: 0.02
2023-03-01 00:53:49 iteration: 41800 loss: 0.0045 lr: 0.02
2023-03-01 00:53:57 iteration: 41900 loss: 0.0043 lr: 0.02
2023-03-01 00:54:04 iteration: 42000 loss: 0.0044 lr: 0.02
2023-03-01 00:54:15 iteration: 42100 loss: 0.0044 lr: 0.02
2023-03-01 00:54:22 iteration: 42200 loss: 0.0045 lr: 0.02
2023-03-01 00:54:30 iteration: 42300 loss: 0.0050 lr: 0.02
2023-03-01 00:54:37 iteration: 42400 loss: 0.0045 lr: 0.02
2023-03-01 00:54:45 iteration: 42500 loss: 0.0042 lr: 0.02
2023-03-01 00:54:52 iteration: 42600 loss: 0.0044 lr: 0.02
2023-03-01 00:54:59 iteration: 42700 loss: 0.0044 lr: 0.02
2023-03-01 00:55:06 iteration: 42800 loss: 0.0046 lr: 0.02
2023-03-01 00:55:14 iteration: 42900 loss: 0.0041 lr: 0.02
2023-03-01 00:55:21 iteration: 43000 loss: 0.0045 lr: 0.02
2023-03-01 00:55:32 iteration: 43100 loss: 0.0044 lr: 0.02
2023-03-01 00:55:39 iteration: 43200 loss: 0.0044 lr: 0.02
2023-03-01 00:55:47 iteration: 43300 loss: 0.0040 lr: 0.02
2023-03-01 00:55:54 iteration: 43400 loss: 0.0046 lr: 0.02
2023-03-01 00:56:02 iteration: 43500 loss: 0.0043 lr: 0.02
2023-03-01 00:56:09 iteration: 43600 loss: 0.0043 lr: 0.02
2023-03-01 00:56:16 iteration: 43700 loss: 0.0042 lr: 0.02
2023-03-01 00:56:24 iteration: 43800 loss: 0.0043 lr: 0.02
2023-03-01 00:56:31 iteration: 43900 loss: 0.0047 lr: 0.02
2023-03-01 00:56:39 iteration: 44000 loss: 0.0048 lr: 0.02
2023-03-01 00:56:49 iteration: 44100 loss: 0.0044 lr: 0.02
2023-03-01 00:56:57 iteration: 44200 loss: 0.0046 lr: 0.02
2023-03-01 00:57:04 iteration: 44300 loss: 0.0040 lr: 0.02
2023-03-01 00:57:12 iteration: 44400 loss: 0.0040 lr: 0.02
2023-03-01 00:57:19 iteration: 44500 loss: 0.0042 lr: 0.02
2023-03-01 00:57:26 iteration: 44600 loss: 0.0043 lr: 0.02
2023-03-01 00:57:34 iteration: 44700 loss: 0.0043 lr: 0.02
2023-03-01 00:57:41 iteration: 44800 loss: 0.0041 lr: 0.02
2023-03-01 00:57:49 iteration: 44900 loss: 0.0045 lr: 0.02
2023-03-01 00:57:56 iteration: 45000 loss: 0.0050 lr: 0.02
2023-03-01 00:58:07 iteration: 45100 loss: 0.0045 lr: 0.02
2023-03-01 00:58:14 iteration: 45200 loss: 0.0044 lr: 0.02
2023-03-01 00:58:22 iteration: 45300 loss: 0.0043 lr: 0.02
2023-03-01 00:58:29 iteration: 45400 loss: 0.0052 lr: 0.02
2023-03-01 00:58:37 iteration: 45500 loss: 0.0045 lr: 0.02
2023-03-01 00:58:44 iteration: 45600 loss: 0.0042 lr: 0.02
2023-03-01 00:58:51 iteration: 45700 loss: 0.0039 lr: 0.02
2023-03-01 00:58:59 iteration: 45800 loss: 0.0043 lr: 0.02
2023-03-01 00:59:06 iteration: 45900 loss: 0.0046 lr: 0.02
2023-03-01 00:59:14 iteration: 46000 loss: 0.0039 lr: 0.02
2023-03-01 00:59:24 iteration: 46100 loss: 0.0043 lr: 0.02
2023-03-01 00:59:31 iteration: 46200 loss: 0.0039 lr: 0.02
2023-03-01 00:59:39 iteration: 46300 loss: 0.0044 lr: 0.02
2023-03-01 00:59:46 iteration: 46400 loss: 0.0039 lr: 0.02
2023-03-01 00:59:54 iteration: 46500 loss: 0.0041 lr: 0.02
2023-03-01 01:00:01 iteration: 46600 loss: 0.0042 lr: 0.02
2023-03-01 01:00:09 iteration: 46700 loss: 0.0039 lr: 0.02
2023-03-01 01:00:16 iteration: 46800 loss: 0.0041 lr: 0.02
2023-03-01 01:00:23 iteration: 46900 loss: 0.0040 lr: 0.02
2023-03-01 01:00:31 iteration: 47000 loss: 0.0044 lr: 0.02
2023-03-01 01:00:41 iteration: 47100 loss: 0.0044 lr: 0.02
2023-03-01 01:00:49 iteration: 47200 loss: 0.0042 lr: 0.02
2023-03-01 01:00:56 iteration: 47300 loss: 0.0040 lr: 0.02
2023-03-01 01:01:03 iteration: 47400 loss: 0.0044 lr: 0.02
2023-03-01 01:01:11 iteration: 47500 loss: 0.0041 lr: 0.02
2023-03-01 01:01:18 iteration: 47600 loss: 0.0042 lr: 0.02
2023-03-01 01:01:26 iteration: 47700 loss: 0.0042 lr: 0.02
2023-03-01 01:01:33 iteration: 47800 loss: 0.0041 lr: 0.02
2023-03-01 01:01:42 iteration: 47900 loss: 0.0042 lr: 0.02
2023-03-01 01:01:49 iteration: 48000 loss: 0.0041 lr: 0.02
2023-03-01 01:01:59 iteration: 48100 loss: 0.0041 lr: 0.02
2023-03-01 01:02:07 iteration: 48200 loss: 0.0037 lr: 0.02
2023-03-01 01:02:14 iteration: 48300 loss: 0.0038 lr: 0.02
2023-03-01 01:02:22 iteration: 48400 loss: 0.0041 lr: 0.02
2023-03-01 01:02:29 iteration: 48500 loss: 0.0043 lr: 0.02
2023-03-01 01:02:36 iteration: 48600 loss: 0.0040 lr: 0.02
2023-03-01 01:02:44 iteration: 48700 loss: 0.0041 lr: 0.02
2023-03-01 01:02:51 iteration: 48800 loss: 0.0037 lr: 0.02
2023-03-01 01:02:58 iteration: 48900 loss: 0.0041 lr: 0.02
2023-03-01 01:03:06 iteration: 49000 loss: 0.0040 lr: 0.02
2023-03-01 01:03:17 iteration: 49100 loss: 0.0040 lr: 0.02
2023-03-01 01:03:24 iteration: 49200 loss: 0.0039 lr: 0.02
2023-03-01 01:03:31 iteration: 49300 loss: 0.0037 lr: 0.02
2023-03-01 01:03:39 iteration: 49400 loss: 0.0040 lr: 0.02
2023-03-01 01:03:46 iteration: 49500 loss: 0.0039 lr: 0.02
2023-03-01 01:03:54 iteration: 49600 loss: 0.0041 lr: 0.02
2023-03-01 01:04:01 iteration: 49700 loss: 0.0046 lr: 0.02
2023-03-01 01:04:09 iteration: 49800 loss: 0.0035 lr: 0.02
2023-03-01 01:04:16 iteration: 49900 loss: 0.0039 lr: 0.02
2023-03-01 01:04:24 iteration: 50000 loss: 0.0039 lr: 0.02
2023-03-01 01:04:34 iteration: 50100 loss: 0.0044 lr: 0.02
2023-03-01 01:04:42 iteration: 50200 loss: 0.0036 lr: 0.02
2023-03-01 01:04:49 iteration: 50300 loss: 0.0038 lr: 0.02
2023-03-01 01:04:56 iteration: 50400 loss: 0.0043 lr: 0.02
2023-03-01 01:05:04 iteration: 50500 loss: 0.0038 lr: 0.02
2023-03-01 01:05:11 iteration: 50600 loss: 0.0039 lr: 0.02
2023-03-01 01:05:18 iteration: 50700 loss: 0.0040 lr: 0.02
2023-03-01 01:05:26 iteration: 50800 loss: 0.0038 lr: 0.02
2023-03-01 01:05:33 iteration: 50900 loss: 0.0039 lr: 0.02
2023-03-01 01:05:42 iteration: 51000 loss: 0.0045 lr: 0.02
2023-03-01 01:05:52 iteration: 51100 loss: 0.0040 lr: 0.02
2023-03-01 01:05:59 iteration: 51200 loss: 0.0037 lr: 0.02
2023-03-01 01:06:07 iteration: 51300 loss: 0.0039 lr: 0.02
2023-03-01 01:06:14 iteration: 51400 loss: 0.0037 lr: 0.02
2023-03-01 01:06:21 iteration: 51500 loss: 0.0042 lr: 0.02
2023-03-01 01:06:29 iteration: 51600 loss: 0.0042 lr: 0.02
2023-03-01 01:06:36 iteration: 51700 loss: 0.0039 lr: 0.02
2023-03-01 01:06:43 iteration: 51800 loss: 0.0039 lr: 0.02
2023-03-01 01:06:51 iteration: 51900 loss: 0.0039 lr: 0.02
2023-03-01 01:06:58 iteration: 52000 loss: 0.0041 lr: 0.02
2023-03-01 01:07:09 iteration: 52100 loss: 0.0037 lr: 0.02
2023-03-01 01:07:16 iteration: 52200 loss: 0.0036 lr: 0.02
2023-03-01 01:07:23 iteration: 52300 loss: 0.0042 lr: 0.02
2023-03-01 01:07:31 iteration: 52400 loss: 0.0038 lr: 0.02
2023-03-01 01:07:38 iteration: 52500 loss: 0.0039 lr: 0.02
2023-03-01 01:07:45 iteration: 52600 loss: 0.0038 lr: 0.02
2023-03-01 01:07:53 iteration: 52700 loss: 0.0037 lr: 0.02
2023-03-01 01:08:00 iteration: 52800 loss: 0.0037 lr: 0.02
2023-03-01 01:08:07 iteration: 52900 loss: 0.0040 lr: 0.02
2023-03-01 01:08:15 iteration: 53000 loss: 0.0041 lr: 0.02
2023-03-01 01:08:25 iteration: 53100 loss: 0.0039 lr: 0.02
2023-03-01 01:08:33 iteration: 53200 loss: 0.0041 lr: 0.02
2023-03-01 01:08:40 iteration: 53300 loss: 0.0035 lr: 0.02
2023-03-01 01:08:47 iteration: 53400 loss: 0.0038 lr: 0.02
2023-03-01 01:08:55 iteration: 53500 loss: 0.0040 lr: 0.02
2023-03-01 01:09:02 iteration: 53600 loss: 0.0038 lr: 0.02
2023-03-01 01:09:09 iteration: 53700 loss: 0.0039 lr: 0.02
2023-03-01 01:09:17 iteration: 53800 loss: 0.0040 lr: 0.02
2023-03-01 01:09:24 iteration: 53900 loss: 0.0045 lr: 0.02
2023-03-01 01:09:31 iteration: 54000 loss: 0.0042 lr: 0.02
2023-03-01 01:09:42 iteration: 54100 loss: 0.0037 lr: 0.02
2023-03-01 01:09:49 iteration: 54200 loss: 0.0041 lr: 0.02
2023-03-01 01:09:57 iteration: 54300 loss: 0.0036 lr: 0.02
2023-03-01 01:10:04 iteration: 54400 loss: 0.0038 lr: 0.02
2023-03-01 01:10:12 iteration: 54500 loss: 0.0040 lr: 0.02
2023-03-01 01:10:19 iteration: 54600 loss: 0.0039 lr: 0.02
2023-03-01 01:10:26 iteration: 54700 loss: 0.0040 lr: 0.02
2023-03-01 01:10:34 iteration: 54800 loss: 0.0036 lr: 0.02
2023-03-01 01:10:41 iteration: 54900 loss: 0.0039 lr: 0.02
2023-03-01 01:10:48 iteration: 55000 loss: 0.0036 lr: 0.02
2023-03-01 01:10:59 iteration: 55100 loss: 0.0038 lr: 0.02
2023-03-01 01:11:07 iteration: 55200 loss: 0.0037 lr: 0.02
2023-03-01 01:11:14 iteration: 55300 loss: 0.0034 lr: 0.02
2023-03-01 01:11:21 iteration: 55400 loss: 0.0040 lr: 0.02
2023-03-01 01:11:29 iteration: 55500 loss: 0.0035 lr: 0.02
2023-03-01 01:11:36 iteration: 55600 loss: 0.0037 lr: 0.02
2023-03-01 01:11:44 iteration: 55700 loss: 0.0037 lr: 0.02
2023-03-01 01:11:51 iteration: 55800 loss: 0.0038 lr: 0.02
2023-03-01 01:11:59 iteration: 55900 loss: 0.0040 lr: 0.02
2023-03-01 01:12:06 iteration: 56000 loss: 0.0036 lr: 0.02
2023-03-01 01:12:17 iteration: 56100 loss: 0.0038 lr: 0.02
2023-03-01 01:12:24 iteration: 56200 loss: 0.0038 lr: 0.02
2023-03-01 01:12:32 iteration: 56300 loss: 0.0038 lr: 0.02
2023-03-01 01:12:39 iteration: 56400 loss: 0.0040 lr: 0.02
2023-03-01 01:12:46 iteration: 56500 loss: 0.0041 lr: 0.02
2023-03-01 01:12:54 iteration: 56600 loss: 0.0036 lr: 0.02
2023-03-01 01:13:01 iteration: 56700 loss: 0.0039 lr: 0.02
2023-03-01 01:13:08 iteration: 56800 loss: 0.0038 lr: 0.02
2023-03-01 01:13:16 iteration: 56900 loss: 0.0034 lr: 0.02
2023-03-01 01:13:23 iteration: 57000 loss: 0.0039 lr: 0.02
2023-03-01 01:13:34 iteration: 57100 loss: 0.0039 lr: 0.02
2023-03-01 01:13:41 iteration: 57200 loss: 0.0036 lr: 0.02
2023-03-01 01:13:49 iteration: 57300 loss: 0.0039 lr: 0.02
2023-03-01 01:13:56 iteration: 57400 loss: 0.0039 lr: 0.02
2023-03-01 01:14:03 iteration: 57500 loss: 0.0037 lr: 0.02
2023-03-01 01:14:11 iteration: 57600 loss: 0.0041 lr: 0.02
2023-03-01 01:14:18 iteration: 57700 loss: 0.0039 lr: 0.02
2023-03-01 01:14:26 iteration: 57800 loss: 0.0040 lr: 0.02
2023-03-01 01:14:33 iteration: 57900 loss: 0.0036 lr: 0.02
2023-03-01 01:14:40 iteration: 58000 loss: 0.0038 lr: 0.02
2023-03-01 01:14:51 iteration: 58100 loss: 0.0038 lr: 0.02
2023-03-01 01:15:00 iteration: 58200 loss: 0.0034 lr: 0.02
2023-03-01 01:15:08 iteration: 58300 loss: 0.0038 lr: 0.02
2023-03-01 01:15:17 iteration: 58400 loss: 0.0037 lr: 0.02
2023-03-01 01:15:24 iteration: 58500 loss: 0.0036 lr: 0.02
2023-03-01 01:15:31 iteration: 58600 loss: 0.0038 lr: 0.02
2023-03-01 01:15:39 iteration: 58700 loss: 0.0036 lr: 0.02
2023-03-01 01:15:46 iteration: 58800 loss: 0.0038 lr: 0.02
2023-03-01 01:15:53 iteration: 58900 loss: 0.0035 lr: 0.02
2023-03-01 01:16:01 iteration: 59000 loss: 0.0038 lr: 0.02
2023-03-01 01:16:12 iteration: 59100 loss: 0.0037 lr: 0.02
2023-03-01 01:16:19 iteration: 59200 loss: 0.0038 lr: 0.02
2023-03-01 01:16:27 iteration: 59300 loss: 0.0041 lr: 0.02
2023-03-01 01:16:34 iteration: 59400 loss: 0.0039 lr: 0.02
2023-03-01 01:16:42 iteration: 59500 loss: 0.0034 lr: 0.02
2023-03-01 01:16:49 iteration: 59600 loss: 0.0038 lr: 0.02
2023-03-01 01:16:56 iteration: 59700 loss: 0.0036 lr: 0.02
2023-03-01 01:17:04 iteration: 59800 loss: 0.0036 lr: 0.02
2023-03-01 01:17:12 iteration: 59900 loss: 0.0034 lr: 0.02
2023-03-01 01:17:19 iteration: 60000 loss: 0.0037 lr: 0.02
2023-03-01 01:17:30 iteration: 60100 loss: 0.0035 lr: 0.02
2023-03-01 01:17:37 iteration: 60200 loss: 0.0037 lr: 0.02
2023-03-01 01:17:44 iteration: 60300 loss: 0.0036 lr: 0.02
2023-03-01 01:17:52 iteration: 60400 loss: 0.0035 lr: 0.02
2023-03-01 01:17:59 iteration: 60500 loss: 0.0038 lr: 0.02
2023-03-01 01:18:07 iteration: 60600 loss: 0.0034 lr: 0.02
2023-03-01 01:18:14 iteration: 60700 loss: 0.0034 lr: 0.02
2023-03-01 01:18:22 iteration: 60800 loss: 0.0036 lr: 0.02
2023-03-01 01:18:29 iteration: 60900 loss: 0.0037 lr: 0.02
2023-03-01 01:18:36 iteration: 61000 loss: 0.0038 lr: 0.02
2023-03-01 01:18:47 iteration: 61100 loss: 0.0035 lr: 0.02
2023-03-01 01:18:54 iteration: 61200 loss: 0.0035 lr: 0.02
2023-03-01 01:19:02 iteration: 61300 loss: 0.0036 lr: 0.02
2023-03-01 01:19:09 iteration: 61400 loss: 0.0039 lr: 0.02
2023-03-01 01:19:17 iteration: 61500 loss: 0.0038 lr: 0.02
2023-03-01 01:19:24 iteration: 61600 loss: 0.0036 lr: 0.02
2023-03-01 01:19:31 iteration: 61700 loss: 0.0037 lr: 0.02
2023-03-01 01:19:40 iteration: 61800 loss: 0.0036 lr: 0.02
2023-03-01 01:19:47 iteration: 61900 loss: 0.0033 lr: 0.02
2023-03-01 01:19:55 iteration: 62000 loss: 0.0036 lr: 0.02
2023-03-01 01:20:05 iteration: 62100 loss: 0.0034 lr: 0.02
2023-03-01 01:20:13 iteration: 62200 loss: 0.0038 lr: 0.02
2023-03-01 01:20:21 iteration: 62300 loss: 0.0039 lr: 0.02
2023-03-01 01:20:28 iteration: 62400 loss: 0.0037 lr: 0.02
2023-03-01 01:20:35 iteration: 62500 loss: 0.0043 lr: 0.02
2023-03-01 01:20:43 iteration: 62600 loss: 0.0034 lr: 0.02
2023-03-01 01:20:50 iteration: 62700 loss: 0.0035 lr: 0.02
2023-03-01 01:20:57 iteration: 62800 loss: 0.0042 lr: 0.02
2023-03-01 01:21:05 iteration: 62900 loss: 0.0040 lr: 0.02
2023-03-01 01:21:12 iteration: 63000 loss: 0.0036 lr: 0.02
2023-03-01 01:21:23 iteration: 63100 loss: 0.0039 lr: 0.02
2023-03-01 01:21:31 iteration: 63200 loss: 0.0035 lr: 0.02
2023-03-01 01:21:38 iteration: 63300 loss: 0.0037 lr: 0.02
2023-03-01 01:21:45 iteration: 63400 loss: 0.0034 lr: 0.02
2023-03-01 01:21:53 iteration: 63500 loss: 0.0038 lr: 0.02
2023-03-01 01:22:00 iteration: 63600 loss: 0.0037 lr: 0.02
2023-03-01 01:22:07 iteration: 63700 loss: 0.0036 lr: 0.02
2023-03-01 01:22:15 iteration: 63800 loss: 0.0037 lr: 0.02
2023-03-01 01:22:22 iteration: 63900 loss: 0.0034 lr: 0.02
2023-03-01 01:22:29 iteration: 64000 loss: 0.0034 lr: 0.02
2023-03-01 01:22:41 iteration: 64100 loss: 0.0039 lr: 0.02
2023-03-01 01:22:48 iteration: 64200 loss: 0.0035 lr: 0.02
2023-03-01 01:22:56 iteration: 64300 loss: 0.0040 lr: 0.02
2023-03-01 01:23:03 iteration: 64400 loss: 0.0037 lr: 0.02
2023-03-01 01:23:11 iteration: 64500 loss: 0.0037 lr: 0.02
2023-03-01 01:23:19 iteration: 64600 loss: 0.0033 lr: 0.02
2023-03-01 01:23:26 iteration: 64700 loss: 0.0034 lr: 0.02
2023-03-01 01:23:33 iteration: 64800 loss: 0.0036 lr: 0.02
2023-03-01 01:23:42 iteration: 64900 loss: 0.0032 lr: 0.02
2023-03-01 01:23:49 iteration: 65000 loss: 0.0037 lr: 0.02
2023-03-01 01:24:00 iteration: 65100 loss: 0.0035 lr: 0.02
2023-03-01 01:24:08 iteration: 65200 loss: 0.0037 lr: 0.02
2023-03-01 01:24:15 iteration: 65300 loss: 0.0031 lr: 0.02
2023-03-01 01:24:22 iteration: 65400 loss: 0.0034 lr: 0.02
2023-03-01 01:24:30 iteration: 65500 loss: 0.0033 lr: 0.02
2023-03-01 01:24:37 iteration: 65600 loss: 0.0035 lr: 0.02
2023-03-01 01:24:45 iteration: 65700 loss: 0.0033 lr: 0.02
2023-03-01 01:24:52 iteration: 65800 loss: 0.0035 lr: 0.02
2023-03-01 01:25:00 iteration: 65900 loss: 0.0034 lr: 0.02
2023-03-01 01:25:07 iteration: 66000 loss: 0.0035 lr: 0.02
2023-03-01 01:25:18 iteration: 66100 loss: 0.0032 lr: 0.02
2023-03-01 01:25:25 iteration: 66200 loss: 0.0032 lr: 0.02
2023-03-01 01:25:32 iteration: 66300 loss: 0.0033 lr: 0.02
2023-03-01 01:25:40 iteration: 66400 loss: 0.0035 lr: 0.02
2023-03-01 01:25:48 iteration: 66500 loss: 0.0031 lr: 0.02
2023-03-01 01:25:55 iteration: 66600 loss: 0.0036 lr: 0.02
2023-03-01 01:26:03 iteration: 66700 loss: 0.0035 lr: 0.02
2023-03-01 01:26:10 iteration: 66800 loss: 0.0032 lr: 0.02
2023-03-01 01:26:17 iteration: 66900 loss: 0.0033 lr: 0.02
2023-03-01 01:26:25 iteration: 67000 loss: 0.0036 lr: 0.02
2023-03-01 01:26:36 iteration: 67100 loss: 0.0034 lr: 0.02
2023-03-01 01:26:43 iteration: 67200 loss: 0.0034 lr: 0.02
2023-03-01 01:26:51 iteration: 67300 loss: 0.0037 lr: 0.02
2023-03-01 01:26:58 iteration: 67400 loss: 0.0034 lr: 0.02
2023-03-01 01:27:05 iteration: 67500 loss: 0.0035 lr: 0.02
2023-03-01 01:27:13 iteration: 67600 loss: 0.0037 lr: 0.02
2023-03-01 01:27:21 iteration: 67700 loss: 0.0036 lr: 0.02
2023-03-01 01:27:28 iteration: 67800 loss: 0.0034 lr: 0.02
2023-03-01 01:27:35 iteration: 67900 loss: 0.0036 lr: 0.02
2023-03-01 01:27:43 iteration: 68000 loss: 0.0034 lr: 0.02
2023-03-01 01:27:53 iteration: 68100 loss: 0.0034 lr: 0.02
2023-03-01 01:28:01 iteration: 68200 loss: 0.0031 lr: 0.02
2023-03-01 01:28:08 iteration: 68300 loss: 0.0037 lr: 0.02
2023-03-01 01:28:16 iteration: 68400 loss: 0.0035 lr: 0.02
2023-03-01 01:28:23 iteration: 68500 loss: 0.0032 lr: 0.02
2023-03-01 01:28:30 iteration: 68600 loss: 0.0033 lr: 0.02
2023-03-01 01:28:38 iteration: 68700 loss: 0.0031 lr: 0.02
2023-03-01 01:28:45 iteration: 68800 loss: 0.0035 lr: 0.02
2023-03-01 01:28:52 iteration: 68900 loss: 0.0032 lr: 0.02
2023-03-01 01:29:00 iteration: 69000 loss: 0.0036 lr: 0.02
2023-03-01 01:29:10 iteration: 69100 loss: 0.0033 lr: 0.02
2023-03-01 01:29:18 iteration: 69200 loss: 0.0033 lr: 0.02
2023-03-01 01:29:25 iteration: 69300 loss: 0.0035 lr: 0.02
2023-03-01 01:29:32 iteration: 69400 loss: 0.0033 lr: 0.02
2023-03-01 01:29:40 iteration: 69500 loss: 0.0032 lr: 0.02
2023-03-01 01:29:48 iteration: 69600 loss: 0.0034 lr: 0.02
2023-03-01 01:29:55 iteration: 69700 loss: 0.0033 lr: 0.02
2023-03-01 01:30:02 iteration: 69800 loss: 0.0035 lr: 0.02
2023-03-01 01:30:09 iteration: 69900 loss: 0.0033 lr: 0.02
2023-03-01 01:30:17 iteration: 70000 loss: 0.0037 lr: 0.02
2023-03-01 01:30:27 iteration: 70100 loss: 0.0033 lr: 0.02
2023-03-01 01:30:35 iteration: 70200 loss: 0.0034 lr: 0.02
2023-03-01 01:30:42 iteration: 70300 loss: 0.0032 lr: 0.02
2023-03-01 01:30:50 iteration: 70400 loss: 0.0033 lr: 0.02
2023-03-01 01:30:57 iteration: 70500 loss: 0.0031 lr: 0.02
2023-03-01 01:31:04 iteration: 70600 loss: 0.0032 lr: 0.02
2023-03-01 01:31:11 iteration: 70700 loss: 0.0033 lr: 0.02
2023-03-01 01:31:19 iteration: 70800 loss: 0.0033 lr: 0.02
2023-03-01 01:31:26 iteration: 70900 loss: 0.0035 lr: 0.02
2023-03-01 01:31:33 iteration: 71000 loss: 0.0033 lr: 0.02
2023-03-01 01:31:44 iteration: 71100 loss: 0.0034 lr: 0.02
2023-03-01 01:31:51 iteration: 71200 loss: 0.0032 lr: 0.02
2023-03-01 01:31:58 iteration: 71300 loss: 0.0034 lr: 0.02
2023-03-01 01:32:05 iteration: 71400 loss: 0.0031 lr: 0.02
2023-03-01 01:32:13 iteration: 71500 loss: 0.0037 lr: 0.02
2023-03-01 01:32:20 iteration: 71600 loss: 0.0033 lr: 0.02
2023-03-01 01:32:28 iteration: 71700 loss: 0.0034 lr: 0.02
2023-03-01 01:32:35 iteration: 71800 loss: 0.0033 lr: 0.02
2023-03-01 01:32:42 iteration: 71900 loss: 0.0034 lr: 0.02
2023-03-01 01:32:50 iteration: 72000 loss: 0.0034 lr: 0.02
2023-03-01 01:33:00 iteration: 72100 loss: 0.0036 lr: 0.02
2023-03-01 01:33:08 iteration: 72200 loss: 0.0037 lr: 0.02
2023-03-01 01:33:15 iteration: 72300 loss: 0.0032 lr: 0.02
2023-03-01 01:33:22 iteration: 72400 loss: 0.0033 lr: 0.02
2023-03-01 01:33:29 iteration: 72500 loss: 0.0030 lr: 0.02
2023-03-01 01:33:37 iteration: 72600 loss: 0.0034 lr: 0.02
2023-03-01 01:33:45 iteration: 72700 loss: 0.0035 lr: 0.02
2023-03-01 01:33:52 iteration: 72800 loss: 0.0030 lr: 0.02
2023-03-01 01:33:59 iteration: 72900 loss: 0.0031 lr: 0.02
2023-03-01 01:34:07 iteration: 73000 loss: 0.0031 lr: 0.02
2023-03-01 01:34:17 iteration: 73100 loss: 0.0034 lr: 0.02
2023-03-01 01:34:24 iteration: 73200 loss: 0.0033 lr: 0.02
2023-03-01 01:34:32 iteration: 73300 loss: 0.0035 lr: 0.02
2023-03-01 01:34:39 iteration: 73400 loss: 0.0032 lr: 0.02
2023-03-01 01:34:46 iteration: 73500 loss: 0.0033 lr: 0.02
2023-03-01 01:34:54 iteration: 73600 loss: 0.0034 lr: 0.02
2023-03-01 01:35:01 iteration: 73700 loss: 0.0034 lr: 0.02
2023-03-01 01:35:08 iteration: 73800 loss: 0.0033 lr: 0.02
2023-03-01 01:35:16 iteration: 73900 loss: 0.0031 lr: 0.02
2023-03-01 01:35:23 iteration: 74000 loss: 0.0035 lr: 0.02
2023-03-01 01:35:34 iteration: 74100 loss: 0.0032 lr: 0.02
2023-03-01 01:35:42 iteration: 74200 loss: 0.0036 lr: 0.02
2023-03-01 01:35:50 iteration: 74300 loss: 0.0034 lr: 0.02
2023-03-01 01:35:58 iteration: 74400 loss: 0.0035 lr: 0.02
2023-03-01 01:36:05 iteration: 74500 loss: 0.0035 lr: 0.02
2023-03-01 01:36:13 iteration: 74600 loss: 0.0037 lr: 0.02
2023-03-01 01:36:20 iteration: 74700 loss: 0.0032 lr: 0.02
2023-03-01 01:36:27 iteration: 74800 loss: 0.0034 lr: 0.02
2023-03-01 01:36:35 iteration: 74900 loss: 0.0032 lr: 0.02
2023-03-01 01:36:42 iteration: 75000 loss: 0.0033 lr: 0.02
2023-03-01 01:36:53 iteration: 75100 loss: 0.0033 lr: 0.02
2023-03-01 01:37:00 iteration: 75200 loss: 0.0031 lr: 0.02
2023-03-01 01:37:08 iteration: 75300 loss: 0.0031 lr: 0.02
2023-03-01 01:37:15 iteration: 75400 loss: 0.0033 lr: 0.02
2023-03-01 01:37:23 iteration: 75500 loss: 0.0033 lr: 0.02
2023-03-01 01:37:30 iteration: 75600 loss: 0.0034 lr: 0.02
2023-03-01 01:37:37 iteration: 75700 loss: 0.0033 lr: 0.02
2023-03-01 01:37:44 iteration: 75800 loss: 0.0033 lr: 0.02
2023-03-01 01:37:52 iteration: 75900 loss: 0.0034 lr: 0.02
2023-03-01 01:37:59 iteration: 76000 loss: 0.0034 lr: 0.02
2023-03-01 01:38:10 iteration: 76100 loss: 0.0034 lr: 0.02
2023-03-01 01:38:17 iteration: 76200 loss: 0.0034 lr: 0.02
2023-03-01 01:38:24 iteration: 76300 loss: 0.0033 lr: 0.02
2023-03-01 01:38:32 iteration: 76400 loss: 0.0036 lr: 0.02
2023-03-01 01:38:39 iteration: 76500 loss: 0.0033 lr: 0.02
2023-03-01 01:38:46 iteration: 76600 loss: 0.0031 lr: 0.02
2023-03-01 01:38:54 iteration: 76700 loss: 0.0031 lr: 0.02
2023-03-01 01:39:02 iteration: 76800 loss: 0.0032 lr: 0.02
2023-03-01 01:39:09 iteration: 76900 loss: 0.0031 lr: 0.02
2023-03-01 01:39:16 iteration: 77000 loss: 0.0035 lr: 0.02
2023-03-01 01:39:26 iteration: 77100 loss: 0.0033 lr: 0.02
2023-03-01 01:39:34 iteration: 77200 loss: 0.0030 lr: 0.02
2023-03-01 01:39:42 iteration: 77300 loss: 0.0032 lr: 0.02
2023-03-01 01:39:50 iteration: 77400 loss: 0.0036 lr: 0.02
2023-03-01 01:39:57 iteration: 77500 loss: 0.0033 lr: 0.02
2023-03-01 01:40:05 iteration: 77600 loss: 0.0038 lr: 0.02
2023-03-01 01:40:12 iteration: 77700 loss: 0.0029 lr: 0.02
2023-03-01 01:40:20 iteration: 77800 loss: 0.0035 lr: 0.02
2023-03-01 01:40:27 iteration: 77900 loss: 0.0038 lr: 0.02
2023-03-01 01:40:34 iteration: 78000 loss: 0.0030 lr: 0.02
2023-03-01 01:40:45 iteration: 78100 loss: 0.0031 lr: 0.02
2023-03-01 01:40:53 iteration: 78200 loss: 0.0037 lr: 0.02
2023-03-01 01:41:00 iteration: 78300 loss: 0.0035 lr: 0.02
2023-03-01 01:41:07 iteration: 78400 loss: 0.0029 lr: 0.02
2023-03-01 01:41:15 iteration: 78500 loss: 0.0035 lr: 0.02
2023-03-01 01:41:22 iteration: 78600 loss: 0.0030 lr: 0.02
2023-03-01 01:41:30 iteration: 78700 loss: 0.0029 lr: 0.02
2023-03-01 01:41:37 iteration: 78800 loss: 0.0029 lr: 0.02
2023-03-01 01:41:45 iteration: 78900 loss: 0.0034 lr: 0.02
2023-03-01 01:41:53 iteration: 79000 loss: 0.0033 lr: 0.02
2023-03-01 01:42:03 iteration: 79100 loss: 0.0032 lr: 0.02
2023-03-01 01:42:11 iteration: 79200 loss: 0.0034 lr: 0.02
2023-03-01 01:42:18 iteration: 79300 loss: 0.0032 lr: 0.02
2023-03-01 01:42:25 iteration: 79400 loss: 0.0031 lr: 0.02
2023-03-01 01:42:33 iteration: 79500 loss: 0.0034 lr: 0.02
2023-03-01 01:42:40 iteration: 79600 loss: 0.0031 lr: 0.02
2023-03-01 01:42:48 iteration: 79700 loss: 0.0031 lr: 0.02
2023-03-01 01:42:55 iteration: 79800 loss: 0.0031 lr: 0.02
2023-03-01 01:43:02 iteration: 79900 loss: 0.0033 lr: 0.02
2023-03-01 01:43:10 iteration: 80000 loss: 0.0034 lr: 0.02
2023-03-01 01:43:21 iteration: 80100 loss: 0.0032 lr: 0.02
2023-03-01 01:43:28 iteration: 80200 loss: 0.0032 lr: 0.02
2023-03-01 01:43:35 iteration: 80300 loss: 0.0029 lr: 0.02
2023-03-01 01:43:43 iteration: 80400 loss: 0.0031 lr: 0.02
2023-03-01 01:43:50 iteration: 80500 loss: 0.0031 lr: 0.02
2023-03-01 01:43:57 iteration: 80600 loss: 0.0031 lr: 0.02
2023-03-01 01:44:05 iteration: 80700 loss: 0.0033 lr: 0.02
2023-03-01 01:44:12 iteration: 80800 loss: 0.0033 lr: 0.02
2023-03-01 01:44:20 iteration: 80900 loss: 0.0033 lr: 0.02
2023-03-01 01:44:27 iteration: 81000 loss: 0.0030 lr: 0.02
2023-03-01 01:44:38 iteration: 81100 loss: 0.0032 lr: 0.02
2023-03-01 01:44:45 iteration: 81200 loss: 0.0032 lr: 0.02
2023-03-01 01:44:52 iteration: 81300 loss: 0.0032 lr: 0.02
2023-03-01 01:45:00 iteration: 81400 loss: 0.0030 lr: 0.02
2023-03-01 01:45:07 iteration: 81500 loss: 0.0030 lr: 0.02
2023-03-01 01:45:15 iteration: 81600 loss: 0.0031 lr: 0.02
2023-03-01 01:45:22 iteration: 81700 loss: 0.0038 lr: 0.02
2023-03-01 01:45:30 iteration: 81800 loss: 0.0035 lr: 0.02
2023-03-01 01:45:37 iteration: 81900 loss: 0.0033 lr: 0.02
2023-03-01 01:45:44 iteration: 82000 loss: 0.0032 lr: 0.02
2023-03-01 01:45:55 iteration: 82100 loss: 0.0030 lr: 0.02
2023-03-01 01:46:03 iteration: 82200 loss: 0.0031 lr: 0.02
2023-03-01 01:46:10 iteration: 82300 loss: 0.0031 lr: 0.02
2023-03-01 01:46:18 iteration: 82400 loss: 0.0033 lr: 0.02
2023-03-01 01:46:25 iteration: 82500 loss: 0.0031 lr: 0.02
2023-03-01 01:46:32 iteration: 82600 loss: 0.0029 lr: 0.02
2023-03-01 01:46:40 iteration: 82700 loss: 0.0031 lr: 0.02
2023-03-01 01:46:47 iteration: 82800 loss: 0.0031 lr: 0.02
2023-03-01 01:46:54 iteration: 82900 loss: 0.0032 lr: 0.02
2023-03-01 01:47:02 iteration: 83000 loss: 0.0032 lr: 0.02
2023-03-01 01:47:12 iteration: 83100 loss: 0.0030 lr: 0.02
2023-03-01 01:47:20 iteration: 83200 loss: 0.0030 lr: 0.02
2023-03-01 01:47:28 iteration: 83300 loss: 0.0032 lr: 0.02
2023-03-01 01:47:35 iteration: 83400 loss: 0.0036 lr: 0.02
2023-03-01 01:47:43 iteration: 83500 loss: 0.0034 lr: 0.02
2023-03-01 01:47:51 iteration: 83600 loss: 0.0031 lr: 0.02
2023-03-01 01:47:58 iteration: 83700 loss: 0.0030 lr: 0.02
2023-03-01 01:48:05 iteration: 83800 loss: 0.0031 lr: 0.02
2023-03-01 01:48:13 iteration: 83900 loss: 0.0032 lr: 0.02
2023-03-01 01:48:20 iteration: 84000 loss: 0.0029 lr: 0.02
2023-03-01 01:48:31 iteration: 84100 loss: 0.0035 lr: 0.02
2023-03-01 01:48:38 iteration: 84200 loss: 0.0033 lr: 0.02
2023-03-01 01:48:45 iteration: 84300 loss: 0.0032 lr: 0.02
2023-03-01 01:48:53 iteration: 84400 loss: 0.0032 lr: 0.02
2023-03-01 01:49:00 iteration: 84500 loss: 0.0031 lr: 0.02
2023-03-01 01:49:07 iteration: 84600 loss: 0.0029 lr: 0.02
2023-03-01 01:49:14 iteration: 84700 loss: 0.0030 lr: 0.02
2023-03-01 01:49:22 iteration: 84800 loss: 0.0032 lr: 0.02
2023-03-01 01:49:29 iteration: 84900 loss: 0.0033 lr: 0.02
2023-03-01 01:49:37 iteration: 85000 loss: 0.0029 lr: 0.02
2023-03-01 01:49:48 iteration: 85100 loss: 0.0029 lr: 0.02
2023-03-01 01:49:56 iteration: 85200 loss: 0.0029 lr: 0.02
2023-03-01 01:50:03 iteration: 85300 loss: 0.0031 lr: 0.02
2023-03-01 01:50:11 iteration: 85400 loss: 0.0030 lr: 0.02
2023-03-01 01:50:18 iteration: 85500 loss: 0.0031 lr: 0.02
2023-03-01 01:50:25 iteration: 85600 loss: 0.0030 lr: 0.02
2023-03-01 01:50:33 iteration: 85700 loss: 0.0031 lr: 0.02
2023-03-01 01:50:40 iteration: 85800 loss: 0.0030 lr: 0.02
2023-03-01 01:50:48 iteration: 85900 loss: 0.0029 lr: 0.02
2023-03-01 01:50:55 iteration: 86000 loss: 0.0031 lr: 0.02
2023-03-01 01:51:05 iteration: 86100 loss: 0.0030 lr: 0.02
2023-03-01 01:51:12 iteration: 86200 loss: 0.0031 lr: 0.02
2023-03-01 01:51:20 iteration: 86300 loss: 0.0030 lr: 0.02
2023-03-01 01:51:27 iteration: 86400 loss: 0.0031 lr: 0.02
2023-03-01 01:51:35 iteration: 86500 loss: 0.0030 lr: 0.02
2023-03-01 01:51:42 iteration: 86600 loss: 0.0028 lr: 0.02
2023-03-01 01:51:49 iteration: 86700 loss: 0.0031 lr: 0.02
2023-03-01 01:51:56 iteration: 86800 loss: 0.0029 lr: 0.02
2023-03-01 01:52:04 iteration: 86900 loss: 0.0031 lr: 0.02
2023-03-01 01:52:11 iteration: 87000 loss: 0.0032 lr: 0.02
2023-03-01 01:52:22 iteration: 87100 loss: 0.0029 lr: 0.02
2023-03-01 01:52:29 iteration: 87200 loss: 0.0030 lr: 0.02
2023-03-01 01:52:36 iteration: 87300 loss: 0.0029 lr: 0.02
2023-03-01 01:52:44 iteration: 87400 loss: 0.0033 lr: 0.02
2023-03-01 01:52:51 iteration: 87500 loss: 0.0032 lr: 0.02
2023-03-01 01:52:59 iteration: 87600 loss: 0.0032 lr: 0.02
2023-03-01 01:53:06 iteration: 87700 loss: 0.0029 lr: 0.02
2023-03-01 01:53:13 iteration: 87800 loss: 0.0029 lr: 0.02
2023-03-01 01:53:20 iteration: 87900 loss: 0.0030 lr: 0.02
2023-03-01 01:53:28 iteration: 88000 loss: 0.0032 lr: 0.02
2023-03-01 01:53:40 iteration: 88100 loss: 0.0032 lr: 0.02
2023-03-01 01:53:48 iteration: 88200 loss: 0.0033 lr: 0.02
2023-03-01 01:53:55 iteration: 88300 loss: 0.0032 lr: 0.02
2023-03-01 01:54:03 iteration: 88400 loss: 0.0032 lr: 0.02
2023-03-01 01:54:11 iteration: 88500 loss: 0.0034 lr: 0.02
2023-03-01 01:54:18 iteration: 88600 loss: 0.0031 lr: 0.02
2023-03-01 01:54:26 iteration: 88700 loss: 0.0029 lr: 0.02
2023-03-01 01:54:33 iteration: 88800 loss: 0.0032 lr: 0.02
2023-03-01 01:54:40 iteration: 88900 loss: 0.0029 lr: 0.02
2023-03-01 01:54:48 iteration: 89000 loss: 0.0029 lr: 0.02
2023-03-01 01:54:58 iteration: 89100 loss: 0.0031 lr: 0.02
2023-03-01 01:55:06 iteration: 89200 loss: 0.0033 lr: 0.02
2023-03-01 01:55:13 iteration: 89300 loss: 0.0030 lr: 0.02
2023-03-01 01:55:20 iteration: 89400 loss: 0.0028 lr: 0.02
2023-03-01 01:55:28 iteration: 89500 loss: 0.0029 lr: 0.02
2023-03-01 01:55:35 iteration: 89600 loss: 0.0031 lr: 0.02
2023-03-01 01:55:42 iteration: 89700 loss: 0.0029 lr: 0.02
2023-03-01 01:55:50 iteration: 89800 loss: 0.0029 lr: 0.02
2023-03-01 01:55:57 iteration: 89900 loss: 0.0030 lr: 0.02
2023-03-01 01:56:05 iteration: 90000 loss: 0.0030 lr: 0.02
2023-03-01 01:56:15 iteration: 90100 loss: 0.0031 lr: 0.02
2023-03-01 01:56:23 iteration: 90200 loss: 0.0031 lr: 0.02
2023-03-01 01:56:30 iteration: 90300 loss: 0.0030 lr: 0.02
2023-03-01 01:56:38 iteration: 90400 loss: 0.0029 lr: 0.02
2023-03-01 01:56:45 iteration: 90500 loss: 0.0029 lr: 0.02
2023-03-01 01:56:52 iteration: 90600 loss: 0.0030 lr: 0.02
2023-03-01 01:57:00 iteration: 90700 loss: 0.0031 lr: 0.02
2023-03-01 01:57:07 iteration: 90800 loss: 0.0031 lr: 0.02
2023-03-01 01:57:14 iteration: 90900 loss: 0.0032 lr: 0.02
2023-03-01 01:57:22 iteration: 91000 loss: 0.0031 lr: 0.02
2023-03-01 01:57:32 iteration: 91100 loss: 0.0032 lr: 0.02
2023-03-01 01:57:40 iteration: 91200 loss: 0.0028 lr: 0.02
2023-03-01 01:57:47 iteration: 91300 loss: 0.0028 lr: 0.02
2023-03-01 01:57:54 iteration: 91400 loss: 0.0029 lr: 0.02
2023-03-01 01:58:02 iteration: 91500 loss: 0.0030 lr: 0.02
2023-03-01 01:58:09 iteration: 91600 loss: 0.0028 lr: 0.02
2023-03-01 01:58:17 iteration: 91700 loss: 0.0029 lr: 0.02
2023-03-01 01:58:24 iteration: 91800 loss: 0.0027 lr: 0.02
2023-03-01 01:58:32 iteration: 91900 loss: 0.0029 lr: 0.02
2023-03-01 01:58:39 iteration: 92000 loss: 0.0031 lr: 0.02
2023-03-01 01:58:50 iteration: 92100 loss: 0.0031 lr: 0.02
2023-03-01 01:58:57 iteration: 92200 loss: 0.0030 lr: 0.02
2023-03-01 01:59:04 iteration: 92300 loss: 0.0032 lr: 0.02
2023-03-01 01:59:12 iteration: 92400 loss: 0.0030 lr: 0.02
2023-03-01 01:59:19 iteration: 92500 loss: 0.0030 lr: 0.02
2023-03-01 01:59:27 iteration: 92600 loss: 0.0032 lr: 0.02
2023-03-01 01:59:34 iteration: 92700 loss: 0.0032 lr: 0.02
2023-03-01 01:59:43 iteration: 92800 loss: 0.0029 lr: 0.02
2023-03-01 01:59:50 iteration: 92900 loss: 0.0029 lr: 0.02
2023-03-01 01:59:57 iteration: 93000 loss: 0.0029 lr: 0.02
2023-03-01 02:00:08 iteration: 93100 loss: 0.0027 lr: 0.02
2023-03-01 02:00:15 iteration: 93200 loss: 0.0028 lr: 0.02
2023-03-01 02:00:23 iteration: 93300 loss: 0.0028 lr: 0.02
2023-03-01 02:00:31 iteration: 93400 loss: 0.0029 lr: 0.02
2023-03-01 02:00:38 iteration: 93500 loss: 0.0029 lr: 0.02
2023-03-01 02:00:45 iteration: 93600 loss: 0.0031 lr: 0.02
2023-03-01 02:00:53 iteration: 93700 loss: 0.0030 lr: 0.02
2023-03-01 02:01:00 iteration: 93800 loss: 0.0028 lr: 0.02
2023-03-01 02:01:08 iteration: 93900 loss: 0.0028 lr: 0.02
2023-03-01 02:01:15 iteration: 94000 loss: 0.0033 lr: 0.02
2023-03-01 02:01:26 iteration: 94100 loss: 0.0030 lr: 0.02
2023-03-01 02:01:33 iteration: 94200 loss: 0.0031 lr: 0.02
2023-03-01 02:01:40 iteration: 94300 loss: 0.0030 lr: 0.02
2023-03-01 02:01:48 iteration: 94400 loss: 0.0031 lr: 0.02
2023-03-01 02:01:55 iteration: 94500 loss: 0.0028 lr: 0.02
2023-03-01 02:02:02 iteration: 94600 loss: 0.0030 lr: 0.02
2023-03-01 02:02:10 iteration: 94700 loss: 0.0031 lr: 0.02
2023-03-01 02:02:17 iteration: 94800 loss: 0.0029 lr: 0.02
2023-03-01 02:02:25 iteration: 94900 loss: 0.0031 lr: 0.02
2023-03-01 02:02:32 iteration: 95000 loss: 0.0030 lr: 0.02
2023-03-01 02:02:42 iteration: 95100 loss: 0.0028 lr: 0.02
2023-03-01 02:02:50 iteration: 95200 loss: 0.0032 lr: 0.02
2023-03-01 02:02:57 iteration: 95300 loss: 0.0027 lr: 0.02
2023-03-01 02:03:04 iteration: 95400 loss: 0.0028 lr: 0.02
2023-03-01 02:03:11 iteration: 95500 loss: 0.0030 lr: 0.02
2023-03-01 02:03:19 iteration: 95600 loss: 0.0031 lr: 0.02
2023-03-01 02:03:26 iteration: 95700 loss: 0.0032 lr: 0.02
2023-03-01 02:03:34 iteration: 95800 loss: 0.0032 lr: 0.02
2023-03-01 02:03:41 iteration: 95900 loss: 0.0028 lr: 0.02
2023-03-01 02:03:49 iteration: 96000 loss: 0.0030 lr: 0.02
2023-03-01 02:04:00 iteration: 96100 loss: 0.0032 lr: 0.02
2023-03-01 02:04:08 iteration: 96200 loss: 0.0029 lr: 0.02
2023-03-01 02:04:15 iteration: 96300 loss: 0.0032 lr: 0.02
2023-03-01 02:04:23 iteration: 96400 loss: 0.0028 lr: 0.02
2023-03-01 02:04:30 iteration: 96500 loss: 0.0027 lr: 0.02
2023-03-01 02:04:38 iteration: 96600 loss: 0.0030 lr: 0.02
2023-03-01 02:04:45 iteration: 96700 loss: 0.0029 lr: 0.02
2023-03-01 02:04:52 iteration: 96800 loss: 0.0029 lr: 0.02
2023-03-01 02:05:00 iteration: 96900 loss: 0.0031 lr: 0.02
2023-03-01 02:05:07 iteration: 97000 loss: 0.0030 lr: 0.02
2023-03-01 02:05:17 iteration: 97100 loss: 0.0029 lr: 0.02
2023-03-01 02:05:24 iteration: 97200 loss: 0.0028 lr: 0.02
2023-03-01 02:05:32 iteration: 97300 loss: 0.0027 lr: 0.02
2023-03-01 02:05:40 iteration: 97400 loss: 0.0028 lr: 0.02
2023-03-01 02:05:48 iteration: 97500 loss: 0.0029 lr: 0.02
2023-03-01 02:05:55 iteration: 97600 loss: 0.0029 lr: 0.02
2023-03-01 02:06:02 iteration: 97700 loss: 0.0026 lr: 0.02
2023-03-01 02:06:10 iteration: 97800 loss: 0.0028 lr: 0.02
2023-03-01 02:06:17 iteration: 97900 loss: 0.0028 lr: 0.02
2023-03-01 02:06:25 iteration: 98000 loss: 0.0029 lr: 0.02
2023-03-01 02:06:35 iteration: 98100 loss: 0.0027 lr: 0.02
2023-03-01 02:06:42 iteration: 98200 loss: 0.0028 lr: 0.02
2023-03-01 02:06:50 iteration: 98300 loss: 0.0029 lr: 0.02
2023-03-01 02:06:57 iteration: 98400 loss: 0.0030 lr: 0.02
2023-03-01 02:07:07 iteration: 98500 loss: 0.0030 lr: 0.02
2023-03-01 02:07:14 iteration: 98600 loss: 0.0027 lr: 0.02
2023-03-01 02:07:22 iteration: 98700 loss: 0.0032 lr: 0.02
2023-03-01 02:07:29 iteration: 98800 loss: 0.0031 lr: 0.02
2023-03-01 02:07:37 iteration: 98900 loss: 0.0028 lr: 0.02
2023-03-01 02:07:44 iteration: 99000 loss: 0.0029 lr: 0.02
2023-03-01 02:07:55 iteration: 99100 loss: 0.0031 lr: 0.02
2023-03-01 02:08:02 iteration: 99200 loss: 0.0029 lr: 0.02
2023-03-01 02:08:09 iteration: 99300 loss: 0.0033 lr: 0.02
2023-03-01 02:08:17 iteration: 99400 loss: 0.0029 lr: 0.02
2023-03-01 02:08:24 iteration: 99500 loss: 0.0031 lr: 0.02
2023-03-01 02:08:32 iteration: 99600 loss: 0.0031 lr: 0.02
2023-03-01 02:08:39 iteration: 99700 loss: 0.0029 lr: 0.02
2023-03-01 02:08:47 iteration: 99800 loss: 0.0030 lr: 0.02
2023-03-01 02:08:54 iteration: 99900 loss: 0.0029 lr: 0.02
2023-03-01 02:09:01 iteration: 100000 loss: 0.0029 lr: 0.02
2023-03-01 02:09:12 iteration: 100100 loss: 0.0031 lr: 0.02
2023-03-01 02:09:19 iteration: 100200 loss: 0.0027 lr: 0.02
2023-03-01 02:09:26 iteration: 100300 loss: 0.0027 lr: 0.02
2023-03-01 02:09:34 iteration: 100400 loss: 0.0028 lr: 0.02
2023-03-01 02:09:41 iteration: 100500 loss: 0.0033 lr: 0.02
2023-03-01 02:09:49 iteration: 100600 loss: 0.0031 lr: 0.02
2023-03-01 02:09:56 iteration: 100700 loss: 0.0028 lr: 0.02
2023-03-01 02:10:04 iteration: 100800 loss: 0.0029 lr: 0.02
2023-03-01 02:10:11 iteration: 100900 loss: 0.0026 lr: 0.02
2023-03-01 02:10:18 iteration: 101000 loss: 0.0030 lr: 0.02
2023-03-01 02:10:29 iteration: 101100 loss: 0.0031 lr: 0.02
2023-03-01 02:10:37 iteration: 101200 loss: 0.0027 lr: 0.02
2023-03-01 02:10:44 iteration: 101300 loss: 0.0031 lr: 0.02
2023-03-01 02:10:51 iteration: 101400 loss: 0.0029 lr: 0.02
2023-03-01 02:10:59 iteration: 101500 loss: 0.0025 lr: 0.02
2023-03-01 02:11:06 iteration: 101600 loss: 0.0029 lr: 0.02
2023-03-01 02:11:14 iteration: 101700 loss: 0.0029 lr: 0.02
2023-03-01 02:11:21 iteration: 101800 loss: 0.0027 lr: 0.02
2023-03-01 02:11:29 iteration: 101900 loss: 0.0030 lr: 0.02
2023-03-01 02:11:36 iteration: 102000 loss: 0.0029 lr: 0.02
2023-03-01 02:11:47 iteration: 102100 loss: 0.0027 lr: 0.02
2023-03-01 02:11:54 iteration: 102200 loss: 0.0028 lr: 0.02
2023-03-01 02:12:02 iteration: 102300 loss: 0.0030 lr: 0.02
2023-03-01 02:12:09 iteration: 102400 loss: 0.0033 lr: 0.02
2023-03-01 02:12:16 iteration: 102500 loss: 0.0026 lr: 0.02
2023-03-01 02:12:24 iteration: 102600 loss: 0.0029 lr: 0.02
2023-03-01 02:12:31 iteration: 102700 loss: 0.0028 lr: 0.02
2023-03-01 02:12:38 iteration: 102800 loss: 0.0030 lr: 0.02
2023-03-01 02:12:45 iteration: 102900 loss: 0.0029 lr: 0.02
2023-03-01 02:12:53 iteration: 103000 loss: 0.0027 lr: 0.02
2023-03-01 02:13:04 iteration: 103100 loss: 0.0029 lr: 0.02
2023-03-01 02:13:11 iteration: 103200 loss: 0.0028 lr: 0.02
2023-03-01 02:13:19 iteration: 103300 loss: 0.0029 lr: 0.02
2023-03-01 02:13:26 iteration: 103400 loss: 0.0030 lr: 0.02
2023-03-01 02:13:34 iteration: 103500 loss: 0.0028 lr: 0.02
2023-03-01 02:13:42 iteration: 103600 loss: 0.0027 lr: 0.02
2023-03-01 02:13:49 iteration: 103700 loss: 0.0026 lr: 0.02
2023-03-01 02:13:57 iteration: 103800 loss: 0.0028 lr: 0.02
2023-03-01 02:14:04 iteration: 103900 loss: 0.0027 lr: 0.02
2023-03-01 02:14:11 iteration: 104000 loss: 0.0027 lr: 0.02
2023-03-01 02:14:22 iteration: 104100 loss: 0.0030 lr: 0.02
2023-03-01 02:14:29 iteration: 104200 loss: 0.0028 lr: 0.02
2023-03-01 02:14:36 iteration: 104300 loss: 0.0027 lr: 0.02
2023-03-01 02:14:44 iteration: 104400 loss: 0.0028 lr: 0.02
2023-03-01 02:14:51 iteration: 104500 loss: 0.0027 lr: 0.02
2023-03-01 02:14:59 iteration: 104600 loss: 0.0030 lr: 0.02
2023-03-01 02:15:06 iteration: 104700 loss: 0.0030 lr: 0.02
2023-03-01 02:15:13 iteration: 104800 loss: 0.0028 lr: 0.02
2023-03-01 02:15:21 iteration: 104900 loss: 0.0026 lr: 0.02
2023-03-01 02:15:28 iteration: 105000 loss: 0.0029 lr: 0.02
2023-03-01 02:15:38 iteration: 105100 loss: 0.0027 lr: 0.02
2023-03-01 02:15:46 iteration: 105200 loss: 0.0032 lr: 0.02
2023-03-01 02:15:54 iteration: 105300 loss: 0.0025 lr: 0.02
2023-03-01 02:16:01 iteration: 105400 loss: 0.0026 lr: 0.02
2023-03-01 02:16:09 iteration: 105500 loss: 0.0028 lr: 0.02
2023-03-01 02:16:16 iteration: 105600 loss: 0.0027 lr: 0.02
2023-03-01 02:16:23 iteration: 105700 loss: 0.0029 lr: 0.02
2023-03-01 02:16:30 iteration: 105800 loss: 0.0029 lr: 0.02
2023-03-01 02:16:38 iteration: 105900 loss: 0.0029 lr: 0.02
2023-03-01 02:16:45 iteration: 106000 loss: 0.0027 lr: 0.02
2023-03-01 02:16:55 iteration: 106100 loss: 0.0028 lr: 0.02
2023-03-01 02:17:03 iteration: 106200 loss: 0.0029 lr: 0.02
2023-03-01 02:17:10 iteration: 106300 loss: 0.0029 lr: 0.02
2023-03-01 02:17:18 iteration: 106400 loss: 0.0027 lr: 0.02
2023-03-01 02:17:25 iteration: 106500 loss: 0.0027 lr: 0.02
2023-03-01 02:17:32 iteration: 106600 loss: 0.0028 lr: 0.02
2023-03-01 02:17:41 iteration: 106700 loss: 0.0029 lr: 0.02
2023-03-01 02:17:48 iteration: 106800 loss: 0.0030 lr: 0.02
2023-03-01 02:17:55 iteration: 106900 loss: 0.0029 lr: 0.02
2023-03-01 02:18:03 iteration: 107000 loss: 0.0032 lr: 0.02
2023-03-01 02:18:14 iteration: 107100 loss: 0.0027 lr: 0.02
2023-03-01 02:18:21 iteration: 107200 loss: 0.0029 lr: 0.02
2023-03-01 02:18:28 iteration: 107300 loss: 0.0028 lr: 0.02
2023-03-01 02:18:36 iteration: 107400 loss: 0.0027 lr: 0.02
2023-03-01 02:18:43 iteration: 107500 loss: 0.0025 lr: 0.02
2023-03-01 02:18:50 iteration: 107600 loss: 0.0029 lr: 0.02
2023-03-01 02:18:58 iteration: 107700 loss: 0.0030 lr: 0.02
2023-03-01 02:19:05 iteration: 107800 loss: 0.0027 lr: 0.02
2023-03-01 02:19:13 iteration: 107900 loss: 0.0028 lr: 0.02
2023-03-01 02:19:21 iteration: 108000 loss: 0.0027 lr: 0.02
2023-03-01 02:19:32 iteration: 108100 loss: 0.0027 lr: 0.02
2023-03-01 02:19:39 iteration: 108200 loss: 0.0029 lr: 0.02
2023-03-01 02:19:46 iteration: 108300 loss: 0.0026 lr: 0.02
2023-03-01 02:19:54 iteration: 108400 loss: 0.0029 lr: 0.02
2023-03-01 02:20:01 iteration: 108500 loss: 0.0028 lr: 0.02
2023-03-01 02:20:08 iteration: 108600 loss: 0.0030 lr: 0.02
2023-03-01 02:20:16 iteration: 108700 loss: 0.0025 lr: 0.02
2023-03-01 02:20:23 iteration: 108800 loss: 0.0026 lr: 0.02
2023-03-01 02:20:30 iteration: 108900 loss: 0.0029 lr: 0.02
2023-03-01 02:20:38 iteration: 109000 loss: 0.0027 lr: 0.02
2023-03-01 02:20:48 iteration: 109100 loss: 0.0031 lr: 0.02
2023-03-01 02:20:56 iteration: 109200 loss: 0.0029 lr: 0.02
2023-03-01 02:21:03 iteration: 109300 loss: 0.0027 lr: 0.02
2023-03-01 02:21:10 iteration: 109400 loss: 0.0026 lr: 0.02
2023-03-01 02:21:18 iteration: 109500 loss: 0.0025 lr: 0.02
2023-03-01 02:21:25 iteration: 109600 loss: 0.0028 lr: 0.02
2023-03-01 02:21:32 iteration: 109700 loss: 0.0031 lr: 0.02
2023-03-01 02:21:40 iteration: 109800 loss: 0.0031 lr: 0.02
2023-03-01 02:21:48 iteration: 109900 loss: 0.0030 lr: 0.02
2023-03-01 02:21:55 iteration: 110000 loss: 0.0028 lr: 0.02
2023-03-01 02:22:06 iteration: 110100 loss: 0.0029 lr: 0.02
2023-03-01 02:22:13 iteration: 110200 loss: 0.0028 lr: 0.02
2023-03-01 02:22:21 iteration: 110300 loss: 0.0028 lr: 0.02
2023-03-01 02:22:28 iteration: 110400 loss: 0.0030 lr: 0.02
2023-03-01 02:22:35 iteration: 110500 loss: 0.0029 lr: 0.02
2023-03-01 02:22:42 iteration: 110600 loss: 0.0029 lr: 0.02
2023-03-01 02:22:50 iteration: 110700 loss: 0.0030 lr: 0.02
2023-03-01 02:22:57 iteration: 110800 loss: 0.0027 lr: 0.02
2023-03-01 02:23:05 iteration: 110900 loss: 0.0026 lr: 0.02
2023-03-01 02:23:12 iteration: 111000 loss: 0.0028 lr: 0.02
2023-03-01 02:23:23 iteration: 111100 loss: 0.0029 lr: 0.02
2023-03-01 02:23:30 iteration: 111200 loss: 0.0026 lr: 0.02
2023-03-01 02:23:37 iteration: 111300 loss: 0.0028 lr: 0.02
2023-03-01 02:23:45 iteration: 111400 loss: 0.0028 lr: 0.02
2023-03-01 02:23:52 iteration: 111500 loss: 0.0028 lr: 0.02
2023-03-01 02:24:00 iteration: 111600 loss: 0.0025 lr: 0.02
2023-03-01 02:24:07 iteration: 111700 loss: 0.0026 lr: 0.02
2023-03-01 02:24:14 iteration: 111800 loss: 0.0027 lr: 0.02
2023-03-01 02:24:21 iteration: 111900 loss: 0.0027 lr: 0.02
2023-03-01 02:24:29 iteration: 112000 loss: 0.0028 lr: 0.02
2023-03-01 02:24:40 iteration: 112100 loss: 0.0028 lr: 0.02
2023-03-01 02:24:47 iteration: 112200 loss: 0.0029 lr: 0.02
2023-03-01 02:24:55 iteration: 112300 loss: 0.0031 lr: 0.02
2023-03-01 02:25:02 iteration: 112400 loss: 0.0026 lr: 0.02
2023-03-01 02:25:09 iteration: 112500 loss: 0.0027 lr: 0.02
2023-03-01 02:25:17 iteration: 112600 loss: 0.0027 lr: 0.02
2023-03-01 02:25:24 iteration: 112700 loss: 0.0026 lr: 0.02
2023-03-01 02:25:31 iteration: 112800 loss: 0.0025 lr: 0.02
2023-03-01 02:25:39 iteration: 112900 loss: 0.0029 lr: 0.02
2023-03-01 02:25:46 iteration: 113000 loss: 0.0030 lr: 0.02
2023-03-01 02:25:57 iteration: 113100 loss: 0.0027 lr: 0.02
2023-03-01 02:26:04 iteration: 113200 loss: 0.0027 lr: 0.02
2023-03-01 02:26:12 iteration: 113300 loss: 0.0029 lr: 0.02
2023-03-01 02:26:19 iteration: 113400 loss: 0.0026 lr: 0.02
2023-03-01 02:26:26 iteration: 113500 loss: 0.0027 lr: 0.02
2023-03-01 02:26:33 iteration: 113600 loss: 0.0028 lr: 0.02
2023-03-01 02:26:41 iteration: 113700 loss: 0.0027 lr: 0.02
2023-03-01 02:26:48 iteration: 113800 loss: 0.0026 lr: 0.02
2023-03-01 02:26:55 iteration: 113900 loss: 0.0030 lr: 0.02
2023-03-01 02:27:03 iteration: 114000 loss: 0.0027 lr: 0.02
2023-03-01 02:27:14 iteration: 114100 loss: 0.0028 lr: 0.02
2023-03-01 02:27:21 iteration: 114200 loss: 0.0026 lr: 0.02
2023-03-01 02:27:28 iteration: 114300 loss: 0.0026 lr: 0.02
2023-03-01 02:27:36 iteration: 114400 loss: 0.0024 lr: 0.02
2023-03-01 02:27:43 iteration: 114500 loss: 0.0027 lr: 0.02
2023-03-01 02:27:51 iteration: 114600 loss: 0.0029 lr: 0.02
2023-03-01 02:27:58 iteration: 114700 loss: 0.0029 lr: 0.02
2023-03-01 02:28:05 iteration: 114800 loss: 0.0027 lr: 0.02
2023-03-01 02:28:13 iteration: 114900 loss: 0.0026 lr: 0.02
2023-03-01 02:28:20 iteration: 115000 loss: 0.0028 lr: 0.02
2023-03-01 02:28:31 iteration: 115100 loss: 0.0028 lr: 0.02
2023-03-01 02:28:38 iteration: 115200 loss: 0.0027 lr: 0.02
2023-03-01 02:28:46 iteration: 115300 loss: 0.0028 lr: 0.02
2023-03-01 02:28:53 iteration: 115400 loss: 0.0028 lr: 0.02
2023-03-01 02:29:00 iteration: 115500 loss: 0.0027 lr: 0.02
2023-03-01 02:29:07 iteration: 115600 loss: 0.0027 lr: 0.02
2023-03-01 02:29:14 iteration: 115700 loss: 0.0026 lr: 0.02
2023-03-01 02:29:22 iteration: 115800 loss: 0.0029 lr: 0.02
2023-03-01 02:29:29 iteration: 115900 loss: 0.0027 lr: 0.02
2023-03-01 02:29:37 iteration: 116000 loss: 0.0028 lr: 0.02
2023-03-01 02:29:47 iteration: 116100 loss: 0.0025 lr: 0.02
2023-03-01 02:29:55 iteration: 116200 loss: 0.0027 lr: 0.02
2023-03-01 02:30:02 iteration: 116300 loss: 0.0026 lr: 0.02
2023-03-01 02:30:09 iteration: 116400 loss: 0.0028 lr: 0.02
2023-03-01 02:30:17 iteration: 116500 loss: 0.0026 lr: 0.02
2023-03-01 02:30:24 iteration: 116600 loss: 0.0025 lr: 0.02
2023-03-01 02:30:31 iteration: 116700 loss: 0.0025 lr: 0.02
2023-03-01 02:30:39 iteration: 116800 loss: 0.0027 lr: 0.02
2023-03-01 02:30:46 iteration: 116900 loss: 0.0026 lr: 0.02
2023-03-01 02:30:53 iteration: 117000 loss: 0.0027 lr: 0.02
2023-03-01 02:31:04 iteration: 117100 loss: 0.0026 lr: 0.02
2023-03-01 02:31:12 iteration: 117200 loss: 0.0025 lr: 0.02
2023-03-01 02:31:19 iteration: 117300 loss: 0.0028 lr: 0.02
2023-03-01 02:31:27 iteration: 117400 loss: 0.0027 lr: 0.02
2023-03-01 02:31:34 iteration: 117500 loss: 0.0027 lr: 0.02
2023-03-01 02:31:43 iteration: 117600 loss: 0.0027 lr: 0.02
2023-03-01 02:31:50 iteration: 117700 loss: 0.0025 lr: 0.02
2023-03-01 02:31:58 iteration: 117800 loss: 0.0027 lr: 0.02
2023-03-01 02:32:05 iteration: 117900 loss: 0.0026 lr: 0.02
2023-03-01 02:32:12 iteration: 118000 loss: 0.0026 lr: 0.02
2023-03-01 02:32:23 iteration: 118100 loss: 0.0027 lr: 0.02
2023-03-01 02:32:30 iteration: 118200 loss: 0.0027 lr: 0.02
2023-03-01 02:32:37 iteration: 118300 loss: 0.0027 lr: 0.02
2023-03-01 02:32:44 iteration: 118400 loss: 0.0024 lr: 0.02
2023-03-01 02:32:52 iteration: 118500 loss: 0.0026 lr: 0.02
2023-03-01 02:32:59 iteration: 118600 loss: 0.0025 lr: 0.02
2023-03-01 02:33:06 iteration: 118700 loss: 0.0030 lr: 0.02
2023-03-01 02:33:14 iteration: 118800 loss: 0.0026 lr: 0.02
2023-03-01 02:33:21 iteration: 118900 loss: 0.0027 lr: 0.02
2023-03-01 02:33:29 iteration: 119000 loss: 0.0027 lr: 0.02
2023-03-01 02:33:40 iteration: 119100 loss: 0.0026 lr: 0.02
2023-03-01 02:33:48 iteration: 119200 loss: 0.0027 lr: 0.02
2023-03-01 02:33:55 iteration: 119300 loss: 0.0028 lr: 0.02
2023-03-01 02:34:03 iteration: 119400 loss: 0.0031 lr: 0.02
2023-03-01 02:34:10 iteration: 119500 loss: 0.0026 lr: 0.02
2023-03-01 02:34:17 iteration: 119600 loss: 0.0030 lr: 0.02
2023-03-01 02:34:25 iteration: 119700 loss: 0.0027 lr: 0.02
2023-03-01 02:34:32 iteration: 119800 loss: 0.0028 lr: 0.02
2023-03-01 02:34:40 iteration: 119900 loss: 0.0030 lr: 0.02
2023-03-01 02:34:47 iteration: 120000 loss: 0.0026 lr: 0.02
2023-03-01 02:34:57 iteration: 120100 loss: 0.0028 lr: 0.02
2023-03-01 02:35:04 iteration: 120200 loss: 0.0025 lr: 0.02
2023-03-01 02:35:12 iteration: 120300 loss: 0.0027 lr: 0.02
2023-03-01 02:35:19 iteration: 120400 loss: 0.0029 lr: 0.02
2023-03-01 02:35:27 iteration: 120500 loss: 0.0026 lr: 0.02
2023-03-01 02:35:34 iteration: 120600 loss: 0.0025 lr: 0.02
2023-03-01 02:35:43 iteration: 120700 loss: 0.0028 lr: 0.02
2023-03-01 02:35:50 iteration: 120800 loss: 0.0024 lr: 0.02
2023-03-01 02:35:58 iteration: 120900 loss: 0.0027 lr: 0.02
2023-03-01 02:36:05 iteration: 121000 loss: 0.0028 lr: 0.02
2023-03-01 02:36:15 iteration: 121100 loss: 0.0024 lr: 0.02
2023-03-01 02:36:23 iteration: 121200 loss: 0.0028 lr: 0.02
2023-03-01 02:36:30 iteration: 121300 loss: 0.0028 lr: 0.02
2023-03-01 02:36:38 iteration: 121400 loss: 0.0026 lr: 0.02
2023-03-01 02:36:45 iteration: 121500 loss: 0.0025 lr: 0.02
2023-03-01 02:36:53 iteration: 121600 loss: 0.0027 lr: 0.02
2023-03-01 02:37:00 iteration: 121700 loss: 0.0028 lr: 0.02
2023-03-01 02:37:07 iteration: 121800 loss: 0.0025 lr: 0.02
2023-03-01 02:37:14 iteration: 121900 loss: 0.0024 lr: 0.02
2023-03-01 02:37:22 iteration: 122000 loss: 0.0026 lr: 0.02
2023-03-01 02:37:32 iteration: 122100 loss: 0.0026 lr: 0.02
2023-03-01 02:37:40 iteration: 122200 loss: 0.0026 lr: 0.02
2023-03-01 02:37:48 iteration: 122300 loss: 0.0026 lr: 0.02
2023-03-01 02:37:55 iteration: 122400 loss: 0.0025 lr: 0.02
2023-03-01 02:38:03 iteration: 122500 loss: 0.0028 lr: 0.02
2023-03-01 02:38:10 iteration: 122600 loss: 0.0026 lr: 0.02
2023-03-01 02:38:18 iteration: 122700 loss: 0.0026 lr: 0.02
2023-03-01 02:38:25 iteration: 122800 loss: 0.0025 lr: 0.02
2023-03-01 02:38:33 iteration: 122900 loss: 0.0025 lr: 0.02
2023-03-01 02:38:40 iteration: 123000 loss: 0.0026 lr: 0.02
2023-03-01 02:38:51 iteration: 123100 loss: 0.0025 lr: 0.02
2023-03-01 02:38:59 iteration: 123200 loss: 0.0024 lr: 0.02
2023-03-01 02:39:06 iteration: 123300 loss: 0.0026 lr: 0.02
2023-03-01 02:39:13 iteration: 123400 loss: 0.0032 lr: 0.02
2023-03-01 02:39:21 iteration: 123500 loss: 0.0027 lr: 0.02
2023-03-01 02:39:28 iteration: 123600 loss: 0.0027 lr: 0.02
2023-03-01 02:39:35 iteration: 123700 loss: 0.0025 lr: 0.02
2023-03-01 02:39:43 iteration: 123800 loss: 0.0026 lr: 0.02
2023-03-01 02:39:50 iteration: 123900 loss: 0.0027 lr: 0.02
2023-03-01 02:39:57 iteration: 124000 loss: 0.0025 lr: 0.02
2023-03-01 02:40:09 iteration: 124100 loss: 0.0026 lr: 0.02
2023-03-01 02:40:16 iteration: 124200 loss: 0.0026 lr: 0.02
2023-03-01 02:40:24 iteration: 124300 loss: 0.0027 lr: 0.02
2023-03-01 02:40:31 iteration: 124400 loss: 0.0026 lr: 0.02
2023-03-01 02:40:39 iteration: 124500 loss: 0.0028 lr: 0.02
2023-03-01 02:40:46 iteration: 124600 loss: 0.0026 lr: 0.02
2023-03-01 02:40:53 iteration: 124700 loss: 0.0027 lr: 0.02
2023-03-01 02:41:01 iteration: 124800 loss: 0.0028 lr: 0.02
2023-03-01 02:41:08 iteration: 124900 loss: 0.0027 lr: 0.02
2023-03-01 02:41:15 iteration: 125000 loss: 0.0026 lr: 0.02
2023-03-01 02:41:26 iteration: 125100 loss: 0.0026 lr: 0.02
2023-03-01 02:41:34 iteration: 125200 loss: 0.0025 lr: 0.02
2023-03-01 02:41:41 iteration: 125300 loss: 0.0024 lr: 0.02
2023-03-01 02:41:49 iteration: 125400 loss: 0.0025 lr: 0.02
2023-03-01 02:41:57 iteration: 125500 loss: 0.0027 lr: 0.02
2023-03-01 02:42:04 iteration: 125600 loss: 0.0026 lr: 0.02
2023-03-01 02:42:11 iteration: 125700 loss: 0.0026 lr: 0.02
2023-03-01 02:42:19 iteration: 125800 loss: 0.0027 lr: 0.02
2023-03-01 02:42:26 iteration: 125900 loss: 0.0025 lr: 0.02
2023-03-01 02:42:35 iteration: 126000 loss: 0.0028 lr: 0.02
2023-03-01 02:42:46 iteration: 126100 loss: 0.0028 lr: 0.02
2023-03-01 02:42:53 iteration: 126200 loss: 0.0026 lr: 0.02
2023-03-01 02:43:00 iteration: 126300 loss: 0.0024 lr: 0.02
2023-03-01 02:43:08 iteration: 126400 loss: 0.0025 lr: 0.02
2023-03-01 02:43:15 iteration: 126500 loss: 0.0027 lr: 0.02
2023-03-01 02:43:23 iteration: 126600 loss: 0.0026 lr: 0.02
2023-03-01 02:43:30 iteration: 126700 loss: 0.0027 lr: 0.02
2023-03-01 02:43:37 iteration: 126800 loss: 0.0029 lr: 0.02
2023-03-01 02:43:45 iteration: 126900 loss: 0.0028 lr: 0.02
2023-03-01 02:43:53 iteration: 127000 loss: 0.0027 lr: 0.02
2023-03-01 02:44:03 iteration: 127100 loss: 0.0026 lr: 0.02
2023-03-01 02:44:10 iteration: 127200 loss: 0.0031 lr: 0.02
2023-03-01 02:44:18 iteration: 127300 loss: 0.0027 lr: 0.02
2023-03-01 02:44:25 iteration: 127400 loss: 0.0026 lr: 0.02
2023-03-01 02:44:33 iteration: 127500 loss: 0.0026 lr: 0.02
2023-03-01 02:44:40 iteration: 127600 loss: 0.0027 lr: 0.02
2023-03-01 02:44:48 iteration: 127700 loss: 0.0029 lr: 0.02
2023-03-01 02:44:55 iteration: 127800 loss: 0.0027 lr: 0.02
2023-03-01 02:45:02 iteration: 127900 loss: 0.0027 lr: 0.02
2023-03-01 02:45:09 iteration: 128000 loss: 0.0024 lr: 0.02
2023-03-01 02:45:20 iteration: 128100 loss: 0.0027 lr: 0.02
2023-03-01 02:45:27 iteration: 128200 loss: 0.0025 lr: 0.02
2023-03-01 02:45:35 iteration: 128300 loss: 0.0025 lr: 0.02
2023-03-01 02:45:43 iteration: 128400 loss: 0.0027 lr: 0.02
2023-03-01 02:45:51 iteration: 128500 loss: 0.0026 lr: 0.02
2023-03-01 02:45:58 iteration: 128600 loss: 0.0027 lr: 0.02
2023-03-01 02:46:05 iteration: 128700 loss: 0.0027 lr: 0.02
2023-03-01 02:46:13 iteration: 128800 loss: 0.0025 lr: 0.02
2023-03-01 02:46:20 iteration: 128900 loss: 0.0026 lr: 0.02
2023-03-01 02:46:27 iteration: 129000 loss: 0.0027 lr: 0.02
2023-03-01 02:46:38 iteration: 129100 loss: 0.0028 lr: 0.02
2023-03-01 02:46:46 iteration: 129200 loss: 0.0026 lr: 0.02
2023-03-01 02:46:53 iteration: 129300 loss: 0.0029 lr: 0.02
2023-03-01 02:47:00 iteration: 129400 loss: 0.0026 lr: 0.02
2023-03-01 02:47:08 iteration: 129500 loss: 0.0027 lr: 0.02
2023-03-01 02:47:15 iteration: 129600 loss: 0.0026 lr: 0.02
2023-03-01 02:47:22 iteration: 129700 loss: 0.0025 lr: 0.02
2023-03-01 02:47:30 iteration: 129800 loss: 0.0026 lr: 0.02
2023-03-01 02:47:38 iteration: 129900 loss: 0.0023 lr: 0.02
2023-03-01 02:47:45 iteration: 130000 loss: 0.0027 lr: 0.02
2023-03-01 02:47:55 iteration: 130100 loss: 0.0026 lr: 0.02
2023-03-01 02:48:03 iteration: 130200 loss: 0.0025 lr: 0.02
2023-03-01 02:48:11 iteration: 130300 loss: 0.0026 lr: 0.02
2023-03-01 02:48:18 iteration: 130400 loss: 0.0027 lr: 0.02
2023-03-01 02:48:25 iteration: 130500 loss: 0.0028 lr: 0.02
2023-03-01 02:48:33 iteration: 130600 loss: 0.0027 lr: 0.02
2023-03-01 02:48:40 iteration: 130700 loss: 0.0026 lr: 0.02
2023-03-01 02:48:47 iteration: 130800 loss: 0.0026 lr: 0.02
2023-03-01 02:48:55 iteration: 130900 loss: 0.0026 lr: 0.02
2023-03-01 02:49:03 iteration: 131000 loss: 0.0026 lr: 0.02
2023-03-01 02:49:14 iteration: 131100 loss: 0.0027 lr: 0.02
2023-03-01 02:49:21 iteration: 131200 loss: 0.0028 lr: 0.02
2023-03-01 02:49:28 iteration: 131300 loss: 0.0024 lr: 0.02
2023-03-01 02:49:36 iteration: 131400 loss: 0.0027 lr: 0.02
2023-03-01 02:49:43 iteration: 131500 loss: 0.0026 lr: 0.02
2023-03-01 02:49:50 iteration: 131600 loss: 0.0024 lr: 0.02
2023-03-01 02:49:58 iteration: 131700 loss: 0.0027 lr: 0.02
2023-03-01 02:50:05 iteration: 131800 loss: 0.0026 lr: 0.02
2023-03-01 02:50:13 iteration: 131900 loss: 0.0024 lr: 0.02
2023-03-01 02:50:20 iteration: 132000 loss: 0.0024 lr: 0.02
2023-03-01 02:50:31 iteration: 132100 loss: 0.0029 lr: 0.02
2023-03-01 02:50:38 iteration: 132200 loss: 0.0025 lr: 0.02
2023-03-01 02:50:46 iteration: 132300 loss: 0.0026 lr: 0.02
2023-03-01 02:50:53 iteration: 132400 loss: 0.0027 lr: 0.02
2023-03-01 02:51:00 iteration: 132500 loss: 0.0027 lr: 0.02
2023-03-01 02:51:08 iteration: 132600 loss: 0.0025 lr: 0.02
2023-03-01 02:51:15 iteration: 132700 loss: 0.0026 lr: 0.02
2023-03-01 02:51:23 iteration: 132800 loss: 0.0027 lr: 0.02
2023-03-01 02:51:30 iteration: 132900 loss: 0.0027 lr: 0.02
2023-03-01 02:51:37 iteration: 133000 loss: 0.0025 lr: 0.02
2023-03-01 02:51:48 iteration: 133100 loss: 0.0026 lr: 0.02
2023-03-01 02:51:55 iteration: 133200 loss: 0.0026 lr: 0.02
2023-03-01 02:52:02 iteration: 133300 loss: 0.0026 lr: 0.02
2023-03-01 02:52:10 iteration: 133400 loss: 0.0026 lr: 0.02
2023-03-01 02:52:17 iteration: 133500 loss: 0.0025 lr: 0.02
2023-03-01 02:52:24 iteration: 133600 loss: 0.0022 lr: 0.02
2023-03-01 02:52:32 iteration: 133700 loss: 0.0025 lr: 0.02
2023-03-01 02:52:39 iteration: 133800 loss: 0.0025 lr: 0.02
2023-03-01 02:52:46 iteration: 133900 loss: 0.0025 lr: 0.02
2023-03-01 02:52:53 iteration: 134000 loss: 0.0025 lr: 0.02
2023-03-01 02:53:04 iteration: 134100 loss: 0.0024 lr: 0.02
2023-03-01 02:53:12 iteration: 134200 loss: 0.0026 lr: 0.02
2023-03-01 02:53:19 iteration: 134300 loss: 0.0025 lr: 0.02
2023-03-01 02:53:26 iteration: 134400 loss: 0.0025 lr: 0.02
2023-03-01 02:53:34 iteration: 134500 loss: 0.0028 lr: 0.02
2023-03-01 02:53:42 iteration: 134600 loss: 0.0024 lr: 0.02
2023-03-01 02:53:49 iteration: 134700 loss: 0.0024 lr: 0.02
2023-03-01 02:53:57 iteration: 134800 loss: 0.0025 lr: 0.02
2023-03-01 02:54:05 iteration: 134900 loss: 0.0027 lr: 0.02
2023-03-01 02:54:13 iteration: 135000 loss: 0.0024 lr: 0.02
2023-03-01 02:54:24 iteration: 135100 loss: 0.0025 lr: 0.02
2023-03-01 02:54:32 iteration: 135200 loss: 0.0027 lr: 0.02
2023-03-01 02:54:39 iteration: 135300 loss: 0.0025 lr: 0.02
2023-03-01 02:54:46 iteration: 135400 loss: 0.0024 lr: 0.02
2023-03-01 02:54:54 iteration: 135500 loss: 0.0025 lr: 0.02
2023-03-01 02:55:01 iteration: 135600 loss: 0.0027 lr: 0.02
2023-03-01 02:55:08 iteration: 135700 loss: 0.0024 lr: 0.02
2023-03-01 02:55:16 iteration: 135800 loss: 0.0023 lr: 0.02
2023-03-01 02:55:23 iteration: 135900 loss: 0.0027 lr: 0.02
2023-03-01 02:55:31 iteration: 136000 loss: 0.0025 lr: 0.02
2023-03-01 02:55:42 iteration: 136100 loss: 0.0026 lr: 0.02
2023-03-01 02:55:49 iteration: 136200 loss: 0.0027 lr: 0.02
2023-03-01 02:55:57 iteration: 136300 loss: 0.0026 lr: 0.02
2023-03-01 02:56:04 iteration: 136400 loss: 0.0024 lr: 0.02
2023-03-01 02:56:12 iteration: 136500 loss: 0.0026 lr: 0.02
2023-03-01 02:56:19 iteration: 136600 loss: 0.0027 lr: 0.02
2023-03-01 02:56:27 iteration: 136700 loss: 0.0028 lr: 0.02
2023-03-01 02:56:34 iteration: 136800 loss: 0.0024 lr: 0.02
2023-03-01 02:56:42 iteration: 136900 loss: 0.0025 lr: 0.02
2023-03-01 02:56:49 iteration: 137000 loss: 0.0028 lr: 0.02
2023-03-01 02:57:00 iteration: 137100 loss: 0.0025 lr: 0.02
2023-03-01 02:57:07 iteration: 137200 loss: 0.0028 lr: 0.02
2023-03-01 02:57:14 iteration: 137300 loss: 0.0025 lr: 0.02
2023-03-01 02:57:22 iteration: 137400 loss: 0.0026 lr: 0.02
2023-03-01 02:57:29 iteration: 137500 loss: 0.0023 lr: 0.02
2023-03-01 02:57:37 iteration: 137600 loss: 0.0025 lr: 0.02
2023-03-01 02:57:44 iteration: 137700 loss: 0.0025 lr: 0.02
2023-03-01 02:57:51 iteration: 137800 loss: 0.0027 lr: 0.02
2023-03-01 02:57:59 iteration: 137900 loss: 0.0025 lr: 0.02
2023-03-01 02:58:06 iteration: 138000 loss: 0.0025 lr: 0.02
2023-03-01 02:58:17 iteration: 138100 loss: 0.0025 lr: 0.02
2023-03-01 02:58:24 iteration: 138200 loss: 0.0027 lr: 0.02
2023-03-01 02:58:32 iteration: 138300 loss: 0.0025 lr: 0.02
2023-03-01 02:58:39 iteration: 138400 loss: 0.0023 lr: 0.02
2023-03-01 02:58:47 iteration: 138500 loss: 0.0025 lr: 0.02
2023-03-01 02:58:54 iteration: 138600 loss: 0.0024 lr: 0.02
2023-03-01 02:59:02 iteration: 138700 loss: 0.0027 lr: 0.02
2023-03-01 02:59:09 iteration: 138800 loss: 0.0024 lr: 0.02
2023-03-01 02:59:17 iteration: 138900 loss: 0.0026 lr: 0.02
2023-03-01 02:59:24 iteration: 139000 loss: 0.0026 lr: 0.02
2023-03-01 02:59:34 iteration: 139100 loss: 0.0024 lr: 0.02
2023-03-01 02:59:42 iteration: 139200 loss: 0.0025 lr: 0.02
2023-03-01 02:59:49 iteration: 139300 loss: 0.0025 lr: 0.02
2023-03-01 02:59:57 iteration: 139400 loss: 0.0023 lr: 0.02
2023-03-01 03:00:04 iteration: 139500 loss: 0.0025 lr: 0.02
2023-03-01 03:00:12 iteration: 139600 loss: 0.0025 lr: 0.02
2023-03-01 03:00:19 iteration: 139700 loss: 0.0026 lr: 0.02
2023-03-01 03:00:27 iteration: 139800 loss: 0.0025 lr: 0.02
2023-03-01 03:00:35 iteration: 139900 loss: 0.0026 lr: 0.02
2023-03-01 03:00:42 iteration: 140000 loss: 0.0025 lr: 0.02
2023-03-01 03:00:53 iteration: 140100 loss: 0.0024 lr: 0.02
2023-03-01 03:01:00 iteration: 140200 loss: 0.0025 lr: 0.02
2023-03-01 03:01:08 iteration: 140300 loss: 0.0026 lr: 0.02
2023-03-01 03:01:15 iteration: 140400 loss: 0.0023 lr: 0.02
2023-03-01 03:01:23 iteration: 140500 loss: 0.0027 lr: 0.02
2023-03-01 03:01:30 iteration: 140600 loss: 0.0023 lr: 0.02
2023-03-01 03:01:38 iteration: 140700 loss: 0.0025 lr: 0.02
2023-03-01 03:01:46 iteration: 140800 loss: 0.0028 lr: 0.02
2023-03-01 03:01:53 iteration: 140900 loss: 0.0024 lr: 0.02
2023-03-01 03:02:01 iteration: 141000 loss: 0.0027 lr: 0.02
2023-03-01 03:02:11 iteration: 141100 loss: 0.0024 lr: 0.02
2023-03-01 03:02:19 iteration: 141200 loss: 0.0028 lr: 0.02
2023-03-01 03:02:26 iteration: 141300 loss: 0.0025 lr: 0.02
2023-03-01 03:02:34 iteration: 141400 loss: 0.0022 lr: 0.02
2023-03-01 03:02:41 iteration: 141500 loss: 0.0027 lr: 0.02
2023-03-01 03:02:48 iteration: 141600 loss: 0.0025 lr: 0.02
2023-03-01 03:02:56 iteration: 141700 loss: 0.0026 lr: 0.02
2023-03-01 03:03:03 iteration: 141800 loss: 0.0026 lr: 0.02
2023-03-01 03:03:11 iteration: 141900 loss: 0.0026 lr: 0.02
2023-03-01 03:03:18 iteration: 142000 loss: 0.0025 lr: 0.02
2023-03-01 03:03:28 iteration: 142100 loss: 0.0024 lr: 0.02
2023-03-01 03:03:36 iteration: 142200 loss: 0.0025 lr: 0.02
2023-03-01 03:03:43 iteration: 142300 loss: 0.0024 lr: 0.02
2023-03-01 03:03:50 iteration: 142400 loss: 0.0025 lr: 0.02
2023-03-01 03:03:58 iteration: 142500 loss: 0.0025 lr: 0.02
2023-03-01 03:04:05 iteration: 142600 loss: 0.0026 lr: 0.02
2023-03-01 03:04:13 iteration: 142700 loss: 0.0026 lr: 0.02
2023-03-01 03:04:20 iteration: 142800 loss: 0.0025 lr: 0.02
2023-03-01 03:04:28 iteration: 142900 loss: 0.0026 lr: 0.02
2023-03-01 03:04:35 iteration: 143000 loss: 0.0024 lr: 0.02
2023-03-01 03:04:46 iteration: 143100 loss: 0.0024 lr: 0.02
2023-03-01 03:04:53 iteration: 143200 loss: 0.0023 lr: 0.02
2023-03-01 03:05:01 iteration: 143300 loss: 0.0024 lr: 0.02
2023-03-01 03:05:08 iteration: 143400 loss: 0.0024 lr: 0.02
2023-03-01 03:05:15 iteration: 143500 loss: 0.0023 lr: 0.02
2023-03-01 03:05:23 iteration: 143600 loss: 0.0025 lr: 0.02
2023-03-01 03:05:30 iteration: 143700 loss: 0.0025 lr: 0.02
2023-03-01 03:05:38 iteration: 143800 loss: 0.0026 lr: 0.02
2023-03-01 03:05:45 iteration: 143900 loss: 0.0024 lr: 0.02
2023-03-01 03:05:53 iteration: 144000 loss: 0.0022 lr: 0.02
2023-03-01 03:06:03 iteration: 144100 loss: 0.0024 lr: 0.02
2023-03-01 03:06:10 iteration: 144200 loss: 0.0026 lr: 0.02
2023-03-01 03:06:18 iteration: 144300 loss: 0.0028 lr: 0.02
2023-03-01 03:06:25 iteration: 144400 loss: 0.0025 lr: 0.02
2023-03-01 03:06:33 iteration: 144500 loss: 0.0024 lr: 0.02
2023-03-01 03:06:40 iteration: 144600 loss: 0.0024 lr: 0.02
2023-03-01 03:06:49 iteration: 144700 loss: 0.0025 lr: 0.02
2023-03-01 03:06:57 iteration: 144800 loss: 0.0025 lr: 0.02
2023-03-01 03:07:04 iteration: 144900 loss: 0.0026 lr: 0.02
2023-03-01 03:07:11 iteration: 145000 loss: 0.0024 lr: 0.02
2023-03-01 03:07:22 iteration: 145100 loss: 0.0024 lr: 0.02
2023-03-01 03:07:29 iteration: 145200 loss: 0.0025 lr: 0.02
2023-03-01 03:07:36 iteration: 145300 loss: 0.0024 lr: 0.02
2023-03-01 03:07:44 iteration: 145400 loss: 0.0025 lr: 0.02
2023-03-01 03:07:51 iteration: 145500 loss: 0.0026 lr: 0.02
2023-03-01 03:07:58 iteration: 145600 loss: 0.0026 lr: 0.02
2023-03-01 03:08:06 iteration: 145700 loss: 0.0025 lr: 0.02
2023-03-01 03:08:13 iteration: 145800 loss: 0.0023 lr: 0.02
2023-03-01 03:08:20 iteration: 145900 loss: 0.0024 lr: 0.02
2023-03-01 03:08:28 iteration: 146000 loss: 0.0026 lr: 0.02
2023-03-01 03:08:38 iteration: 146100 loss: 0.0023 lr: 0.02
2023-03-01 03:08:46 iteration: 146200 loss: 0.0023 lr: 0.02
2023-03-01 03:08:54 iteration: 146300 loss: 0.0025 lr: 0.02
2023-03-01 03:09:01 iteration: 146400 loss: 0.0025 lr: 0.02
2023-03-01 03:09:08 iteration: 146500 loss: 0.0024 lr: 0.02
2023-03-01 03:09:16 iteration: 146600 loss: 0.0022 lr: 0.02
2023-03-01 03:09:23 iteration: 146700 loss: 0.0024 lr: 0.02
2023-03-01 03:09:30 iteration: 146800 loss: 0.0023 lr: 0.02
2023-03-01 03:09:38 iteration: 146900 loss: 0.0023 lr: 0.02
2023-03-01 03:09:46 iteration: 147000 loss: 0.0025 lr: 0.02
2023-03-01 03:09:57 iteration: 147100 loss: 0.0023 lr: 0.02
2023-03-01 03:10:04 iteration: 147200 loss: 0.0025 lr: 0.02
2023-03-01 03:10:11 iteration: 147300 loss: 0.0025 lr: 0.02
2023-03-01 03:10:19 iteration: 147400 loss: 0.0026 lr: 0.02
2023-03-01 03:10:26 iteration: 147500 loss: 0.0023 lr: 0.02
2023-03-01 03:10:33 iteration: 147600 loss: 0.0024 lr: 0.02
2023-03-01 03:10:41 iteration: 147700 loss: 0.0027 lr: 0.02
2023-03-01 03:10:48 iteration: 147800 loss: 0.0023 lr: 0.02
2023-03-01 03:10:56 iteration: 147900 loss: 0.0026 lr: 0.02
2023-03-01 03:11:03 iteration: 148000 loss: 0.0026 lr: 0.02
2023-03-01 03:11:14 iteration: 148100 loss: 0.0023 lr: 0.02
2023-03-01 03:11:22 iteration: 148200 loss: 0.0026 lr: 0.02
2023-03-01 03:11:30 iteration: 148300 loss: 0.0025 lr: 0.02
2023-03-01 03:11:37 iteration: 148400 loss: 0.0025 lr: 0.02
2023-03-01 03:11:44 iteration: 148500 loss: 0.0025 lr: 0.02
2023-03-01 03:11:51 iteration: 148600 loss: 0.0025 lr: 0.02
2023-03-01 03:11:59 iteration: 148700 loss: 0.0024 lr: 0.02
2023-03-01 03:12:06 iteration: 148800 loss: 0.0024 lr: 0.02
2023-03-01 03:12:14 iteration: 148900 loss: 0.0025 lr: 0.02
2023-03-01 03:12:21 iteration: 149000 loss: 0.0025 lr: 0.02
2023-03-01 03:12:32 iteration: 149100 loss: 0.0023 lr: 0.02
2023-03-01 03:12:39 iteration: 149200 loss: 0.0025 lr: 0.02
2023-03-01 03:12:47 iteration: 149300 loss: 0.0024 lr: 0.02
2023-03-01 03:12:54 iteration: 149400 loss: 0.0023 lr: 0.02
2023-03-01 03:13:01 iteration: 149500 loss: 0.0024 lr: 0.02
2023-03-01 03:13:09 iteration: 149600 loss: 0.0022 lr: 0.02
2023-03-01 03:13:16 iteration: 149700 loss: 0.0024 lr: 0.02
2023-03-01 03:13:23 iteration: 149800 loss: 0.0026 lr: 0.02
2023-03-01 03:13:30 iteration: 149900 loss: 0.0023 lr: 0.02
2023-03-01 03:13:38 iteration: 150000 loss: 0.0022 lr: 0.02
2023-03-01 03:13:49 iteration: 150100 loss: 0.0023 lr: 0.02
2023-03-01 03:13:56 iteration: 150200 loss: 0.0023 lr: 0.02
2023-03-01 03:14:04 iteration: 150300 loss: 0.0025 lr: 0.02
2023-03-01 03:14:11 iteration: 150400 loss: 0.0025 lr: 0.02
2023-03-01 03:14:18 iteration: 150500 loss: 0.0024 lr: 0.02
2023-03-01 03:14:26 iteration: 150600 loss: 0.0024 lr: 0.02
2023-03-01 03:14:33 iteration: 150700 loss: 0.0025 lr: 0.02
2023-03-01 03:14:41 iteration: 150800 loss: 0.0025 lr: 0.02
2023-03-01 03:14:48 iteration: 150900 loss: 0.0023 lr: 0.02
2023-03-01 03:14:55 iteration: 151000 loss: 0.0022 lr: 0.02
2023-03-01 03:15:06 iteration: 151100 loss: 0.0024 lr: 0.02
2023-03-01 03:15:13 iteration: 151200 loss: 0.0026 lr: 0.02
2023-03-01 03:15:21 iteration: 151300 loss: 0.0024 lr: 0.02
2023-03-01 03:15:28 iteration: 151400 loss: 0.0024 lr: 0.02
2023-03-01 03:15:36 iteration: 151500 loss: 0.0025 lr: 0.02
2023-03-01 03:15:44 iteration: 151600 loss: 0.0024 lr: 0.02
2023-03-01 03:15:51 iteration: 151700 loss: 0.0024 lr: 0.02
2023-03-01 03:15:59 iteration: 151800 loss: 0.0024 lr: 0.02
2023-03-01 03:16:06 iteration: 151900 loss: 0.0022 lr: 0.02
2023-03-01 03:16:14 iteration: 152000 loss: 0.0024 lr: 0.02
2023-03-01 03:16:24 iteration: 152100 loss: 0.0026 lr: 0.02
2023-03-01 03:16:32 iteration: 152200 loss: 0.0024 lr: 0.02
2023-03-01 03:16:39 iteration: 152300 loss: 0.0025 lr: 0.02
2023-03-01 03:16:47 iteration: 152400 loss: 0.0025 lr: 0.02
2023-03-01 03:16:54 iteration: 152500 loss: 0.0022 lr: 0.02
2023-03-01 03:17:01 iteration: 152600 loss: 0.0023 lr: 0.02
2023-03-01 03:17:09 iteration: 152700 loss: 0.0022 lr: 0.02
2023-03-01 03:17:16 iteration: 152800 loss: 0.0026 lr: 0.02
2023-03-01 03:17:23 iteration: 152900 loss: 0.0026 lr: 0.02
2023-03-01 03:17:30 iteration: 153000 loss: 0.0023 lr: 0.02
2023-03-01 03:17:42 iteration: 153100 loss: 0.0024 lr: 0.02
2023-03-01 03:17:49 iteration: 153200 loss: 0.0025 lr: 0.02
2023-03-01 03:17:56 iteration: 153300 loss: 0.0025 lr: 0.02
2023-03-01 03:18:04 iteration: 153400 loss: 0.0026 lr: 0.02
2023-03-01 03:18:11 iteration: 153500 loss: 0.0025 lr: 0.02
2023-03-01 03:18:18 iteration: 153600 loss: 0.0024 lr: 0.02
2023-03-01 03:18:26 iteration: 153700 loss: 0.0024 lr: 0.02
2023-03-01 03:18:33 iteration: 153800 loss: 0.0026 lr: 0.02
2023-03-01 03:18:41 iteration: 153900 loss: 0.0026 lr: 0.02
2023-03-01 03:18:48 iteration: 154000 loss: 0.0024 lr: 0.02
2023-03-01 03:18:58 iteration: 154100 loss: 0.0026 lr: 0.02
2023-03-01 03:19:06 iteration: 154200 loss: 0.0024 lr: 0.02
2023-03-01 03:19:13 iteration: 154300 loss: 0.0023 lr: 0.02
2023-03-01 03:19:20 iteration: 154400 loss: 0.0022 lr: 0.02
2023-03-01 03:19:28 iteration: 154500 loss: 0.0027 lr: 0.02
2023-03-01 03:19:35 iteration: 154600 loss: 0.0022 lr: 0.02
2023-03-01 03:19:43 iteration: 154700 loss: 0.0025 lr: 0.02
2023-03-01 03:19:50 iteration: 154800 loss: 0.0021 lr: 0.02
2023-03-01 03:19:57 iteration: 154900 loss: 0.0024 lr: 0.02
2023-03-01 03:20:05 iteration: 155000 loss: 0.0024 lr: 0.02
2023-03-01 03:20:15 iteration: 155100 loss: 0.0022 lr: 0.02
2023-03-01 03:20:23 iteration: 155200 loss: 0.0022 lr: 0.02
2023-03-01 03:20:31 iteration: 155300 loss: 0.0024 lr: 0.02
2023-03-01 03:20:38 iteration: 155400 loss: 0.0025 lr: 0.02
2023-03-01 03:20:46 iteration: 155500 loss: 0.0024 lr: 0.02
2023-03-01 03:20:53 iteration: 155600 loss: 0.0024 lr: 0.02
2023-03-01 03:21:01 iteration: 155700 loss: 0.0026 lr: 0.02
2023-03-01 03:21:08 iteration: 155800 loss: 0.0025 lr: 0.02
2023-03-01 03:21:15 iteration: 155900 loss: 0.0024 lr: 0.02
2023-03-01 03:21:23 iteration: 156000 loss: 0.0024 lr: 0.02
2023-03-01 03:21:33 iteration: 156100 loss: 0.0026 lr: 0.02
2023-03-01 03:21:41 iteration: 156200 loss: 0.0022 lr: 0.02
2023-03-01 03:21:48 iteration: 156300 loss: 0.0025 lr: 0.02
2023-03-01 03:21:56 iteration: 156400 loss: 0.0027 lr: 0.02
2023-03-01 03:22:03 iteration: 156500 loss: 0.0025 lr: 0.02
2023-03-01 03:22:10 iteration: 156600 loss: 0.0029 lr: 0.02
2023-03-01 03:22:18 iteration: 156700 loss: 0.0026 lr: 0.02
2023-03-01 03:22:25 iteration: 156800 loss: 0.0023 lr: 0.02
2023-03-01 03:22:33 iteration: 156900 loss: 0.0023 lr: 0.02
2023-03-01 03:22:40 iteration: 157000 loss: 0.0028 lr: 0.02
2023-03-01 03:22:50 iteration: 157100 loss: 0.0024 lr: 0.02
2023-03-01 03:22:58 iteration: 157200 loss: 0.0026 lr: 0.02
2023-03-01 03:23:05 iteration: 157300 loss: 0.0026 lr: 0.02
2023-03-01 03:23:12 iteration: 157400 loss: 0.0023 lr: 0.02
2023-03-01 03:23:20 iteration: 157500 loss: 0.0025 lr: 0.02
2023-03-01 03:23:27 iteration: 157600 loss: 0.0023 lr: 0.02
2023-03-01 03:23:34 iteration: 157700 loss: 0.0023 lr: 0.02
2023-03-01 03:23:43 iteration: 157800 loss: 0.0022 lr: 0.02
2023-03-01 03:23:50 iteration: 157900 loss: 0.0023 lr: 0.02
2023-03-01 03:23:57 iteration: 158000 loss: 0.0025 lr: 0.02
2023-03-01 03:24:08 iteration: 158100 loss: 0.0025 lr: 0.02
2023-03-01 03:24:15 iteration: 158200 loss: 0.0025 lr: 0.02
2023-03-01 03:24:22 iteration: 158300 loss: 0.0024 lr: 0.02
2023-03-01 03:24:30 iteration: 158400 loss: 0.0023 lr: 0.02
2023-03-01 03:24:38 iteration: 158500 loss: 0.0024 lr: 0.02
2023-03-01 03:24:45 iteration: 158600 loss: 0.0025 lr: 0.02
2023-03-01 03:24:52 iteration: 158700 loss: 0.0022 lr: 0.02
2023-03-01 03:24:59 iteration: 158800 loss: 0.0025 lr: 0.02
2023-03-01 03:25:06 iteration: 158900 loss: 0.0023 lr: 0.02
2023-03-01 03:25:14 iteration: 159000 loss: 0.0025 lr: 0.02
2023-03-01 03:25:25 iteration: 159100 loss: 0.0023 lr: 0.02
2023-03-01 03:25:32 iteration: 159200 loss: 0.0022 lr: 0.02
2023-03-01 03:25:39 iteration: 159300 loss: 0.0025 lr: 0.02
2023-03-01 03:25:47 iteration: 159400 loss: 0.0024 lr: 0.02
2023-03-01 03:25:54 iteration: 159500 loss: 0.0022 lr: 0.02
2023-03-01 03:26:01 iteration: 159600 loss: 0.0023 lr: 0.02
2023-03-01 03:26:09 iteration: 159700 loss: 0.0023 lr: 0.02
2023-03-01 03:26:17 iteration: 159800 loss: 0.0025 lr: 0.02
2023-03-01 03:26:24 iteration: 159900 loss: 0.0026 lr: 0.02
2023-03-01 03:26:31 iteration: 160000 loss: 0.0024 lr: 0.02
2023-03-01 03:26:42 iteration: 160100 loss: 0.0023 lr: 0.02
2023-03-01 03:26:49 iteration: 160200 loss: 0.0023 lr: 0.02
2023-03-01 03:26:56 iteration: 160300 loss: 0.0023 lr: 0.02
2023-03-01 03:27:04 iteration: 160400 loss: 0.0022 lr: 0.02
2023-03-01 03:27:11 iteration: 160500 loss: 0.0024 lr: 0.02
2023-03-01 03:27:18 iteration: 160600 loss: 0.0025 lr: 0.02
2023-03-01 03:27:26 iteration: 160700 loss: 0.0023 lr: 0.02
2023-03-01 03:27:33 iteration: 160800 loss: 0.0026 lr: 0.02
2023-03-01 03:27:40 iteration: 160900 loss: 0.0026 lr: 0.02
2023-03-01 03:27:48 iteration: 161000 loss: 0.0025 lr: 0.02
2023-03-01 03:27:58 iteration: 161100 loss: 0.0024 lr: 0.02
2023-03-01 03:28:06 iteration: 161200 loss: 0.0023 lr: 0.02
2023-03-01 03:28:13 iteration: 161300 loss: 0.0024 lr: 0.02
2023-03-01 03:28:21 iteration: 161400 loss: 0.0025 lr: 0.02
2023-03-01 03:28:28 iteration: 161500 loss: 0.0024 lr: 0.02
2023-03-01 03:28:36 iteration: 161600 loss: 0.0026 lr: 0.02
2023-03-01 03:28:43 iteration: 161700 loss: 0.0023 lr: 0.02
2023-03-01 03:28:50 iteration: 161800 loss: 0.0024 lr: 0.02
2023-03-01 03:28:58 iteration: 161900 loss: 0.0024 lr: 0.02
2023-03-01 03:29:05 iteration: 162000 loss: 0.0022 lr: 0.02
2023-03-01 03:29:16 iteration: 162100 loss: 0.0025 lr: 0.02
2023-03-01 03:29:23 iteration: 162200 loss: 0.0024 lr: 0.02
2023-03-01 03:29:30 iteration: 162300 loss: 0.0025 lr: 0.02
2023-03-01 03:29:38 iteration: 162400 loss: 0.0024 lr: 0.02
2023-03-01 03:29:45 iteration: 162500 loss: 0.0022 lr: 0.02
2023-03-01 03:29:53 iteration: 162600 loss: 0.0026 lr: 0.02
2023-03-01 03:30:00 iteration: 162700 loss: 0.0023 lr: 0.02
2023-03-01 03:30:08 iteration: 162800 loss: 0.0023 lr: 0.02
2023-03-01 03:30:15 iteration: 162900 loss: 0.0022 lr: 0.02
2023-03-01 03:30:22 iteration: 163000 loss: 0.0023 lr: 0.02
2023-03-01 03:30:33 iteration: 163100 loss: 0.0024 lr: 0.02
2023-03-01 03:30:40 iteration: 163200 loss: 0.0023 lr: 0.02
2023-03-01 03:30:48 iteration: 163300 loss: 0.0023 lr: 0.02
2023-03-01 03:30:55 iteration: 163400 loss: 0.0023 lr: 0.02
2023-03-01 03:31:02 iteration: 163500 loss: 0.0023 lr: 0.02
2023-03-01 03:31:11 iteration: 163600 loss: 0.0023 lr: 0.02
2023-03-01 03:31:18 iteration: 163700 loss: 0.0025 lr: 0.02
2023-03-01 03:31:26 iteration: 163800 loss: 0.0023 lr: 0.02
2023-03-01 03:31:33 iteration: 163900 loss: 0.0024 lr: 0.02
2023-03-01 03:31:41 iteration: 164000 loss: 0.0027 lr: 0.02
2023-03-01 03:31:52 iteration: 164100 loss: 0.0024 lr: 0.02
2023-03-01 03:31:59 iteration: 164200 loss: 0.0026 lr: 0.02
2023-03-01 03:32:07 iteration: 164300 loss: 0.0023 lr: 0.02
2023-03-01 03:32:14 iteration: 164400 loss: 0.0023 lr: 0.02
2023-03-01 03:32:22 iteration: 164500 loss: 0.0026 lr: 0.02
2023-03-01 03:32:29 iteration: 164600 loss: 0.0023 lr: 0.02
2023-03-01 03:32:36 iteration: 164700 loss: 0.0026 lr: 0.02
2023-03-01 03:32:44 iteration: 164800 loss: 0.0025 lr: 0.02
2023-03-01 03:32:51 iteration: 164900 loss: 0.0021 lr: 0.02
2023-03-01 03:32:58 iteration: 165000 loss: 0.0024 lr: 0.02
2023-03-01 03:33:09 iteration: 165100 loss: 0.0023 lr: 0.02
2023-03-01 03:33:17 iteration: 165200 loss: 0.0027 lr: 0.02
2023-03-01 03:33:24 iteration: 165300 loss: 0.0022 lr: 0.02
2023-03-01 03:33:32 iteration: 165400 loss: 0.0025 lr: 0.02
2023-03-01 03:33:39 iteration: 165500 loss: 0.0024 lr: 0.02
2023-03-01 03:33:47 iteration: 165600 loss: 0.0025 lr: 0.02
2023-03-01 03:33:54 iteration: 165700 loss: 0.0025 lr: 0.02
2023-03-01 03:34:01 iteration: 165800 loss: 0.0026 lr: 0.02
2023-03-01 03:34:09 iteration: 165900 loss: 0.0026 lr: 0.02
2023-03-01 03:34:16 iteration: 166000 loss: 0.0025 lr: 0.02
2023-03-01 03:34:27 iteration: 166100 loss: 0.0021 lr: 0.02
2023-03-01 03:34:34 iteration: 166200 loss: 0.0022 lr: 0.02
2023-03-01 03:34:41 iteration: 166300 loss: 0.0022 lr: 0.02
2023-03-01 03:34:49 iteration: 166400 loss: 0.0022 lr: 0.02
2023-03-01 03:34:56 iteration: 166500 loss: 0.0023 lr: 0.02
2023-03-01 03:35:03 iteration: 166600 loss: 0.0023 lr: 0.02
2023-03-01 03:35:11 iteration: 166700 loss: 0.0023 lr: 0.02
2023-03-01 03:35:18 iteration: 166800 loss: 0.0025 lr: 0.02
2023-03-01 03:35:25 iteration: 166900 loss: 0.0024 lr: 0.02
2023-03-01 03:35:33 iteration: 167000 loss: 0.0024 lr: 0.02
2023-03-01 03:35:44 iteration: 167100 loss: 0.0026 lr: 0.02
2023-03-01 03:35:52 iteration: 167200 loss: 0.0022 lr: 0.02
2023-03-01 03:35:59 iteration: 167300 loss: 0.0025 lr: 0.02
2023-03-01 03:36:06 iteration: 167400 loss: 0.0026 lr: 0.02
2023-03-01 03:36:13 iteration: 167500 loss: 0.0024 lr: 0.02
2023-03-01 03:36:21 iteration: 167600 loss: 0.0023 lr: 0.02
2023-03-01 03:36:28 iteration: 167700 loss: 0.0024 lr: 0.02
2023-03-01 03:36:36 iteration: 167800 loss: 0.0023 lr: 0.02
2023-03-01 03:36:43 iteration: 167900 loss: 0.0023 lr: 0.02
2023-03-01 03:36:50 iteration: 168000 loss: 0.0023 lr: 0.02
2023-03-01 03:37:01 iteration: 168100 loss: 0.0022 lr: 0.02
2023-03-01 03:37:08 iteration: 168200 loss: 0.0024 lr: 0.02
2023-03-01 03:37:16 iteration: 168300 loss: 0.0023 lr: 0.02
2023-03-01 03:37:23 iteration: 168400 loss: 0.0024 lr: 0.02
2023-03-01 03:37:30 iteration: 168500 loss: 0.0023 lr: 0.02
2023-03-01 03:37:38 iteration: 168600 loss: 0.0022 lr: 0.02
2023-03-01 03:37:45 iteration: 168700 loss: 0.0024 lr: 0.02
2023-03-01 03:37:52 iteration: 168800 loss: 0.0023 lr: 0.02
2023-03-01 03:38:00 iteration: 168900 loss: 0.0023 lr: 0.02
2023-03-01 03:38:07 iteration: 169000 loss: 0.0022 lr: 0.02
2023-03-01 03:38:17 iteration: 169100 loss: 0.0022 lr: 0.02
2023-03-01 03:38:25 iteration: 169200 loss: 0.0023 lr: 0.02
2023-03-01 03:38:32 iteration: 169300 loss: 0.0023 lr: 0.02
2023-03-01 03:38:40 iteration: 169400 loss: 0.0024 lr: 0.02
2023-03-01 03:38:47 iteration: 169500 loss: 0.0023 lr: 0.02
2023-03-01 03:38:54 iteration: 169600 loss: 0.0023 lr: 0.02
2023-03-01 03:39:01 iteration: 169700 loss: 0.0023 lr: 0.02
2023-03-01 03:39:09 iteration: 169800 loss: 0.0021 lr: 0.02
2023-03-01 03:39:16 iteration: 169900 loss: 0.0023 lr: 0.02
2023-03-01 03:39:24 iteration: 170000 loss: 0.0024 lr: 0.02
2023-03-01 03:39:35 iteration: 170100 loss: 0.0023 lr: 0.02
2023-03-01 03:39:42 iteration: 170200 loss: 0.0023 lr: 0.02
2023-03-01 03:39:50 iteration: 170300 loss: 0.0023 lr: 0.02
2023-03-01 03:39:57 iteration: 170400 loss: 0.0023 lr: 0.02
2023-03-01 03:40:04 iteration: 170500 loss: 0.0021 lr: 0.02
2023-03-01 03:40:12 iteration: 170600 loss: 0.0023 lr: 0.02
2023-03-01 03:40:19 iteration: 170700 loss: 0.0023 lr: 0.02
2023-03-01 03:40:26 iteration: 170800 loss: 0.0022 lr: 0.02
2023-03-01 03:40:34 iteration: 170900 loss: 0.0021 lr: 0.02
2023-03-01 03:40:41 iteration: 171000 loss: 0.0025 lr: 0.02
2023-03-01 03:40:52 iteration: 171100 loss: 0.0022 lr: 0.02
2023-03-01 03:40:59 iteration: 171200 loss: 0.0022 lr: 0.02
2023-03-01 03:41:07 iteration: 171300 loss: 0.0023 lr: 0.02
2023-03-01 03:41:14 iteration: 171400 loss: 0.0022 lr: 0.02
2023-03-01 03:41:22 iteration: 171500 loss: 0.0025 lr: 0.02
2023-03-01 03:41:29 iteration: 171600 loss: 0.0024 lr: 0.02
2023-03-01 03:41:36 iteration: 171700 loss: 0.0021 lr: 0.02
2023-03-01 03:41:44 iteration: 171800 loss: 0.0024 lr: 0.02
2023-03-01 03:41:51 iteration: 171900 loss: 0.0025 lr: 0.02
2023-03-01 03:41:59 iteration: 172000 loss: 0.0020 lr: 0.02
2023-03-01 03:42:09 iteration: 172100 loss: 0.0023 lr: 0.02
2023-03-01 03:42:17 iteration: 172200 loss: 0.0023 lr: 0.02
2023-03-01 03:42:24 iteration: 172300 loss: 0.0022 lr: 0.02
2023-03-01 03:42:32 iteration: 172400 loss: 0.0023 lr: 0.02
2023-03-01 03:42:39 iteration: 172500 loss: 0.0020 lr: 0.02
2023-03-01 03:42:46 iteration: 172600 loss: 0.0023 lr: 0.02
2023-03-01 03:42:54 iteration: 172700 loss: 0.0021 lr: 0.02
2023-03-01 03:43:01 iteration: 172800 loss: 0.0023 lr: 0.02
2023-03-01 03:43:08 iteration: 172900 loss: 0.0025 lr: 0.02
2023-03-01 03:43:15 iteration: 173000 loss: 0.0023 lr: 0.02
2023-03-01 03:43:26 iteration: 173100 loss: 0.0021 lr: 0.02
2023-03-01 03:43:33 iteration: 173200 loss: 0.0021 lr: 0.02
2023-03-01 03:43:42 iteration: 173300 loss: 0.0021 lr: 0.02
2023-03-01 03:43:49 iteration: 173400 loss: 0.0023 lr: 0.02
2023-03-01 03:43:57 iteration: 173500 loss: 0.0024 lr: 0.02
2023-03-01 03:44:04 iteration: 173600 loss: 0.0022 lr: 0.02
2023-03-01 03:44:11 iteration: 173700 loss: 0.0023 lr: 0.02
2023-03-01 03:44:19 iteration: 173800 loss: 0.0023 lr: 0.02
2023-03-01 03:44:26 iteration: 173900 loss: 0.0023 lr: 0.02
2023-03-01 03:44:33 iteration: 174000 loss: 0.0023 lr: 0.02
2023-03-01 03:44:44 iteration: 174100 loss: 0.0022 lr: 0.02
2023-03-01 03:44:51 iteration: 174200 loss: 0.0024 lr: 0.02
2023-03-01 03:44:59 iteration: 174300 loss: 0.0022 lr: 0.02
2023-03-01 03:45:06 iteration: 174400 loss: 0.0022 lr: 0.02
2023-03-01 03:45:13 iteration: 174500 loss: 0.0024 lr: 0.02
2023-03-01 03:45:20 iteration: 174600 loss: 0.0022 lr: 0.02
2023-03-01 03:45:28 iteration: 174700 loss: 0.0024 lr: 0.02
2023-03-01 03:45:35 iteration: 174800 loss: 0.0022 lr: 0.02
2023-03-01 03:45:42 iteration: 174900 loss: 0.0022 lr: 0.02
2023-03-01 03:45:50 iteration: 175000 loss: 0.0022 lr: 0.02
2023-03-01 03:46:00 iteration: 175100 loss: 0.0022 lr: 0.02
2023-03-01 03:46:08 iteration: 175200 loss: 0.0024 lr: 0.02
2023-03-01 03:46:15 iteration: 175300 loss: 0.0023 lr: 0.02
2023-03-01 03:46:22 iteration: 175400 loss: 0.0024 lr: 0.02
2023-03-01 03:46:30 iteration: 175500 loss: 0.0023 lr: 0.02
2023-03-01 03:46:37 iteration: 175600 loss: 0.0025 lr: 0.02
2023-03-01 03:46:45 iteration: 175700 loss: 0.0023 lr: 0.02
2023-03-01 03:46:52 iteration: 175800 loss: 0.0022 lr: 0.02
2023-03-01 03:46:59 iteration: 175900 loss: 0.0024 lr: 0.02
2023-03-01 03:47:07 iteration: 176000 loss: 0.0022 lr: 0.02
2023-03-01 03:47:18 iteration: 176100 loss: 0.0024 lr: 0.02
2023-03-01 03:47:25 iteration: 176200 loss: 0.0021 lr: 0.02
2023-03-01 03:47:32 iteration: 176300 loss: 0.0024 lr: 0.02
2023-03-01 03:47:40 iteration: 176400 loss: 0.0023 lr: 0.02
2023-03-01 03:47:47 iteration: 176500 loss: 0.0022 lr: 0.02
2023-03-01 03:47:54 iteration: 176600 loss: 0.0024 lr: 0.02
2023-03-01 03:48:02 iteration: 176700 loss: 0.0024 lr: 0.02
2023-03-01 03:48:09 iteration: 176800 loss: 0.0022 lr: 0.02
2023-03-01 03:48:17 iteration: 176900 loss: 0.0023 lr: 0.02
2023-03-01 03:48:24 iteration: 177000 loss: 0.0025 lr: 0.02
2023-03-01 03:48:34 iteration: 177100 loss: 0.0023 lr: 0.02
2023-03-01 03:48:42 iteration: 177200 loss: 0.0024 lr: 0.02
2023-03-01 03:48:49 iteration: 177300 loss: 0.0023 lr: 0.02
2023-03-01 03:48:57 iteration: 177400 loss: 0.0024 lr: 0.02
2023-03-01 03:49:04 iteration: 177500 loss: 0.0024 lr: 0.02
2023-03-01 03:49:11 iteration: 177600 loss: 0.0024 lr: 0.02
2023-03-01 03:49:19 iteration: 177700 loss: 0.0024 lr: 0.02
2023-03-01 03:49:26 iteration: 177800 loss: 0.0024 lr: 0.02
2023-03-01 03:49:33 iteration: 177900 loss: 0.0022 lr: 0.02
2023-03-01 03:49:41 iteration: 178000 loss: 0.0022 lr: 0.02
2023-03-01 03:49:51 iteration: 178100 loss: 0.0023 lr: 0.02
2023-03-01 03:49:59 iteration: 178200 loss: 0.0022 lr: 0.02
2023-03-01 03:50:06 iteration: 178300 loss: 0.0022 lr: 0.02
2023-03-01 03:50:13 iteration: 178400 loss: 0.0022 lr: 0.02
2023-03-01 03:50:20 iteration: 178500 loss: 0.0022 lr: 0.02
2023-03-01 03:50:28 iteration: 178600 loss: 0.0022 lr: 0.02
2023-03-01 03:50:36 iteration: 178700 loss: 0.0023 lr: 0.02
2023-03-01 03:50:44 iteration: 178800 loss: 0.0023 lr: 0.02
2023-03-01 03:50:51 iteration: 178900 loss: 0.0022 lr: 0.02
2023-03-01 03:50:58 iteration: 179000 loss: 0.0023 lr: 0.02
2023-03-01 03:51:09 iteration: 179100 loss: 0.0025 lr: 0.02
2023-03-01 03:51:17 iteration: 179200 loss: 0.0023 lr: 0.02
2023-03-01 03:51:24 iteration: 179300 loss: 0.0022 lr: 0.02
2023-03-01 03:51:32 iteration: 179400 loss: 0.0022 lr: 0.02
2023-03-01 03:51:40 iteration: 179500 loss: 0.0026 lr: 0.02
2023-03-01 03:51:48 iteration: 179600 loss: 0.0022 lr: 0.02
2023-03-01 03:51:55 iteration: 179700 loss: 0.0024 lr: 0.02
2023-03-01 03:52:03 iteration: 179800 loss: 0.0023 lr: 0.02
2023-03-01 03:52:10 iteration: 179900 loss: 0.0023 lr: 0.02
2023-03-01 03:52:17 iteration: 180000 loss: 0.0022 lr: 0.02
2023-03-01 03:52:28 iteration: 180100 loss: 0.0022 lr: 0.02
2023-03-01 03:52:35 iteration: 180200 loss: 0.0024 lr: 0.02
2023-03-01 03:52:42 iteration: 180300 loss: 0.0023 lr: 0.02
2023-03-01 03:52:50 iteration: 180400 loss: 0.0024 lr: 0.02
2023-03-01 03:52:57 iteration: 180500 loss: 0.0023 lr: 0.02
2023-03-01 03:53:04 iteration: 180600 loss: 0.0023 lr: 0.02
2023-03-01 03:53:12 iteration: 180700 loss: 0.0023 lr: 0.02
2023-03-01 03:53:19 iteration: 180800 loss: 0.0023 lr: 0.02
2023-03-01 03:53:27 iteration: 180900 loss: 0.0022 lr: 0.02
2023-03-01 03:53:34 iteration: 181000 loss: 0.0024 lr: 0.02
2023-03-01 03:53:45 iteration: 181100 loss: 0.0023 lr: 0.02
2023-03-01 03:53:53 iteration: 181200 loss: 0.0022 lr: 0.02
2023-03-01 03:54:00 iteration: 181300 loss: 0.0021 lr: 0.02
2023-03-01 03:54:08 iteration: 181400 loss: 0.0023 lr: 0.02
2023-03-01 03:54:15 iteration: 181500 loss: 0.0023 lr: 0.02
2023-03-01 03:54:23 iteration: 181600 loss: 0.0025 lr: 0.02
2023-03-01 03:54:30 iteration: 181700 loss: 0.0024 lr: 0.02
2023-03-01 03:54:37 iteration: 181800 loss: 0.0021 lr: 0.02
2023-03-01 03:54:45 iteration: 181900 loss: 0.0022 lr: 0.02
2023-03-01 03:54:52 iteration: 182000 loss: 0.0022 lr: 0.02
2023-03-01 03:55:03 iteration: 182100 loss: 0.0022 lr: 0.02
2023-03-01 03:55:10 iteration: 182200 loss: 0.0023 lr: 0.02
2023-03-01 03:55:18 iteration: 182300 loss: 0.0023 lr: 0.02
2023-03-01 03:55:25 iteration: 182400 loss: 0.0023 lr: 0.02
2023-03-01 03:55:33 iteration: 182500 loss: 0.0023 lr: 0.02
2023-03-01 03:55:40 iteration: 182600 loss: 0.0025 lr: 0.02
2023-03-01 03:55:48 iteration: 182700 loss: 0.0023 lr: 0.02
2023-03-01 03:55:55 iteration: 182800 loss: 0.0022 lr: 0.02
2023-03-01 03:56:03 iteration: 182900 loss: 0.0023 lr: 0.02
2023-03-01 03:56:10 iteration: 183000 loss: 0.0021 lr: 0.02
2023-03-01 03:56:20 iteration: 183100 loss: 0.0023 lr: 0.02
2023-03-01 03:56:28 iteration: 183200 loss: 0.0024 lr: 0.02
2023-03-01 03:56:35 iteration: 183300 loss: 0.0023 lr: 0.02
2023-03-01 03:56:43 iteration: 183400 loss: 0.0024 lr: 0.02
2023-03-01 03:56:51 iteration: 183500 loss: 0.0024 lr: 0.02
2023-03-01 03:56:58 iteration: 183600 loss: 0.0024 lr: 0.02
2023-03-01 03:57:05 iteration: 183700 loss: 0.0023 lr: 0.02
2023-03-01 03:57:13 iteration: 183800 loss: 0.0025 lr: 0.02
2023-03-01 03:57:20 iteration: 183900 loss: 0.0022 lr: 0.02
2023-03-01 03:57:27 iteration: 184000 loss: 0.0025 lr: 0.02
2023-03-01 03:57:37 iteration: 184100 loss: 0.0022 lr: 0.02
2023-03-01 03:57:46 iteration: 184200 loss: 0.0022 lr: 0.02
2023-03-01 03:57:53 iteration: 184300 loss: 0.0024 lr: 0.02
2023-03-01 03:58:00 iteration: 184400 loss: 0.0026 lr: 0.02
2023-03-01 03:58:08 iteration: 184500 loss: 0.0023 lr: 0.02
2023-03-01 03:58:15 iteration: 184600 loss: 0.0021 lr: 0.02
2023-03-01 03:58:22 iteration: 184700 loss: 0.0024 lr: 0.02
2023-03-01 03:58:30 iteration: 184800 loss: 0.0022 lr: 0.02
2023-03-01 03:58:37 iteration: 184900 loss: 0.0022 lr: 0.02
2023-03-01 03:58:44 iteration: 185000 loss: 0.0024 lr: 0.02
2023-03-01 03:58:55 iteration: 185100 loss: 0.0024 lr: 0.02
2023-03-01 03:59:02 iteration: 185200 loss: 0.0022 lr: 0.02
2023-03-01 03:59:09 iteration: 185300 loss: 0.0025 lr: 0.02
2023-03-01 03:59:17 iteration: 185400 loss: 0.0023 lr: 0.02
2023-03-01 03:59:24 iteration: 185500 loss: 0.0023 lr: 0.02
2023-03-01 03:59:32 iteration: 185600 loss: 0.0024 lr: 0.02
2023-03-01 03:59:39 iteration: 185700 loss: 0.0023 lr: 0.02
2023-03-01 03:59:47 iteration: 185800 loss: 0.0022 lr: 0.02
2023-03-01 03:59:55 iteration: 185900 loss: 0.0024 lr: 0.02
2023-03-01 04:00:02 iteration: 186000 loss: 0.0023 lr: 0.02
2023-03-01 04:00:12 iteration: 186100 loss: 0.0023 lr: 0.02
2023-03-01 04:00:20 iteration: 186200 loss: 0.0022 lr: 0.02
2023-03-01 04:00:27 iteration: 186300 loss: 0.0023 lr: 0.02
2023-03-01 04:00:35 iteration: 186400 loss: 0.0023 lr: 0.02
2023-03-01 04:00:42 iteration: 186500 loss: 0.0023 lr: 0.02
2023-03-01 04:00:49 iteration: 186600 loss: 0.0022 lr: 0.02
2023-03-01 04:00:57 iteration: 186700 loss: 0.0021 lr: 0.02
2023-03-01 04:01:04 iteration: 186800 loss: 0.0022 lr: 0.02
2023-03-01 04:01:12 iteration: 186900 loss: 0.0022 lr: 0.02
2023-03-01 04:01:19 iteration: 187000 loss: 0.0022 lr: 0.02
2023-03-01 04:01:30 iteration: 187100 loss: 0.0022 lr: 0.02
2023-03-01 04:01:38 iteration: 187200 loss: 0.0023 lr: 0.02
2023-03-01 04:01:46 iteration: 187300 loss: 0.0022 lr: 0.02
2023-03-01 04:01:53 iteration: 187400 loss: 0.0023 lr: 0.02
2023-03-01 04:02:00 iteration: 187500 loss: 0.0022 lr: 0.02
2023-03-01 04:02:07 iteration: 187600 loss: 0.0023 lr: 0.02
2023-03-01 04:02:15 iteration: 187700 loss: 0.0022 lr: 0.02
2023-03-01 04:02:22 iteration: 187800 loss: 0.0021 lr: 0.02
2023-03-01 04:02:30 iteration: 187900 loss: 0.0023 lr: 0.02
2023-03-01 04:02:37 iteration: 188000 loss: 0.0020 lr: 0.02
2023-03-01 04:02:47 iteration: 188100 loss: 0.0023 lr: 0.02
2023-03-01 04:02:55 iteration: 188200 loss: 0.0021 lr: 0.02
2023-03-01 04:03:02 iteration: 188300 loss: 0.0024 lr: 0.02
2023-03-01 04:03:10 iteration: 188400 loss: 0.0021 lr: 0.02
2023-03-01 04:03:17 iteration: 188500 loss: 0.0022 lr: 0.02
2023-03-01 04:03:25 iteration: 188600 loss: 0.0022 lr: 0.02
2023-03-01 04:03:32 iteration: 188700 loss: 0.0022 lr: 0.02
2023-03-01 04:03:40 iteration: 188800 loss: 0.0025 lr: 0.02
2023-03-01 04:03:47 iteration: 188900 loss: 0.0021 lr: 0.02
2023-03-01 04:03:54 iteration: 189000 loss: 0.0021 lr: 0.02
2023-03-01 04:04:05 iteration: 189100 loss: 0.0023 lr: 0.02
2023-03-01 04:04:13 iteration: 189200 loss: 0.0024 lr: 0.02
2023-03-01 04:04:20 iteration: 189300 loss: 0.0021 lr: 0.02
2023-03-01 04:04:27 iteration: 189400 loss: 0.0021 lr: 0.02
2023-03-01 04:04:35 iteration: 189500 loss: 0.0021 lr: 0.02
2023-03-01 04:04:42 iteration: 189600 loss: 0.0022 lr: 0.02
2023-03-01 04:04:49 iteration: 189700 loss: 0.0022 lr: 0.02
2023-03-01 04:04:56 iteration: 189800 loss: 0.0023 lr: 0.02
2023-03-01 04:05:04 iteration: 189900 loss: 0.0021 lr: 0.02
2023-03-01 04:05:11 iteration: 190000 loss: 0.0023 lr: 0.02
2023-03-01 04:05:21 iteration: 190100 loss: 0.0020 lr: 0.02
2023-03-01 04:05:30 iteration: 190200 loss: 0.0022 lr: 0.02
2023-03-01 04:05:38 iteration: 190300 loss: 0.0023 lr: 0.02
2023-03-01 04:05:45 iteration: 190400 loss: 0.0024 lr: 0.02
2023-03-01 04:05:53 iteration: 190500 loss: 0.0021 lr: 0.02
2023-03-01 04:06:00 iteration: 190600 loss: 0.0021 lr: 0.02
2023-03-01 04:06:08 iteration: 190700 loss: 0.0022 lr: 0.02
2023-03-01 04:06:16 iteration: 190800 loss: 0.0021 lr: 0.02
2023-03-01 04:06:24 iteration: 190900 loss: 0.0020 lr: 0.02
2023-03-01 04:06:32 iteration: 191000 loss: 0.0024 lr: 0.02
2023-03-01 04:06:42 iteration: 191100 loss: 0.0022 lr: 0.02
2023-03-01 04:06:50 iteration: 191200 loss: 0.0021 lr: 0.02
2023-03-01 04:06:57 iteration: 191300 loss: 0.0024 lr: 0.02
2023-03-01 04:07:05 iteration: 191400 loss: 0.0022 lr: 0.02
2023-03-01 04:07:12 iteration: 191500 loss: 0.0021 lr: 0.02
2023-03-01 04:07:20 iteration: 191600 loss: 0.0020 lr: 0.02
2023-03-01 04:07:27 iteration: 191700 loss: 0.0020 lr: 0.02
2023-03-01 04:07:35 iteration: 191800 loss: 0.0021 lr: 0.02
2023-03-01 04:07:42 iteration: 191900 loss: 0.0020 lr: 0.02
2023-03-01 04:07:50 iteration: 192000 loss: 0.0025 lr: 0.02
2023-03-01 04:08:00 iteration: 192100 loss: 0.0023 lr: 0.02
2023-03-01 04:08:08 iteration: 192200 loss: 0.0021 lr: 0.02
2023-03-01 04:08:15 iteration: 192300 loss: 0.0022 lr: 0.02
2023-03-01 04:08:23 iteration: 192400 loss: 0.0023 lr: 0.02
2023-03-01 04:08:30 iteration: 192500 loss: 0.0022 lr: 0.02
2023-03-01 04:08:38 iteration: 192600 loss: 0.0023 lr: 0.02
2023-03-01 04:08:46 iteration: 192700 loss: 0.0024 lr: 0.02
2023-03-01 04:08:53 iteration: 192800 loss: 0.0022 lr: 0.02
2023-03-01 04:09:01 iteration: 192900 loss: 0.0024 lr: 0.02
2023-03-01 04:09:08 iteration: 193000 loss: 0.0024 lr: 0.02
2023-03-01 04:09:19 iteration: 193100 loss: 0.0021 lr: 0.02
2023-03-01 04:09:26 iteration: 193200 loss: 0.0022 lr: 0.02
2023-03-01 04:09:34 iteration: 193300 loss: 0.0023 lr: 0.02
2023-03-01 04:09:42 iteration: 193400 loss: 0.0020 lr: 0.02
2023-03-01 04:09:49 iteration: 193500 loss: 0.0025 lr: 0.02
2023-03-01 04:09:57 iteration: 193600 loss: 0.0020 lr: 0.02
2023-03-01 04:10:04 iteration: 193700 loss: 0.0020 lr: 0.02
2023-03-01 04:10:12 iteration: 193800 loss: 0.0022 lr: 0.02
2023-03-01 04:10:20 iteration: 193900 loss: 0.0023 lr: 0.02
2023-03-01 04:10:28 iteration: 194000 loss: 0.0022 lr: 0.02
2023-03-01 04:10:38 iteration: 194100 loss: 0.0022 lr: 0.02
2023-03-01 04:10:46 iteration: 194200 loss: 0.0021 lr: 0.02
2023-03-01 04:10:53 iteration: 194300 loss: 0.0022 lr: 0.02
2023-03-01 04:11:01 iteration: 194400 loss: 0.0021 lr: 0.02
2023-03-01 04:11:08 iteration: 194500 loss: 0.0020 lr: 0.02
2023-03-01 04:11:15 iteration: 194600 loss: 0.0022 lr: 0.02
2023-03-01 04:11:23 iteration: 194700 loss: 0.0022 lr: 0.02
2023-03-01 04:11:30 iteration: 194800 loss: 0.0022 lr: 0.02
2023-03-01 04:11:38 iteration: 194900 loss: 0.0022 lr: 0.02
2023-03-01 04:11:45 iteration: 195000 loss: 0.0020 lr: 0.02
2023-03-01 04:11:56 iteration: 195100 loss: 0.0024 lr: 0.02
2023-03-01 04:12:03 iteration: 195200 loss: 0.0022 lr: 0.02
2023-03-01 04:12:10 iteration: 195300 loss: 0.0022 lr: 0.02
2023-03-01 04:12:17 iteration: 195400 loss: 0.0023 lr: 0.02
2023-03-01 04:12:25 iteration: 195500 loss: 0.0022 lr: 0.02
2023-03-01 04:12:32 iteration: 195600 loss: 0.0022 lr: 0.02
2023-03-01 04:12:40 iteration: 195700 loss: 0.0022 lr: 0.02
2023-03-01 04:12:47 iteration: 195800 loss: 0.0022 lr: 0.02
2023-03-01 04:12:55 iteration: 195900 loss: 0.0024 lr: 0.02
2023-03-01 04:13:02 iteration: 196000 loss: 0.0022 lr: 0.02
2023-03-01 04:13:13 iteration: 196100 loss: 0.0022 lr: 0.02
2023-03-01 04:13:21 iteration: 196200 loss: 0.0023 lr: 0.02
2023-03-01 04:13:28 iteration: 196300 loss: 0.0023 lr: 0.02
2023-03-01 04:13:36 iteration: 196400 loss: 0.0022 lr: 0.02
2023-03-01 04:13:43 iteration: 196500 loss: 0.0023 lr: 0.02
2023-03-01 04:13:50 iteration: 196600 loss: 0.0020 lr: 0.02
2023-03-01 04:13:58 iteration: 196700 loss: 0.0023 lr: 0.02
2023-03-01 04:14:05 iteration: 196800 loss: 0.0022 lr: 0.02
2023-03-01 04:14:13 iteration: 196900 loss: 0.0023 lr: 0.02
2023-03-01 04:14:20 iteration: 197000 loss: 0.0020 lr: 0.02
2023-03-01 04:14:31 iteration: 197100 loss: 0.0025 lr: 0.02
2023-03-01 04:14:38 iteration: 197200 loss: 0.0023 lr: 0.02
2023-03-01 04:14:46 iteration: 197300 loss: 0.0021 lr: 0.02
2023-03-01 04:14:53 iteration: 197400 loss: 0.0021 lr: 0.02
2023-03-01 04:15:00 iteration: 197500 loss: 0.0023 lr: 0.02
2023-03-01 04:15:08 iteration: 197600 loss: 0.0022 lr: 0.02
2023-03-01 04:15:15 iteration: 197700 loss: 0.0022 lr: 0.02
2023-03-01 04:15:23 iteration: 197800 loss: 0.0024 lr: 0.02
2023-03-01 04:15:30 iteration: 197900 loss: 0.0023 lr: 0.02
2023-03-01 04:15:37 iteration: 198000 loss: 0.0022 lr: 0.02
2023-03-01 04:15:48 iteration: 198100 loss: 0.0021 lr: 0.02
2023-03-01 04:15:55 iteration: 198200 loss: 0.0022 lr: 0.02
2023-03-01 04:16:02 iteration: 198300 loss: 0.0024 lr: 0.02
2023-03-01 04:16:10 iteration: 198400 loss: 0.0021 lr: 0.02
2023-03-01 04:16:17 iteration: 198500 loss: 0.0023 lr: 0.02
2023-03-01 04:16:25 iteration: 198600 loss: 0.0024 lr: 0.02
2023-03-01 04:16:32 iteration: 198700 loss: 0.0021 lr: 0.02
2023-03-01 04:16:40 iteration: 198800 loss: 0.0022 lr: 0.02
2023-03-01 04:16:47 iteration: 198900 loss: 0.0022 lr: 0.02
2023-03-01 04:16:54 iteration: 199000 loss: 0.0024 lr: 0.02
2023-03-01 04:17:05 iteration: 199100 loss: 0.0023 lr: 0.02
2023-03-01 04:17:13 iteration: 199200 loss: 0.0022 lr: 0.02
2023-03-01 04:17:20 iteration: 199300 loss: 0.0020 lr: 0.02
2023-03-01 04:17:27 iteration: 199400 loss: 0.0021 lr: 0.02
2023-03-01 04:17:35 iteration: 199500 loss: 0.0024 lr: 0.02
2023-03-01 04:17:43 iteration: 199600 loss: 0.0022 lr: 0.02
2023-03-01 04:17:51 iteration: 199700 loss: 0.0021 lr: 0.02
2023-03-01 04:17:59 iteration: 199800 loss: 0.0021 lr: 0.02
2023-03-01 04:18:06 iteration: 199900 loss: 0.0021 lr: 0.02
2023-03-01 04:18:13 iteration: 200000 loss: 0.0021 lr: 0.02
2023-03-01 10:29:47 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 1,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'imgaug',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\Pc-Ryzen\\anaconda3\\envs\\DEEPLABCUT\\Lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': 'D:/DLC/CV-Mena-2023-02-26',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-03-01 10:30:36 iteration: 100 loss: 0.0785 lr: 0.005
2023-03-01 10:30:58 iteration: 200 loss: 0.0248 lr: 0.005
2023-03-01 10:31:20 iteration: 300 loss: 0.0228 lr: 0.005
2023-03-01 10:31:34 iteration: 400 loss: 0.0228 lr: 0.005
2023-03-01 10:31:47 iteration: 500 loss: 0.0211 lr: 0.005
2023-03-01 10:32:00 iteration: 600 loss: 0.0206 lr: 0.005
2023-03-01 10:32:10 iteration: 700 loss: 0.0189 lr: 0.005
2023-03-01 10:32:21 iteration: 800 loss: 0.0204 lr: 0.005
2023-03-01 10:32:31 iteration: 900 loss: 0.0193 lr: 0.005
2023-03-01 10:32:40 iteration: 1000 loss: 0.0195 lr: 0.005
2023-03-01 10:32:52 iteration: 1100 loss: 0.0187 lr: 0.005
2023-03-01 10:33:00 iteration: 1200 loss: 0.0206 lr: 0.005
2023-03-01 10:33:10 iteration: 1300 loss: 0.0194 lr: 0.005
2023-03-01 10:33:18 iteration: 1400 loss: 0.0181 lr: 0.005
2023-03-01 10:33:26 iteration: 1500 loss: 0.0176 lr: 0.005
2023-03-01 10:33:34 iteration: 1600 loss: 0.0170 lr: 0.005
2023-03-01 10:33:43 iteration: 1700 loss: 0.0165 lr: 0.005
2023-03-01 10:33:51 iteration: 1800 loss: 0.0175 lr: 0.005
2023-03-01 10:33:59 iteration: 1900 loss: 0.0177 lr: 0.005
2023-03-01 10:34:07 iteration: 2000 loss: 0.0161 lr: 0.005
2023-03-01 10:36:45 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 1,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'imgaug',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\Pc-Ryzen\\anaconda3\\envs\\DEEPLABCUT\\Lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': 'D:/DLC/CV-Mena-2023-02-26',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-03-01 10:37:36 iteration: 100 loss: 0.0683 lr: 0.005
2023-03-01 10:38:01 iteration: 200 loss: 0.0248 lr: 0.005
2023-03-01 10:38:19 iteration: 300 loss: 0.0224 lr: 0.005
2023-03-01 10:38:35 iteration: 400 loss: 0.0228 lr: 0.005
2023-03-01 10:38:50 iteration: 500 loss: 0.0216 lr: 0.005
2023-03-01 10:39:06 iteration: 600 loss: 0.0206 lr: 0.005
2023-03-01 10:39:18 iteration: 700 loss: 0.0191 lr: 0.005
2023-03-01 10:39:31 iteration: 800 loss: 0.0206 lr: 0.005
2023-03-01 10:39:43 iteration: 900 loss: 0.0193 lr: 0.005
2023-03-01 10:39:56 iteration: 1000 loss: 0.0197 lr: 0.005
2023-03-01 10:40:07 iteration: 1100 loss: 0.0182 lr: 0.005
2023-03-01 10:40:18 iteration: 1200 loss: 0.0189 lr: 0.005
2023-03-01 10:40:29 iteration: 1300 loss: 0.0191 lr: 0.005
2023-03-01 10:40:40 iteration: 1400 loss: 0.0181 lr: 0.005
2023-03-01 10:40:50 iteration: 1500 loss: 0.0177 lr: 0.005
2023-03-01 10:41:00 iteration: 1600 loss: 0.0172 lr: 0.005
2023-03-01 10:41:12 iteration: 1700 loss: 0.0169 lr: 0.005
2023-03-01 10:41:22 iteration: 1800 loss: 0.0177 lr: 0.005
2023-03-01 10:41:33 iteration: 1900 loss: 0.0178 lr: 0.005
2023-03-01 10:41:42 iteration: 2000 loss: 0.0165 lr: 0.005
2023-03-01 10:41:53 iteration: 2100 loss: 0.0176 lr: 0.005
2023-03-01 10:42:03 iteration: 2200 loss: 0.0163 lr: 0.005
2023-03-01 10:42:12 iteration: 2300 loss: 0.0159 lr: 0.005
2023-03-01 10:42:21 iteration: 2400 loss: 0.0165 lr: 0.005
2023-03-01 10:42:31 iteration: 2500 loss: 0.0165 lr: 0.005
2023-03-01 10:42:41 iteration: 2600 loss: 0.0161 lr: 0.005
2023-03-01 10:42:50 iteration: 2700 loss: 0.0158 lr: 0.005
2023-03-01 10:42:59 iteration: 2800 loss: 0.0157 lr: 0.005
2023-03-01 10:43:08 iteration: 2900 loss: 0.0151 lr: 0.005
2023-03-01 10:43:17 iteration: 3000 loss: 0.0165 lr: 0.005
2023-03-01 10:43:26 iteration: 3100 loss: 0.0156 lr: 0.005
2023-03-01 10:43:35 iteration: 3200 loss: 0.0152 lr: 0.005
2023-03-01 10:43:45 iteration: 3300 loss: 0.0159 lr: 0.005
2023-03-01 10:43:54 iteration: 3400 loss: 0.0149 lr: 0.005
2023-03-01 10:44:02 iteration: 3500 loss: 0.0150 lr: 0.005
2023-03-01 10:44:11 iteration: 3600 loss: 0.0151 lr: 0.005
2023-03-01 10:44:20 iteration: 3700 loss: 0.0146 lr: 0.005
2023-03-01 10:44:29 iteration: 3800 loss: 0.0145 lr: 0.005
2023-03-01 10:44:38 iteration: 3900 loss: 0.0151 lr: 0.005
2023-03-01 10:44:47 iteration: 4000 loss: 0.0148 lr: 0.005
2023-03-01 10:44:55 iteration: 4100 loss: 0.0149 lr: 0.005
2023-03-01 10:45:04 iteration: 4200 loss: 0.0141 lr: 0.005
2023-03-01 10:45:13 iteration: 4300 loss: 0.0144 lr: 0.005
2023-03-01 10:45:22 iteration: 4400 loss: 0.0138 lr: 0.005
2023-03-01 10:45:30 iteration: 4500 loss: 0.0142 lr: 0.005
2023-03-01 10:45:40 iteration: 4600 loss: 0.0138 lr: 0.005
2023-03-01 10:45:48 iteration: 4700 loss: 0.0140 lr: 0.005
2023-03-01 10:45:57 iteration: 4800 loss: 0.0134 lr: 0.005
2023-03-01 10:46:05 iteration: 4900 loss: 0.0129 lr: 0.005
2023-03-01 10:46:14 iteration: 5000 loss: 0.0134 lr: 0.005
2023-03-01 10:46:23 iteration: 5100 loss: 0.0138 lr: 0.005
2023-03-01 10:46:31 iteration: 5200 loss: 0.0149 lr: 0.005
2023-03-01 10:46:39 iteration: 5300 loss: 0.0124 lr: 0.005
2023-03-01 10:46:48 iteration: 5400 loss: 0.0124 lr: 0.005
2023-03-01 10:46:57 iteration: 5500 loss: 0.0134 lr: 0.005
2023-03-01 10:47:05 iteration: 5600 loss: 0.0134 lr: 0.005
2023-03-01 10:47:14 iteration: 5700 loss: 0.0139 lr: 0.005
2023-03-01 10:47:22 iteration: 5800 loss: 0.0124 lr: 0.005
2023-03-01 10:47:31 iteration: 5900 loss: 0.0129 lr: 0.005
2023-03-01 10:47:39 iteration: 6000 loss: 0.0136 lr: 0.005
2023-03-01 10:47:48 iteration: 6100 loss: 0.0132 lr: 0.005
2023-03-01 10:47:57 iteration: 6200 loss: 0.0132 lr: 0.005
2023-03-01 10:48:05 iteration: 6300 loss: 0.0129 lr: 0.005
2023-03-01 10:48:13 iteration: 6400 loss: 0.0129 lr: 0.005
2023-03-01 10:48:23 iteration: 6500 loss: 0.0126 lr: 0.005
2023-03-01 10:48:31 iteration: 6600 loss: 0.0131 lr: 0.005
2023-03-01 10:48:40 iteration: 6700 loss: 0.0130 lr: 0.005
2023-03-01 10:48:48 iteration: 6800 loss: 0.0127 lr: 0.005
2023-03-01 10:48:56 iteration: 6900 loss: 0.0126 lr: 0.005
2023-03-01 10:49:05 iteration: 7000 loss: 0.0131 lr: 0.005
2023-03-01 10:49:14 iteration: 7100 loss: 0.0125 lr: 0.005
2023-03-01 10:49:22 iteration: 7200 loss: 0.0127 lr: 0.005
2023-03-01 10:49:31 iteration: 7300 loss: 0.0122 lr: 0.005
2023-03-01 10:49:39 iteration: 7400 loss: 0.0125 lr: 0.005
2023-03-01 10:49:48 iteration: 7500 loss: 0.0119 lr: 0.005
2023-03-01 10:49:56 iteration: 7600 loss: 0.0120 lr: 0.005
2023-03-01 10:50:04 iteration: 7700 loss: 0.0121 lr: 0.005
2023-03-01 10:50:13 iteration: 7800 loss: 0.0115 lr: 0.005
2023-03-01 10:50:21 iteration: 7900 loss: 0.0114 lr: 0.005
2023-03-01 10:50:29 iteration: 8000 loss: 0.0123 lr: 0.005
2023-03-01 10:50:37 iteration: 8100 loss: 0.0113 lr: 0.005
2023-03-01 10:50:46 iteration: 8200 loss: 0.0126 lr: 0.005
2023-03-01 10:50:54 iteration: 8300 loss: 0.0119 lr: 0.005
2023-03-01 10:51:03 iteration: 8400 loss: 0.0113 lr: 0.005
2023-03-01 10:51:11 iteration: 8500 loss: 0.0118 lr: 0.005
2023-03-01 10:51:19 iteration: 8600 loss: 0.0116 lr: 0.005
2023-03-01 10:51:28 iteration: 8700 loss: 0.0117 lr: 0.005
2023-03-01 10:51:37 iteration: 8800 loss: 0.0118 lr: 0.005
2023-03-01 10:51:45 iteration: 8900 loss: 0.0111 lr: 0.005
2023-03-01 10:51:53 iteration: 9000 loss: 0.0112 lr: 0.005
2023-03-01 10:52:01 iteration: 9100 loss: 0.0119 lr: 0.005
2023-03-01 10:52:09 iteration: 9200 loss: 0.0113 lr: 0.005
2023-03-01 10:52:17 iteration: 9300 loss: 0.0112 lr: 0.005
2023-03-01 10:52:26 iteration: 9400 loss: 0.0129 lr: 0.005
2023-03-01 10:52:33 iteration: 9500 loss: 0.0102 lr: 0.005
2023-03-01 10:52:42 iteration: 9600 loss: 0.0118 lr: 0.005
2023-03-01 10:52:50 iteration: 9700 loss: 0.0117 lr: 0.005
2023-03-01 10:52:58 iteration: 9800 loss: 0.0107 lr: 0.005
2023-03-01 10:53:07 iteration: 9900 loss: 0.0114 lr: 0.005
2023-03-01 10:53:16 iteration: 10000 loss: 0.0108 lr: 0.005
2023-03-01 10:53:24 iteration: 10100 loss: 0.0121 lr: 0.02
2023-03-01 10:53:33 iteration: 10200 loss: 0.0121 lr: 0.02
2023-03-01 10:53:41 iteration: 10300 loss: 0.0136 lr: 0.02
2023-03-01 10:53:49 iteration: 10400 loss: 0.0143 lr: 0.02
2023-03-01 10:53:58 iteration: 10500 loss: 0.0140 lr: 0.02
2023-03-01 10:54:07 iteration: 10600 loss: 0.0122 lr: 0.02
2023-03-01 10:54:15 iteration: 10700 loss: 0.0120 lr: 0.02
2023-03-01 10:54:23 iteration: 10800 loss: 0.0119 lr: 0.02
2023-03-01 10:54:32 iteration: 10900 loss: 0.0117 lr: 0.02
2023-03-01 10:54:40 iteration: 11000 loss: 0.0111 lr: 0.02
2023-03-01 10:54:49 iteration: 11100 loss: 0.0114 lr: 0.02
2023-03-01 10:54:57 iteration: 11200 loss: 0.0128 lr: 0.02
2023-03-01 10:55:05 iteration: 11300 loss: 0.0110 lr: 0.02
2023-03-01 10:55:14 iteration: 11400 loss: 0.0115 lr: 0.02
2023-03-01 10:55:22 iteration: 11500 loss: 0.0109 lr: 0.02
2023-03-01 10:55:30 iteration: 11600 loss: 0.0106 lr: 0.02
2023-03-01 10:55:39 iteration: 11700 loss: 0.0105 lr: 0.02
2023-03-01 10:55:47 iteration: 11800 loss: 0.0115 lr: 0.02
2023-03-01 10:55:55 iteration: 11900 loss: 0.0101 lr: 0.02
2023-03-01 10:56:03 iteration: 12000 loss: 0.0105 lr: 0.02
2023-03-01 10:56:12 iteration: 12100 loss: 0.0105 lr: 0.02
2023-03-01 10:56:21 iteration: 12200 loss: 0.0103 lr: 0.02
2023-03-01 10:56:29 iteration: 12300 loss: 0.0102 lr: 0.02
2023-03-01 10:56:37 iteration: 12400 loss: 0.0117 lr: 0.02
2023-03-01 10:56:45 iteration: 12500 loss: 0.0105 lr: 0.02
2023-03-01 10:56:53 iteration: 12600 loss: 0.0099 lr: 0.02
2023-03-01 10:57:02 iteration: 12700 loss: 0.0102 lr: 0.02
2023-03-01 10:57:11 iteration: 12800 loss: 0.0096 lr: 0.02
2023-03-01 10:57:19 iteration: 12900 loss: 0.0096 lr: 0.02
2023-03-01 10:57:28 iteration: 13000 loss: 0.0102 lr: 0.02
2023-03-01 10:57:37 iteration: 13100 loss: 0.0097 lr: 0.02
2023-03-01 10:57:45 iteration: 13200 loss: 0.0098 lr: 0.02
2023-03-01 10:57:53 iteration: 13300 loss: 0.0097 lr: 0.02
2023-03-01 10:58:01 iteration: 13400 loss: 0.0090 lr: 0.02
2023-03-01 10:58:09 iteration: 13500 loss: 0.0089 lr: 0.02
2023-03-01 10:58:18 iteration: 13600 loss: 0.0094 lr: 0.02
2023-03-01 10:58:26 iteration: 13700 loss: 0.0095 lr: 0.02
2023-03-01 10:58:34 iteration: 13800 loss: 0.0092 lr: 0.02
2023-03-01 10:58:42 iteration: 13900 loss: 0.0087 lr: 0.02
2023-03-01 10:58:51 iteration: 14000 loss: 0.0092 lr: 0.02
2023-03-01 10:59:00 iteration: 14100 loss: 0.0100 lr: 0.02
2023-03-01 10:59:08 iteration: 14200 loss: 0.0094 lr: 0.02
2023-03-01 10:59:16 iteration: 14300 loss: 0.0089 lr: 0.02
2023-03-01 10:59:25 iteration: 14400 loss: 0.0090 lr: 0.02
2023-03-01 10:59:34 iteration: 14500 loss: 0.0085 lr: 0.02
2023-03-01 10:59:42 iteration: 14600 loss: 0.0087 lr: 0.02
2023-03-01 10:59:50 iteration: 14700 loss: 0.0094 lr: 0.02
2023-03-01 10:59:58 iteration: 14800 loss: 0.0090 lr: 0.02
2023-03-01 11:00:07 iteration: 14900 loss: 0.0086 lr: 0.02
2023-03-01 11:00:16 iteration: 15000 loss: 0.0090 lr: 0.02
2023-03-01 11:00:24 iteration: 15100 loss: 0.0090 lr: 0.02
2023-03-01 11:00:32 iteration: 15200 loss: 0.0092 lr: 0.02
2023-03-01 11:00:41 iteration: 15300 loss: 0.0092 lr: 0.02
2023-03-01 11:00:50 iteration: 15400 loss: 0.0090 lr: 0.02
2023-03-01 11:00:58 iteration: 15500 loss: 0.0084 lr: 0.02
2023-03-01 11:01:06 iteration: 15600 loss: 0.0093 lr: 0.02
2023-03-01 11:01:14 iteration: 15700 loss: 0.0088 lr: 0.02
2023-03-01 11:01:23 iteration: 15800 loss: 0.0089 lr: 0.02
2023-03-01 11:01:31 iteration: 15900 loss: 0.0083 lr: 0.02
2023-03-01 11:01:40 iteration: 16000 loss: 0.0085 lr: 0.02
2023-03-01 11:01:48 iteration: 16100 loss: 0.0084 lr: 0.02
2023-03-01 11:01:57 iteration: 16200 loss: 0.0082 lr: 0.02
2023-03-01 11:02:05 iteration: 16300 loss: 0.0075 lr: 0.02
2023-03-01 11:02:13 iteration: 16400 loss: 0.0078 lr: 0.02
2023-03-01 11:02:21 iteration: 16500 loss: 0.0080 lr: 0.02
2023-03-01 11:02:30 iteration: 16600 loss: 0.0084 lr: 0.02
2023-03-01 11:02:39 iteration: 16700 loss: 0.0083 lr: 0.02
2023-03-01 11:02:48 iteration: 16800 loss: 0.0083 lr: 0.02
2023-03-01 11:02:56 iteration: 16900 loss: 0.0075 lr: 0.02
2023-03-01 11:03:04 iteration: 17000 loss: 0.0084 lr: 0.02
2023-03-01 11:03:13 iteration: 17100 loss: 0.0080 lr: 0.02
2023-03-01 11:03:22 iteration: 17200 loss: 0.0077 lr: 0.02
2023-03-01 11:03:30 iteration: 17300 loss: 0.0081 lr: 0.02
2023-03-01 11:03:38 iteration: 17400 loss: 0.0078 lr: 0.02
2023-03-01 11:03:47 iteration: 17500 loss: 0.0078 lr: 0.02
2023-03-01 11:03:55 iteration: 17600 loss: 0.0074 lr: 0.02
2023-03-01 11:04:04 iteration: 17700 loss: 0.0079 lr: 0.02
2023-03-01 11:04:12 iteration: 17800 loss: 0.0082 lr: 0.02
2023-03-01 11:04:21 iteration: 17900 loss: 0.0078 lr: 0.02
2023-03-01 11:04:30 iteration: 18000 loss: 0.0079 lr: 0.02
2023-03-01 11:04:38 iteration: 18100 loss: 0.0072 lr: 0.02
2023-03-01 11:04:47 iteration: 18200 loss: 0.0074 lr: 0.02
2023-03-01 11:04:56 iteration: 18300 loss: 0.0079 lr: 0.02
2023-03-01 11:05:04 iteration: 18400 loss: 0.0071 lr: 0.02
2023-03-01 11:05:13 iteration: 18500 loss: 0.0073 lr: 0.02
2023-03-01 11:05:21 iteration: 18600 loss: 0.0073 lr: 0.02
2023-03-01 11:05:30 iteration: 18700 loss: 0.0076 lr: 0.02
2023-03-01 11:05:38 iteration: 18800 loss: 0.0075 lr: 0.02
2023-03-01 11:05:47 iteration: 18900 loss: 0.0073 lr: 0.02
2023-03-01 11:05:55 iteration: 19000 loss: 0.0073 lr: 0.02
2023-03-01 11:06:04 iteration: 19100 loss: 0.0075 lr: 0.02
2023-03-01 11:06:12 iteration: 19200 loss: 0.0074 lr: 0.02
2023-03-01 11:06:20 iteration: 19300 loss: 0.0069 lr: 0.02
2023-03-01 11:06:28 iteration: 19400 loss: 0.0065 lr: 0.02
2023-03-01 11:06:37 iteration: 19500 loss: 0.0069 lr: 0.02
2023-03-01 11:06:44 iteration: 19600 loss: 0.0070 lr: 0.02
2023-03-01 11:06:53 iteration: 19700 loss: 0.0068 lr: 0.02
2023-03-01 11:07:01 iteration: 19800 loss: 0.0071 lr: 0.02
2023-03-01 11:07:09 iteration: 19900 loss: 0.0070 lr: 0.02
2023-03-01 11:07:18 iteration: 20000 loss: 0.0069 lr: 0.02
2023-03-01 11:07:26 iteration: 20100 loss: 0.0067 lr: 0.02
2023-03-01 11:07:34 iteration: 20200 loss: 0.0069 lr: 0.02
2023-03-01 11:07:44 iteration: 20300 loss: 0.0070 lr: 0.02
2023-03-01 11:07:53 iteration: 20400 loss: 0.0067 lr: 0.02
2023-03-01 11:08:01 iteration: 20500 loss: 0.0072 lr: 0.02
2023-03-01 11:08:09 iteration: 20600 loss: 0.0063 lr: 0.02
2023-03-01 11:08:17 iteration: 20700 loss: 0.0064 lr: 0.02
2023-03-01 11:08:26 iteration: 20800 loss: 0.0070 lr: 0.02
2023-03-01 11:08:34 iteration: 20900 loss: 0.0074 lr: 0.02
2023-03-01 11:08:42 iteration: 21000 loss: 0.0069 lr: 0.02
2023-03-01 11:08:50 iteration: 21100 loss: 0.0063 lr: 0.02
2023-03-01 11:08:59 iteration: 21200 loss: 0.0069 lr: 0.02
2023-03-01 11:09:08 iteration: 21300 loss: 0.0062 lr: 0.02
2023-03-01 11:09:17 iteration: 21400 loss: 0.0067 lr: 0.02
2023-03-01 11:09:26 iteration: 21500 loss: 0.0068 lr: 0.02
2023-03-01 11:09:35 iteration: 21600 loss: 0.0069 lr: 0.02
2023-03-01 11:09:43 iteration: 21700 loss: 0.0068 lr: 0.02
2023-03-01 11:09:52 iteration: 21800 loss: 0.0063 lr: 0.02
2023-03-01 11:10:00 iteration: 21900 loss: 0.0061 lr: 0.02
2023-03-01 11:10:08 iteration: 22000 loss: 0.0067 lr: 0.02
2023-03-01 11:10:17 iteration: 22100 loss: 0.0065 lr: 0.02
2023-03-01 11:10:25 iteration: 22200 loss: 0.0068 lr: 0.02
2023-03-01 11:10:34 iteration: 22300 loss: 0.0066 lr: 0.02
2023-03-01 11:10:42 iteration: 22400 loss: 0.0064 lr: 0.02
2023-03-01 11:10:51 iteration: 22500 loss: 0.0063 lr: 0.02
2023-03-01 11:10:59 iteration: 22600 loss: 0.0059 lr: 0.02
2023-03-01 11:11:08 iteration: 22700 loss: 0.0069 lr: 0.02
2023-03-01 11:11:16 iteration: 22800 loss: 0.0063 lr: 0.02
2023-03-01 11:11:25 iteration: 22900 loss: 0.0062 lr: 0.02
2023-03-01 11:11:33 iteration: 23000 loss: 0.0063 lr: 0.02
2023-03-01 11:11:42 iteration: 23100 loss: 0.0065 lr: 0.02
2023-03-01 11:11:50 iteration: 23200 loss: 0.0065 lr: 0.02
2023-03-01 11:11:58 iteration: 23300 loss: 0.0060 lr: 0.02
2023-03-01 11:12:07 iteration: 23400 loss: 0.0057 lr: 0.02
2023-03-01 11:12:15 iteration: 23500 loss: 0.0063 lr: 0.02
2023-03-01 11:12:24 iteration: 23600 loss: 0.0064 lr: 0.02
2023-03-01 11:12:31 iteration: 23700 loss: 0.0063 lr: 0.02
2023-03-01 11:12:41 iteration: 23800 loss: 0.0063 lr: 0.02
2023-03-01 11:12:49 iteration: 23900 loss: 0.0059 lr: 0.02
2023-03-01 11:12:57 iteration: 24000 loss: 0.0058 lr: 0.02
2023-03-01 11:13:06 iteration: 24100 loss: 0.0060 lr: 0.02
2023-03-01 11:13:16 iteration: 24200 loss: 0.0060 lr: 0.02
2023-03-01 11:13:25 iteration: 24300 loss: 0.0061 lr: 0.02
2023-03-01 11:13:34 iteration: 24400 loss: 0.0062 lr: 0.02
2023-03-01 11:13:43 iteration: 24500 loss: 0.0060 lr: 0.02
2023-03-01 11:13:51 iteration: 24600 loss: 0.0063 lr: 0.02
2023-03-01 11:14:00 iteration: 24700 loss: 0.0062 lr: 0.02
2023-03-01 11:14:08 iteration: 24800 loss: 0.0060 lr: 0.02
2023-03-01 11:14:16 iteration: 24900 loss: 0.0060 lr: 0.02
2023-03-01 11:14:24 iteration: 25000 loss: 0.0059 lr: 0.02
2023-03-01 11:14:33 iteration: 25100 loss: 0.0056 lr: 0.02
2023-03-01 11:14:42 iteration: 25200 loss: 0.0066 lr: 0.02
2023-03-01 11:14:50 iteration: 25300 loss: 0.0059 lr: 0.02
2023-03-01 11:15:00 iteration: 25400 loss: 0.0057 lr: 0.02
2023-03-01 11:15:09 iteration: 25500 loss: 0.0062 lr: 0.02
2023-03-01 11:15:19 iteration: 25600 loss: 0.0060 lr: 0.02
2023-03-01 11:15:28 iteration: 25700 loss: 0.0054 lr: 0.02
2023-03-01 11:15:36 iteration: 25800 loss: 0.0057 lr: 0.02
2023-03-01 11:15:44 iteration: 25900 loss: 0.0060 lr: 0.02
2023-03-01 11:15:53 iteration: 26000 loss: 0.0060 lr: 0.02
2023-03-01 11:16:02 iteration: 26100 loss: 0.0061 lr: 0.02
2023-03-01 11:16:11 iteration: 26200 loss: 0.0056 lr: 0.02
2023-03-01 11:16:19 iteration: 26300 loss: 0.0058 lr: 0.02
2023-03-01 11:16:27 iteration: 26400 loss: 0.0060 lr: 0.02
2023-03-01 11:16:36 iteration: 26500 loss: 0.0057 lr: 0.02
2023-03-01 11:16:45 iteration: 26600 loss: 0.0060 lr: 0.02
2023-03-01 11:16:53 iteration: 26700 loss: 0.0053 lr: 0.02
2023-03-01 11:17:01 iteration: 26800 loss: 0.0055 lr: 0.02
2023-03-01 11:17:10 iteration: 26900 loss: 0.0057 lr: 0.02
2023-03-01 11:17:18 iteration: 27000 loss: 0.0056 lr: 0.02
2023-03-01 11:17:28 iteration: 27100 loss: 0.0060 lr: 0.02
2023-03-01 11:17:36 iteration: 27200 loss: 0.0060 lr: 0.02
2023-03-01 11:17:44 iteration: 27300 loss: 0.0055 lr: 0.02
2023-03-01 11:17:53 iteration: 27400 loss: 0.0053 lr: 0.02
2023-03-01 11:18:02 iteration: 27500 loss: 0.0056 lr: 0.02
2023-03-01 11:18:10 iteration: 27600 loss: 0.0061 lr: 0.02
2023-03-01 11:18:19 iteration: 27700 loss: 0.0059 lr: 0.02
2023-03-01 11:18:29 iteration: 27800 loss: 0.0054 lr: 0.02
2023-03-01 11:18:38 iteration: 27900 loss: 0.0061 lr: 0.02
2023-03-01 11:18:47 iteration: 28000 loss: 0.0054 lr: 0.02
2023-03-01 11:18:55 iteration: 28100 loss: 0.0052 lr: 0.02
2023-03-01 11:19:04 iteration: 28200 loss: 0.0054 lr: 0.02
2023-03-01 11:19:12 iteration: 28300 loss: 0.0053 lr: 0.02
2023-03-01 11:19:21 iteration: 28400 loss: 0.0058 lr: 0.02
2023-03-01 11:19:29 iteration: 28500 loss: 0.0051 lr: 0.02
2023-03-01 11:19:38 iteration: 28600 loss: 0.0050 lr: 0.02
2023-03-01 11:19:47 iteration: 28700 loss: 0.0051 lr: 0.02
2023-03-01 11:19:56 iteration: 28800 loss: 0.0060 lr: 0.02
2023-03-01 11:20:05 iteration: 28900 loss: 0.0055 lr: 0.02
2023-03-01 11:20:13 iteration: 29000 loss: 0.0052 lr: 0.02
2023-03-01 11:20:22 iteration: 29100 loss: 0.0057 lr: 0.02
2023-03-01 11:20:31 iteration: 29200 loss: 0.0052 lr: 0.02
2023-03-01 11:20:39 iteration: 29300 loss: 0.0055 lr: 0.02
2023-03-01 11:20:48 iteration: 29400 loss: 0.0053 lr: 0.02
2023-03-01 11:20:56 iteration: 29500 loss: 0.0054 lr: 0.02
2023-03-01 11:21:05 iteration: 29600 loss: 0.0053 lr: 0.02
2023-03-01 11:21:13 iteration: 29700 loss: 0.0053 lr: 0.02
2023-03-01 11:21:21 iteration: 29800 loss: 0.0055 lr: 0.02
2023-03-01 11:21:30 iteration: 29900 loss: 0.0054 lr: 0.02
2023-03-01 11:21:39 iteration: 30000 loss: 0.0051 lr: 0.02
2023-03-01 11:21:47 iteration: 30100 loss: 0.0057 lr: 0.02
2023-03-01 11:21:56 iteration: 30200 loss: 0.0052 lr: 0.02
2023-03-01 11:22:04 iteration: 30300 loss: 0.0051 lr: 0.02
2023-03-01 11:22:13 iteration: 30400 loss: 0.0057 lr: 0.02
2023-03-01 11:22:21 iteration: 30500 loss: 0.0053 lr: 0.02
2023-03-01 11:22:30 iteration: 30600 loss: 0.0052 lr: 0.02
2023-03-01 11:22:39 iteration: 30700 loss: 0.0056 lr: 0.02
2023-03-01 11:22:47 iteration: 30800 loss: 0.0056 lr: 0.02
2023-03-01 11:22:56 iteration: 30900 loss: 0.0054 lr: 0.02
2023-03-01 11:23:05 iteration: 31000 loss: 0.0052 lr: 0.02
2023-03-01 11:23:13 iteration: 31100 loss: 0.0056 lr: 0.02
2023-03-01 11:23:21 iteration: 31200 loss: 0.0049 lr: 0.02
2023-03-01 11:23:30 iteration: 31300 loss: 0.0051 lr: 0.02
2023-03-01 11:23:38 iteration: 31400 loss: 0.0053 lr: 0.02
2023-03-01 11:23:47 iteration: 31500 loss: 0.0054 lr: 0.02
2023-03-01 11:23:56 iteration: 31600 loss: 0.0057 lr: 0.02
2023-03-01 11:24:04 iteration: 31700 loss: 0.0051 lr: 0.02
2023-03-01 11:24:13 iteration: 31800 loss: 0.0053 lr: 0.02
2023-03-01 11:24:21 iteration: 31900 loss: 0.0051 lr: 0.02
2023-03-01 11:24:30 iteration: 32000 loss: 0.0052 lr: 0.02
2023-03-01 11:24:39 iteration: 32100 loss: 0.0051 lr: 0.02
2023-03-01 11:24:47 iteration: 32200 loss: 0.0052 lr: 0.02
2023-03-01 11:24:56 iteration: 32300 loss: 0.0054 lr: 0.02
2023-03-01 11:25:05 iteration: 32400 loss: 0.0052 lr: 0.02
2023-03-01 11:25:13 iteration: 32500 loss: 0.0051 lr: 0.02
2023-03-01 11:25:21 iteration: 32600 loss: 0.0051 lr: 0.02
2023-03-01 12:08:07 Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15],
                [16]],
 'all_joints_names': ['Head',
                      'Snout',
                      'LeftEar',
                      'RightEar',
                      'Shoulder',
                      'Spine1',
                      'Spine2',
                      'Spine3',
                      'Spine4',
                      'TailBase',
                      'Tail1',
                      'Tail2',
                      'TailEnd',
                      'LeftPaw1',
                      'RightPaw1',
                      'LeftPaw2',
                      'RightPaw2'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 1,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\CV_Mena95shuffle1.mat',
 'dataset_type': 'imgaug',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot-32600',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_CVFeb26\\Documentation_data-CV_95shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 17,
 'optimizer': 'sgd',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': 'D:/DLC/CV-Mena-2023-02-26',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'D:\\DLC\\CV-Mena-2023-02-26\\dlc-models\\iteration-0\\CVFeb26-trainset95shuffle1\\train\\snapshot',
 'stride': 8.0,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2023-03-01 12:08:24 iteration: 32700 loss: 0.0047 lr: 0.005
2023-03-01 12:08:35 iteration: 32800 loss: 0.0048 lr: 0.005
2023-03-01 12:08:45 iteration: 32900 loss: 0.0044 lr: 0.005
2023-03-01 12:08:55 iteration: 33000 loss: 0.0047 lr: 0.005
2023-03-01 12:09:05 iteration: 33100 loss: 0.0043 lr: 0.005
2023-03-01 12:09:15 iteration: 33200 loss: 0.0046 lr: 0.005
2023-03-01 12:09:24 iteration: 33300 loss: 0.0047 lr: 0.005
2023-03-01 12:09:35 iteration: 33400 loss: 0.0043 lr: 0.005
2023-03-01 12:09:44 iteration: 33500 loss: 0.0042 lr: 0.005
2023-03-01 12:09:52 iteration: 33600 loss: 0.0041 lr: 0.005
2023-03-01 12:10:01 iteration: 33700 loss: 0.0043 lr: 0.005
2023-03-01 12:10:10 iteration: 33800 loss: 0.0042 lr: 0.005
2023-03-01 12:10:20 iteration: 33900 loss: 0.0044 lr: 0.005
2023-03-01 12:10:29 iteration: 34000 loss: 0.0040 lr: 0.005
2023-03-01 12:10:37 iteration: 34100 loss: 0.0044 lr: 0.005
2023-03-01 12:10:47 iteration: 34200 loss: 0.0044 lr: 0.005
2023-03-01 12:10:56 iteration: 34300 loss: 0.0043 lr: 0.005
2023-03-01 12:11:05 iteration: 34400 loss: 0.0041 lr: 0.005
2023-03-01 12:11:14 iteration: 34500 loss: 0.0043 lr: 0.005
2023-03-01 12:11:23 iteration: 34600 loss: 0.0041 lr: 0.005
2023-03-01 12:11:31 iteration: 34700 loss: 0.0041 lr: 0.005
2023-03-01 12:11:40 iteration: 34800 loss: 0.0044 lr: 0.005
2023-03-01 12:11:49 iteration: 34900 loss: 0.0042 lr: 0.005
2023-03-01 12:11:59 iteration: 35000 loss: 0.0042 lr: 0.005
2023-03-01 12:12:08 iteration: 35100 loss: 0.0045 lr: 0.005
2023-03-01 12:12:16 iteration: 35200 loss: 0.0045 lr: 0.005
2023-03-01 12:12:24 iteration: 35300 loss: 0.0039 lr: 0.005
2023-03-01 12:12:33 iteration: 35400 loss: 0.0043 lr: 0.005
2023-03-01 12:12:42 iteration: 35500 loss: 0.0041 lr: 0.005
2023-03-01 12:12:50 iteration: 35600 loss: 0.0038 lr: 0.005
2023-03-01 12:13:00 iteration: 35700 loss: 0.0045 lr: 0.005
2023-03-01 12:13:08 iteration: 35800 loss: 0.0042 lr: 0.005
2023-03-01 12:13:17 iteration: 35900 loss: 0.0041 lr: 0.005
2023-03-01 12:13:25 iteration: 36000 loss: 0.0042 lr: 0.005
2023-03-01 12:13:35 iteration: 36100 loss: 0.0039 lr: 0.005
2023-03-01 12:13:44 iteration: 36200 loss: 0.0043 lr: 0.005
2023-03-01 12:13:53 iteration: 36300 loss: 0.0043 lr: 0.005
2023-03-01 12:14:01 iteration: 36400 loss: 0.0044 lr: 0.005
2023-03-01 12:14:10 iteration: 36500 loss: 0.0039 lr: 0.005
2023-03-01 12:14:19 iteration: 36600 loss: 0.0041 lr: 0.005
2023-03-01 12:14:28 iteration: 36700 loss: 0.0044 lr: 0.005
2023-03-01 12:14:36 iteration: 36800 loss: 0.0039 lr: 0.005
2023-03-01 12:14:45 iteration: 36900 loss: 0.0041 lr: 0.005
2023-03-01 12:14:53 iteration: 37000 loss: 0.0041 lr: 0.005
2023-03-01 12:15:02 iteration: 37100 loss: 0.0044 lr: 0.005
2023-03-01 12:15:11 iteration: 37200 loss: 0.0040 lr: 0.005
2023-03-01 12:15:19 iteration: 37300 loss: 0.0037 lr: 0.005
2023-03-01 12:15:28 iteration: 37400 loss: 0.0040 lr: 0.005
2023-03-01 12:15:36 iteration: 37500 loss: 0.0042 lr: 0.005
2023-03-01 12:15:45 iteration: 37600 loss: 0.0040 lr: 0.005
2023-03-01 12:15:53 iteration: 37700 loss: 0.0039 lr: 0.005
2023-03-01 12:16:02 iteration: 37800 loss: 0.0041 lr: 0.005
2023-03-01 12:16:11 iteration: 37900 loss: 0.0041 lr: 0.005
2023-03-01 12:16:21 iteration: 38000 loss: 0.0040 lr: 0.005
2023-03-01 12:16:30 iteration: 38100 loss: 0.0043 lr: 0.005
2023-03-01 12:16:38 iteration: 38200 loss: 0.0038 lr: 0.005
2023-03-01 12:16:47 iteration: 38300 loss: 0.0042 lr: 0.005
2023-03-01 12:16:56 iteration: 38400 loss: 0.0040 lr: 0.005
2023-03-01 12:17:04 iteration: 38500 loss: 0.0041 lr: 0.005
2023-03-01 12:17:14 iteration: 38600 loss: 0.0039 lr: 0.005
2023-03-01 12:17:22 iteration: 38700 loss: 0.0039 lr: 0.005
2023-03-01 12:17:30 iteration: 38800 loss: 0.0041 lr: 0.005
2023-03-01 12:17:39 iteration: 38900 loss: 0.0044 lr: 0.005
2023-03-01 12:17:47 iteration: 39000 loss: 0.0043 lr: 0.005
2023-03-01 12:17:56 iteration: 39100 loss: 0.0038 lr: 0.005
2023-03-01 12:18:05 iteration: 39200 loss: 0.0040 lr: 0.005
2023-03-01 12:18:14 iteration: 39300 loss: 0.0041 lr: 0.005
2023-03-01 12:18:22 iteration: 39400 loss: 0.0039 lr: 0.005
2023-03-01 12:18:30 iteration: 39500 loss: 0.0039 lr: 0.005
2023-03-01 12:18:39 iteration: 39600 loss: 0.0041 lr: 0.005
2023-03-01 12:18:49 iteration: 39700 loss: 0.0040 lr: 0.005
2023-03-01 12:18:57 iteration: 39800 loss: 0.0044 lr: 0.005
2023-03-01 12:19:05 iteration: 39900 loss: 0.0038 lr: 0.005
2023-03-01 12:19:13 iteration: 40000 loss: 0.0040 lr: 0.005
2023-03-01 12:19:22 iteration: 40100 loss: 0.0040 lr: 0.005
2023-03-01 12:19:30 iteration: 40200 loss: 0.0040 lr: 0.005
2023-03-01 12:19:38 iteration: 40300 loss: 0.0038 lr: 0.005
2023-03-01 12:19:47 iteration: 40400 loss: 0.0041 lr: 0.005
2023-03-01 12:19:55 iteration: 40500 loss: 0.0037 lr: 0.005
2023-03-01 12:20:03 iteration: 40600 loss: 0.0039 lr: 0.005
2023-03-01 12:20:11 iteration: 40700 loss: 0.0040 lr: 0.005
2023-03-01 12:20:20 iteration: 40800 loss: 0.0037 lr: 0.005
2023-03-01 12:20:28 iteration: 40900 loss: 0.0041 lr: 0.005
2023-03-01 12:20:36 iteration: 41000 loss: 0.0043 lr: 0.005
2023-03-01 12:20:46 iteration: 41100 loss: 0.0040 lr: 0.005
2023-03-01 12:20:54 iteration: 41200 loss: 0.0041 lr: 0.005
2023-03-01 12:21:03 iteration: 41300 loss: 0.0039 lr: 0.005
2023-03-01 12:21:12 iteration: 41400 loss: 0.0039 lr: 0.005
2023-03-01 12:21:20 iteration: 41500 loss: 0.0039 lr: 0.005
2023-03-01 12:21:28 iteration: 41600 loss: 0.0039 lr: 0.005
2023-03-01 12:21:37 iteration: 41700 loss: 0.0039 lr: 0.005
2023-03-01 12:21:46 iteration: 41800 loss: 0.0042 lr: 0.005
2023-03-01 12:21:54 iteration: 41900 loss: 0.0037 lr: 0.005
2023-03-01 12:22:03 iteration: 42000 loss: 0.0038 lr: 0.005
2023-03-01 12:22:12 iteration: 42100 loss: 0.0041 lr: 0.005
2023-03-01 12:22:23 iteration: 42200 loss: 0.0039 lr: 0.005
2023-03-01 12:22:31 iteration: 42300 loss: 0.0039 lr: 0.005
2023-03-01 12:22:40 iteration: 42400 loss: 0.0038 lr: 0.005
2023-03-01 12:22:49 iteration: 42500 loss: 0.0038 lr: 0.005
2023-03-01 12:22:57 iteration: 42600 loss: 0.0042 lr: 0.005
2023-03-01 12:23:07 iteration: 42700 loss: 0.0047 lr: 0.02
2023-03-01 12:23:15 iteration: 42800 loss: 0.0049 lr: 0.02
2023-03-01 12:23:24 iteration: 42900 loss: 0.0048 lr: 0.02
2023-03-01 12:23:33 iteration: 43000 loss: 0.0048 lr: 0.02
2023-03-01 12:23:41 iteration: 43100 loss: 0.0043 lr: 0.02
2023-03-01 12:23:49 iteration: 43200 loss: 0.0051 lr: 0.02
2023-03-01 12:23:58 iteration: 43300 loss: 0.0046 lr: 0.02
2023-03-01 12:24:07 iteration: 43400 loss: 0.0048 lr: 0.02
2023-03-01 12:24:16 iteration: 43500 loss: 0.0047 lr: 0.02
2023-03-01 12:24:25 iteration: 43600 loss: 0.0047 lr: 0.02
2023-03-01 12:24:35 iteration: 43700 loss: 0.0043 lr: 0.02
2023-03-01 12:24:43 iteration: 43800 loss: 0.0051 lr: 0.02
2023-03-01 12:24:52 iteration: 43900 loss: 0.0049 lr: 0.02
2023-03-01 12:25:00 iteration: 44000 loss: 0.0048 lr: 0.02
2023-03-01 12:25:08 iteration: 44100 loss: 0.0047 lr: 0.02
2023-03-01 12:25:17 iteration: 44200 loss: 0.0048 lr: 0.02
2023-03-01 12:25:26 iteration: 44300 loss: 0.0045 lr: 0.02
2023-03-01 12:25:34 iteration: 44400 loss: 0.0046 lr: 0.02
2023-03-01 12:25:43 iteration: 44500 loss: 0.0044 lr: 0.02
2023-03-01 12:25:51 iteration: 44600 loss: 0.0048 lr: 0.02
2023-03-01 12:26:01 iteration: 44700 loss: 0.0043 lr: 0.02
2023-03-01 12:26:10 iteration: 44800 loss: 0.0046 lr: 0.02
2023-03-01 12:26:21 iteration: 44900 loss: 0.0046 lr: 0.02
2023-03-01 12:26:30 iteration: 45000 loss: 0.0046 lr: 0.02
2023-03-01 12:26:39 iteration: 45100 loss: 0.0048 lr: 0.02
2023-03-01 12:26:47 iteration: 45200 loss: 0.0047 lr: 0.02
2023-03-01 12:26:57 iteration: 45300 loss: 0.0046 lr: 0.02
2023-03-01 12:27:05 iteration: 45400 loss: 0.0049 lr: 0.02
2023-03-01 12:27:14 iteration: 45500 loss: 0.0048 lr: 0.02
2023-03-01 12:27:22 iteration: 45600 loss: 0.0045 lr: 0.02
2023-03-01 12:27:31 iteration: 45700 loss: 0.0043 lr: 0.02
2023-03-01 12:27:40 iteration: 45800 loss: 0.0044 lr: 0.02
2023-03-01 12:27:48 iteration: 45900 loss: 0.0046 lr: 0.02
2023-03-01 12:27:57 iteration: 46000 loss: 0.0044 lr: 0.02
2023-03-01 12:28:05 iteration: 46100 loss: 0.0045 lr: 0.02
2023-03-01 12:28:13 iteration: 46200 loss: 0.0047 lr: 0.02
2023-03-01 12:28:22 iteration: 46300 loss: 0.0042 lr: 0.02
2023-03-01 12:28:31 iteration: 46400 loss: 0.0046 lr: 0.02
2023-03-01 12:28:40 iteration: 46500 loss: 0.0050 lr: 0.02
2023-03-01 12:28:48 iteration: 46600 loss: 0.0045 lr: 0.02
2023-03-01 12:28:57 iteration: 46700 loss: 0.0047 lr: 0.02
2023-03-01 12:29:06 iteration: 46800 loss: 0.0047 lr: 0.02
2023-03-01 12:29:14 iteration: 46900 loss: 0.0045 lr: 0.02
2023-03-01 12:29:22 iteration: 47000 loss: 0.0042 lr: 0.02
2023-03-01 12:29:31 iteration: 47100 loss: 0.0047 lr: 0.02
2023-03-01 12:29:40 iteration: 47200 loss: 0.0044 lr: 0.02
2023-03-01 12:29:48 iteration: 47300 loss: 0.0044 lr: 0.02
2023-03-01 12:29:57 iteration: 47400 loss: 0.0048 lr: 0.02
2023-03-01 12:30:06 iteration: 47500 loss: 0.0050 lr: 0.02
2023-03-01 12:30:15 iteration: 47600 loss: 0.0049 lr: 0.02
2023-03-01 12:30:24 iteration: 47700 loss: 0.0049 lr: 0.02
2023-03-01 12:30:33 iteration: 47800 loss: 0.0047 lr: 0.02
2023-03-01 12:30:42 iteration: 47900 loss: 0.0044 lr: 0.02
2023-03-01 12:30:50 iteration: 48000 loss: 0.0048 lr: 0.02
2023-03-01 12:30:59 iteration: 48100 loss: 0.0047 lr: 0.02
2023-03-01 12:31:07 iteration: 48200 loss: 0.0045 lr: 0.02
2023-03-01 12:31:16 iteration: 48300 loss: 0.0049 lr: 0.02
2023-03-01 12:31:25 iteration: 48400 loss: 0.0044 lr: 0.02
2023-03-01 12:31:34 iteration: 48500 loss: 0.0044 lr: 0.02
2023-03-01 12:31:42 iteration: 48600 loss: 0.0045 lr: 0.02
2023-03-01 12:31:51 iteration: 48700 loss: 0.0041 lr: 0.02
2023-03-01 12:31:59 iteration: 48800 loss: 0.0042 lr: 0.02
2023-03-01 12:32:08 iteration: 48900 loss: 0.0043 lr: 0.02
2023-03-01 12:32:16 iteration: 49000 loss: 0.0046 lr: 0.02
2023-03-01 12:32:25 iteration: 49100 loss: 0.0039 lr: 0.02
2023-03-01 12:32:34 iteration: 49200 loss: 0.0039 lr: 0.02
2023-03-01 12:32:42 iteration: 49300 loss: 0.0042 lr: 0.02
2023-03-01 12:32:50 iteration: 49400 loss: 0.0047 lr: 0.02
2023-03-01 12:32:58 iteration: 49500 loss: 0.0044 lr: 0.02
2023-03-01 12:33:07 iteration: 49600 loss: 0.0043 lr: 0.02
2023-03-01 12:33:15 iteration: 49700 loss: 0.0044 lr: 0.02
2023-03-01 12:33:25 iteration: 49800 loss: 0.0045 lr: 0.02
2023-03-01 12:33:33 iteration: 49900 loss: 0.0043 lr: 0.02
2023-03-01 12:33:41 iteration: 50000 loss: 0.0046 lr: 0.02
2023-03-01 12:33:50 iteration: 50100 loss: 0.0042 lr: 0.02
2023-03-01 12:33:59 iteration: 50200 loss: 0.0041 lr: 0.02
2023-03-01 12:34:07 iteration: 50300 loss: 0.0043 lr: 0.02
2023-03-01 12:34:15 iteration: 50400 loss: 0.0044 lr: 0.02
2023-03-01 12:34:24 iteration: 50500 loss: 0.0041 lr: 0.02
2023-03-01 12:34:33 iteration: 50600 loss: 0.0040 lr: 0.02
2023-03-01 12:34:41 iteration: 50700 loss: 0.0044 lr: 0.02
2023-03-01 12:34:49 iteration: 50800 loss: 0.0039 lr: 0.02
2023-03-01 12:34:57 iteration: 50900 loss: 0.0043 lr: 0.02
2023-03-01 12:35:06 iteration: 51000 loss: 0.0042 lr: 0.02
2023-03-01 12:35:14 iteration: 51100 loss: 0.0043 lr: 0.02
2023-03-01 12:35:23 iteration: 51200 loss: 0.0043 lr: 0.02
2023-03-01 12:35:31 iteration: 51300 loss: 0.0040 lr: 0.02
2023-03-01 12:35:40 iteration: 51400 loss: 0.0043 lr: 0.02
2023-03-01 12:35:49 iteration: 51500 loss: 0.0046 lr: 0.02
2023-03-01 12:35:58 iteration: 51600 loss: 0.0046 lr: 0.02
2023-03-01 12:36:07 iteration: 51700 loss: 0.0040 lr: 0.02
2023-03-01 12:36:15 iteration: 51800 loss: 0.0043 lr: 0.02
2023-03-01 12:36:24 iteration: 51900 loss: 0.0040 lr: 0.02
2023-03-01 12:36:33 iteration: 52000 loss: 0.0047 lr: 0.02
2023-03-01 12:36:41 iteration: 52100 loss: 0.0040 lr: 0.02
2023-03-01 12:36:49 iteration: 52200 loss: 0.0043 lr: 0.02
2023-03-01 12:36:57 iteration: 52300 loss: 0.0044 lr: 0.02
2023-03-01 12:37:06 iteration: 52400 loss: 0.0042 lr: 0.02
2023-03-01 12:37:14 iteration: 52500 loss: 0.0043 lr: 0.02
2023-03-01 12:37:23 iteration: 52600 loss: 0.0043 lr: 0.02
2023-03-01 12:37:32 iteration: 52700 loss: 0.0046 lr: 0.02
2023-03-01 12:37:40 iteration: 52800 loss: 0.0043 lr: 0.02
2023-03-01 12:37:49 iteration: 52900 loss: 0.0043 lr: 0.02
2023-03-01 12:37:57 iteration: 53000 loss: 0.0038 lr: 0.02
2023-03-01 12:38:07 iteration: 53100 loss: 0.0037 lr: 0.02
2023-03-01 12:38:15 iteration: 53200 loss: 0.0043 lr: 0.02
2023-03-01 12:38:24 iteration: 53300 loss: 0.0038 lr: 0.02
2023-03-01 12:38:32 iteration: 53400 loss: 0.0041 lr: 0.02
2023-03-01 12:38:41 iteration: 53500 loss: 0.0042 lr: 0.02
2023-03-01 12:38:49 iteration: 53600 loss: 0.0041 lr: 0.02
2023-03-01 12:38:58 iteration: 53700 loss: 0.0042 lr: 0.02
2023-03-01 12:39:06 iteration: 53800 loss: 0.0042 lr: 0.02
2023-03-01 12:39:15 iteration: 53900 loss: 0.0045 lr: 0.02
2023-03-01 12:39:24 iteration: 54000 loss: 0.0040 lr: 0.02
2023-03-01 12:39:33 iteration: 54100 loss: 0.0041 lr: 0.02
2023-03-01 12:39:41 iteration: 54200 loss: 0.0038 lr: 0.02
2023-03-01 12:39:49 iteration: 54300 loss: 0.0040 lr: 0.02
2023-03-01 12:39:57 iteration: 54400 loss: 0.0043 lr: 0.02
2023-03-01 12:40:05 iteration: 54500 loss: 0.0040 lr: 0.02
2023-03-01 12:40:14 iteration: 54600 loss: 0.0041 lr: 0.02
2023-03-01 12:40:23 iteration: 54700 loss: 0.0040 lr: 0.02
2023-03-01 12:40:30 iteration: 54800 loss: 0.0040 lr: 0.02
2023-03-01 12:40:39 iteration: 54900 loss: 0.0040 lr: 0.02
2023-03-01 12:40:48 iteration: 55000 loss: 0.0040 lr: 0.02
2023-03-01 12:40:56 iteration: 55100 loss: 0.0040 lr: 0.02
2023-03-01 12:41:05 iteration: 55200 loss: 0.0042 lr: 0.02
2023-03-01 12:41:14 iteration: 55300 loss: 0.0037 lr: 0.02
2023-03-01 12:41:22 iteration: 55400 loss: 0.0041 lr: 0.02
2023-03-01 12:41:30 iteration: 55500 loss: 0.0040 lr: 0.02
2023-03-01 12:41:39 iteration: 55600 loss: 0.0041 lr: 0.02
2023-03-01 12:41:47 iteration: 55700 loss: 0.0039 lr: 0.02
2023-03-01 12:41:55 iteration: 55800 loss: 0.0038 lr: 0.02
2023-03-01 12:42:04 iteration: 55900 loss: 0.0042 lr: 0.02
2023-03-01 12:42:12 iteration: 56000 loss: 0.0041 lr: 0.02
2023-03-01 12:42:21 iteration: 56100 loss: 0.0044 lr: 0.02
2023-03-01 12:42:29 iteration: 56200 loss: 0.0045 lr: 0.02
2023-03-01 12:42:38 iteration: 56300 loss: 0.0042 lr: 0.02
2023-03-01 12:42:46 iteration: 56400 loss: 0.0039 lr: 0.02
2023-03-01 12:42:54 iteration: 56500 loss: 0.0041 lr: 0.02
2023-03-01 12:43:02 iteration: 56600 loss: 0.0040 lr: 0.02
2023-03-01 12:43:11 iteration: 56700 loss: 0.0043 lr: 0.02
2023-03-01 12:43:19 iteration: 56800 loss: 0.0039 lr: 0.02
2023-03-01 12:43:28 iteration: 56900 loss: 0.0038 lr: 0.02
2023-03-01 12:43:37 iteration: 57000 loss: 0.0040 lr: 0.02
2023-03-01 12:43:46 iteration: 57100 loss: 0.0042 lr: 0.02
2023-03-01 12:43:55 iteration: 57200 loss: 0.0037 lr: 0.02
2023-03-01 12:44:04 iteration: 57300 loss: 0.0040 lr: 0.02
2023-03-01 12:44:12 iteration: 57400 loss: 0.0040 lr: 0.02
2023-03-01 12:44:21 iteration: 57500 loss: 0.0037 lr: 0.02
2023-03-01 12:44:30 iteration: 57600 loss: 0.0039 lr: 0.02
2023-03-01 12:44:38 iteration: 57700 loss: 0.0041 lr: 0.02
2023-03-01 12:44:47 iteration: 57800 loss: 0.0041 lr: 0.02
2023-03-01 12:44:56 iteration: 57900 loss: 0.0041 lr: 0.02
2023-03-01 12:45:04 iteration: 58000 loss: 0.0042 lr: 0.02
2023-03-01 12:45:12 iteration: 58100 loss: 0.0043 lr: 0.02
2023-03-01 12:45:21 iteration: 58200 loss: 0.0035 lr: 0.02
2023-03-01 12:45:30 iteration: 58300 loss: 0.0039 lr: 0.02
2023-03-01 12:45:39 iteration: 58400 loss: 0.0041 lr: 0.02
2023-03-01 12:45:47 iteration: 58500 loss: 0.0038 lr: 0.02
2023-03-01 12:45:56 iteration: 58600 loss: 0.0037 lr: 0.02
2023-03-01 12:46:04 iteration: 58700 loss: 0.0041 lr: 0.02
2023-03-01 12:46:12 iteration: 58800 loss: 0.0039 lr: 0.02
2023-03-01 12:46:21 iteration: 58900 loss: 0.0039 lr: 0.02
2023-03-01 12:46:30 iteration: 59000 loss: 0.0043 lr: 0.02
2023-03-01 12:46:39 iteration: 59100 loss: 0.0040 lr: 0.02
2023-03-01 12:46:48 iteration: 59200 loss: 0.0041 lr: 0.02
2023-03-01 12:46:56 iteration: 59300 loss: 0.0039 lr: 0.02
2023-03-01 12:47:04 iteration: 59400 loss: 0.0037 lr: 0.02
2023-03-01 12:47:13 iteration: 59500 loss: 0.0039 lr: 0.02
2023-03-01 12:47:22 iteration: 59600 loss: 0.0040 lr: 0.02
2023-03-01 12:47:30 iteration: 59700 loss: 0.0045 lr: 0.02
2023-03-01 12:47:39 iteration: 59800 loss: 0.0040 lr: 0.02
2023-03-01 12:47:48 iteration: 59900 loss: 0.0039 lr: 0.02
2023-03-01 12:47:56 iteration: 60000 loss: 0.0043 lr: 0.02
2023-03-01 12:48:05 iteration: 60100 loss: 0.0042 lr: 0.02
2023-03-01 12:48:13 iteration: 60200 loss: 0.0039 lr: 0.02
2023-03-01 12:48:21 iteration: 60300 loss: 0.0044 lr: 0.02
2023-03-01 12:48:29 iteration: 60400 loss: 0.0036 lr: 0.02
2023-03-01 12:48:38 iteration: 60500 loss: 0.0038 lr: 0.02
2023-03-01 12:48:46 iteration: 60600 loss: 0.0040 lr: 0.02
2023-03-01 12:48:54 iteration: 60700 loss: 0.0036 lr: 0.02
2023-03-01 12:49:03 iteration: 60800 loss: 0.0037 lr: 0.02
2023-03-01 12:49:11 iteration: 60900 loss: 0.0039 lr: 0.02
2023-03-01 12:49:21 iteration: 61000 loss: 0.0035 lr: 0.02
2023-03-01 12:49:29 iteration: 61100 loss: 0.0035 lr: 0.02
2023-03-01 12:49:38 iteration: 61200 loss: 0.0037 lr: 0.02
2023-03-01 12:49:46 iteration: 61300 loss: 0.0035 lr: 0.02
2023-03-01 12:49:54 iteration: 61400 loss: 0.0037 lr: 0.02
2023-03-01 12:50:03 iteration: 61500 loss: 0.0040 lr: 0.02
2023-03-01 12:50:11 iteration: 61600 loss: 0.0038 lr: 0.02
2023-03-01 12:50:20 iteration: 61700 loss: 0.0039 lr: 0.02
2023-03-01 12:50:28 iteration: 61800 loss: 0.0039 lr: 0.02
2023-03-01 12:50:36 iteration: 61900 loss: 0.0041 lr: 0.02
2023-03-01 12:50:45 iteration: 62000 loss: 0.0039 lr: 0.02
2023-03-01 12:50:53 iteration: 62100 loss: 0.0038 lr: 0.02
2023-03-01 12:51:01 iteration: 62200 loss: 0.0039 lr: 0.02
2023-03-01 12:51:10 iteration: 62300 loss: 0.0043 lr: 0.02
2023-03-01 12:51:19 iteration: 62400 loss: 0.0040 lr: 0.02
2023-03-01 12:51:28 iteration: 62500 loss: 0.0040 lr: 0.02
2023-03-01 12:51:36 iteration: 62600 loss: 0.0034 lr: 0.02
2023-03-01 12:51:44 iteration: 62700 loss: 0.0037 lr: 0.02
2023-03-01 12:51:52 iteration: 62800 loss: 0.0038 lr: 0.02
2023-03-01 12:52:00 iteration: 62900 loss: 0.0036 lr: 0.02
2023-03-01 12:52:08 iteration: 63000 loss: 0.0034 lr: 0.02
2023-03-01 12:52:18 iteration: 63100 loss: 0.0039 lr: 0.02
2023-03-01 12:52:27 iteration: 63200 loss: 0.0034 lr: 0.02
2023-03-01 12:52:35 iteration: 63300 loss: 0.0039 lr: 0.02
2023-03-01 12:52:44 iteration: 63400 loss: 0.0037 lr: 0.02
2023-03-01 12:52:52 iteration: 63500 loss: 0.0037 lr: 0.02
2023-03-01 12:53:01 iteration: 63600 loss: 0.0037 lr: 0.02
2023-03-01 12:53:10 iteration: 63700 loss: 0.0038 lr: 0.02
2023-03-01 12:53:19 iteration: 63800 loss: 0.0040 lr: 0.02
2023-03-01 12:53:29 iteration: 63900 loss: 0.0038 lr: 0.02
2023-03-01 12:53:37 iteration: 64000 loss: 0.0035 lr: 0.02
2023-03-01 12:53:46 iteration: 64100 loss: 0.0036 lr: 0.02
2023-03-01 12:53:54 iteration: 64200 loss: 0.0032 lr: 0.02
2023-03-01 12:54:02 iteration: 64300 loss: 0.0033 lr: 0.02
2023-03-01 12:54:11 iteration: 64400 loss: 0.0039 lr: 0.02
2023-03-01 12:54:19 iteration: 64500 loss: 0.0038 lr: 0.02
2023-03-01 12:54:27 iteration: 64600 loss: 0.0037 lr: 0.02
2023-03-01 12:54:37 iteration: 64700 loss: 0.0039 lr: 0.02
2023-03-01 12:54:46 iteration: 64800 loss: 0.0037 lr: 0.02
2023-03-01 12:54:55 iteration: 64900 loss: 0.0036 lr: 0.02
2023-03-01 12:55:04 iteration: 65000 loss: 0.0035 lr: 0.02
2023-03-01 12:55:13 iteration: 65100 loss: 0.0039 lr: 0.02
2023-03-01 12:55:21 iteration: 65200 loss: 0.0039 lr: 0.02
2023-03-01 12:55:30 iteration: 65300 loss: 0.0036 lr: 0.02
2023-03-01 12:55:40 iteration: 65400 loss: 0.0036 lr: 0.02
2023-03-01 12:55:48 iteration: 65500 loss: 0.0036 lr: 0.02
2023-03-01 12:55:56 iteration: 65600 loss: 0.0037 lr: 0.02
2023-03-01 12:56:04 iteration: 65700 loss: 0.0037 lr: 0.02
2023-03-01 12:56:13 iteration: 65800 loss: 0.0036 lr: 0.02
2023-03-01 12:56:22 iteration: 65900 loss: 0.0036 lr: 0.02
2023-03-01 12:56:31 iteration: 66000 loss: 0.0037 lr: 0.02
2023-03-01 12:56:39 iteration: 66100 loss: 0.0036 lr: 0.02
2023-03-01 12:56:48 iteration: 66200 loss: 0.0036 lr: 0.02
2023-03-01 12:56:56 iteration: 66300 loss: 0.0038 lr: 0.02
2023-03-01 12:57:05 iteration: 66400 loss: 0.0039 lr: 0.02
2023-03-01 12:57:13 iteration: 66500 loss: 0.0035 lr: 0.02
2023-03-01 12:57:22 iteration: 66600 loss: 0.0035 lr: 0.02
2023-03-01 12:57:31 iteration: 66700 loss: 0.0037 lr: 0.02
2023-03-01 12:57:39 iteration: 66800 loss: 0.0036 lr: 0.02
2023-03-01 12:57:47 iteration: 66900 loss: 0.0036 lr: 0.02
2023-03-01 12:57:56 iteration: 67000 loss: 0.0034 lr: 0.02
2023-03-01 12:58:05 iteration: 67100 loss: 0.0035 lr: 0.02
2023-03-01 12:58:15 iteration: 67200 loss: 0.0037 lr: 0.02
2023-03-01 12:58:23 iteration: 67300 loss: 0.0035 lr: 0.02
2023-03-01 12:58:32 iteration: 67400 loss: 0.0036 lr: 0.02
2023-03-01 12:58:40 iteration: 67500 loss: 0.0038 lr: 0.02
2023-03-01 12:58:49 iteration: 67600 loss: 0.0035 lr: 0.02
2023-03-01 12:58:57 iteration: 67700 loss: 0.0037 lr: 0.02
2023-03-01 12:59:06 iteration: 67800 loss: 0.0037 lr: 0.02
2023-03-01 12:59:14 iteration: 67900 loss: 0.0038 lr: 0.02
2023-03-01 12:59:23 iteration: 68000 loss: 0.0038 lr: 0.02
2023-03-01 12:59:31 iteration: 68100 loss: 0.0037 lr: 0.02
2023-03-01 12:59:40 iteration: 68200 loss: 0.0034 lr: 0.02
2023-03-01 12:59:48 iteration: 68300 loss: 0.0036 lr: 0.02
2023-03-01 12:59:57 iteration: 68400 loss: 0.0034 lr: 0.02
2023-03-01 13:00:05 iteration: 68500 loss: 0.0036 lr: 0.02
2023-03-01 13:00:14 iteration: 68600 loss: 0.0036 lr: 0.02
2023-03-01 13:00:22 iteration: 68700 loss: 0.0036 lr: 0.02
2023-03-01 13:00:31 iteration: 68800 loss: 0.0035 lr: 0.02
2023-03-01 13:00:40 iteration: 68900 loss: 0.0036 lr: 0.02
2023-03-01 13:00:49 iteration: 69000 loss: 0.0035 lr: 0.02
2023-03-01 13:00:58 iteration: 69100 loss: 0.0035 lr: 0.02
2023-03-01 13:01:08 iteration: 69200 loss: 0.0040 lr: 0.02
2023-03-01 13:01:18 iteration: 69300 loss: 0.0036 lr: 0.02
2023-03-01 13:01:26 iteration: 69400 loss: 0.0034 lr: 0.02
2023-03-01 13:01:35 iteration: 69500 loss: 0.0035 lr: 0.02
2023-03-01 13:01:44 iteration: 69600 loss: 0.0036 lr: 0.02
2023-03-01 13:01:52 iteration: 69700 loss: 0.0038 lr: 0.02
2023-03-01 13:02:01 iteration: 69800 loss: 0.0035 lr: 0.02
2023-03-01 13:02:09 iteration: 69900 loss: 0.0034 lr: 0.02
2023-03-01 13:02:17 iteration: 70000 loss: 0.0035 lr: 0.02
2023-03-01 13:02:26 iteration: 70100 loss: 0.0038 lr: 0.02
2023-03-01 13:02:35 iteration: 70200 loss: 0.0036 lr: 0.02
2023-03-01 13:02:43 iteration: 70300 loss: 0.0034 lr: 0.02
2023-03-01 13:02:52 iteration: 70400 loss: 0.0033 lr: 0.02
2023-03-01 13:03:01 iteration: 70500 loss: 0.0033 lr: 0.02
2023-03-01 13:03:10 iteration: 70600 loss: 0.0034 lr: 0.02
2023-03-01 13:03:19 iteration: 70700 loss: 0.0036 lr: 0.02
2023-03-01 13:03:28 iteration: 70800 loss: 0.0036 lr: 0.02
2023-03-01 13:03:36 iteration: 70900 loss: 0.0042 lr: 0.02
2023-03-01 13:03:45 iteration: 71000 loss: 0.0039 lr: 0.02
2023-03-01 13:03:53 iteration: 71100 loss: 0.0033 lr: 0.02
2023-03-01 13:04:02 iteration: 71200 loss: 0.0035 lr: 0.02
2023-03-01 13:04:10 iteration: 71300 loss: 0.0035 lr: 0.02
2023-03-01 13:04:19 iteration: 71400 loss: 0.0034 lr: 0.02
2023-03-01 13:04:28 iteration: 71500 loss: 0.0037 lr: 0.02
2023-03-01 13:04:36 iteration: 71600 loss: 0.0036 lr: 0.02
2023-03-01 13:04:45 iteration: 71700 loss: 0.0037 lr: 0.02
2023-03-01 13:04:53 iteration: 71800 loss: 0.0037 lr: 0.02
2023-03-01 13:05:02 iteration: 71900 loss: 0.0035 lr: 0.02
2023-03-01 13:05:10 iteration: 72000 loss: 0.0033 lr: 0.02
2023-03-01 13:05:19 iteration: 72100 loss: 0.0035 lr: 0.02
2023-03-01 13:05:28 iteration: 72200 loss: 0.0037 lr: 0.02
2023-03-01 13:05:36 iteration: 72300 loss: 0.0039 lr: 0.02
2023-03-01 13:05:45 iteration: 72400 loss: 0.0038 lr: 0.02
2023-03-01 13:05:54 iteration: 72500 loss: 0.0034 lr: 0.02
2023-03-01 13:06:02 iteration: 72600 loss: 0.0036 lr: 0.02
2023-03-01 13:06:10 iteration: 72700 loss: 0.0042 lr: 0.02
2023-03-01 13:06:19 iteration: 72800 loss: 0.0036 lr: 0.02
2023-03-01 13:06:27 iteration: 72900 loss: 0.0037 lr: 0.02
2023-03-01 13:06:38 iteration: 73000 loss: 0.0036 lr: 0.02
2023-03-01 13:06:47 iteration: 73100 loss: 0.0034 lr: 0.02
2023-03-01 13:06:56 iteration: 73200 loss: 0.0031 lr: 0.02
2023-03-01 13:07:05 iteration: 73300 loss: 0.0037 lr: 0.02
2023-03-01 13:07:15 iteration: 73400 loss: 0.0037 lr: 0.02
2023-03-01 13:07:23 iteration: 73500 loss: 0.0036 lr: 0.02
2023-03-01 13:07:32 iteration: 73600 loss: 0.0031 lr: 0.02
2023-03-01 13:07:40 iteration: 73700 loss: 0.0031 lr: 0.02
2023-03-01 13:07:48 iteration: 73800 loss: 0.0034 lr: 0.02
2023-03-01 13:07:57 iteration: 73900 loss: 0.0035 lr: 0.02
2023-03-01 13:08:06 iteration: 74000 loss: 0.0032 lr: 0.02
2023-03-01 13:08:14 iteration: 74100 loss: 0.0034 lr: 0.02
2023-03-01 13:08:22 iteration: 74200 loss: 0.0037 lr: 0.02
2023-03-01 13:08:31 iteration: 74300 loss: 0.0034 lr: 0.02
2023-03-01 13:08:40 iteration: 74400 loss: 0.0035 lr: 0.02
2023-03-01 13:08:48 iteration: 74500 loss: 0.0036 lr: 0.02
2023-03-01 13:08:57 iteration: 74600 loss: 0.0036 lr: 0.02
2023-03-01 13:09:05 iteration: 74700 loss: 0.0038 lr: 0.02
2023-03-01 13:09:14 iteration: 74800 loss: 0.0034 lr: 0.02
2023-03-01 13:09:22 iteration: 74900 loss: 0.0033 lr: 0.02
2023-03-01 13:09:30 iteration: 75000 loss: 0.0036 lr: 0.02
2023-03-01 13:09:39 iteration: 75100 loss: 0.0036 lr: 0.02
2023-03-01 13:09:49 iteration: 75200 loss: 0.0037 lr: 0.02
2023-03-01 13:09:58 iteration: 75300 loss: 0.0034 lr: 0.02
2023-03-01 13:10:07 iteration: 75400 loss: 0.0036 lr: 0.02
2023-03-01 13:10:15 iteration: 75500 loss: 0.0036 lr: 0.02
2023-03-01 13:10:25 iteration: 75600 loss: 0.0032 lr: 0.02
2023-03-01 13:10:34 iteration: 75700 loss: 0.0036 lr: 0.02
2023-03-01 13:10:42 iteration: 75800 loss: 0.0035 lr: 0.02
2023-03-01 13:10:52 iteration: 75900 loss: 0.0034 lr: 0.02
2023-03-01 13:11:00 iteration: 76000 loss: 0.0032 lr: 0.02
2023-03-01 13:11:09 iteration: 76100 loss: 0.0034 lr: 0.02
2023-03-01 13:11:19 iteration: 76200 loss: 0.0031 lr: 0.02
2023-03-01 13:11:28 iteration: 76300 loss: 0.0035 lr: 0.02
2023-03-01 13:11:37 iteration: 76400 loss: 0.0035 lr: 0.02
2023-03-01 13:11:46 iteration: 76500 loss: 0.0035 lr: 0.02
2023-03-01 13:11:54 iteration: 76600 loss: 0.0034 lr: 0.02
2023-03-01 13:12:03 iteration: 76700 loss: 0.0035 lr: 0.02
2023-03-01 13:12:12 iteration: 76800 loss: 0.0035 lr: 0.02
2023-03-01 13:12:22 iteration: 76900 loss: 0.0034 lr: 0.02
2023-03-01 13:12:31 iteration: 77000 loss: 0.0035 lr: 0.02
2023-03-01 13:12:40 iteration: 77100 loss: 0.0036 lr: 0.02
2023-03-01 13:12:48 iteration: 77200 loss: 0.0035 lr: 0.02
2023-03-01 13:12:57 iteration: 77300 loss: 0.0034 lr: 0.02
2023-03-01 13:13:05 iteration: 77400 loss: 0.0031 lr: 0.02
2023-03-01 13:13:15 iteration: 77500 loss: 0.0035 lr: 0.02
2023-03-01 13:13:24 iteration: 77600 loss: 0.0036 lr: 0.02
2023-03-01 13:13:32 iteration: 77700 loss: 0.0032 lr: 0.02
2023-03-01 13:13:42 iteration: 77800 loss: 0.0035 lr: 0.02
2023-03-01 13:13:50 iteration: 77900 loss: 0.0033 lr: 0.02
2023-03-01 13:13:59 iteration: 78000 loss: 0.0035 lr: 0.02
2023-03-01 13:14:08 iteration: 78100 loss: 0.0033 lr: 0.02
2023-03-01 13:14:17 iteration: 78200 loss: 0.0035 lr: 0.02
2023-03-01 13:14:25 iteration: 78300 loss: 0.0035 lr: 0.02
2023-03-01 13:14:34 iteration: 78400 loss: 0.0030 lr: 0.02
2023-03-01 13:14:42 iteration: 78500 loss: 0.0038 lr: 0.02
2023-03-01 13:14:51 iteration: 78600 loss: 0.0031 lr: 0.02
2023-03-01 13:15:00 iteration: 78700 loss: 0.0034 lr: 0.02
2023-03-01 13:15:08 iteration: 78800 loss: 0.0031 lr: 0.02
2023-03-01 13:15:18 iteration: 78900 loss: 0.0034 lr: 0.02
2023-03-01 13:15:26 iteration: 79000 loss: 0.0032 lr: 0.02
2023-03-01 13:15:36 iteration: 79100 loss: 0.0033 lr: 0.02
2023-03-01 13:15:46 iteration: 79200 loss: 0.0033 lr: 0.02
2023-03-01 13:15:54 iteration: 79300 loss: 0.0034 lr: 0.02
2023-03-01 13:16:03 iteration: 79400 loss: 0.0034 lr: 0.02
2023-03-01 13:16:12 iteration: 79500 loss: 0.0035 lr: 0.02
2023-03-01 13:16:20 iteration: 79600 loss: 0.0035 lr: 0.02
2023-03-01 13:16:28 iteration: 79700 loss: 0.0034 lr: 0.02
2023-03-01 13:16:37 iteration: 79800 loss: 0.0033 lr: 0.02
2023-03-01 13:16:48 iteration: 79900 loss: 0.0032 lr: 0.02
2023-03-01 13:16:57 iteration: 80000 loss: 0.0035 lr: 0.02
2023-03-01 13:17:06 iteration: 80100 loss: 0.0032 lr: 0.02
2023-03-01 13:17:14 iteration: 80200 loss: 0.0033 lr: 0.02
2023-03-01 13:17:23 iteration: 80300 loss: 0.0031 lr: 0.02
2023-03-01 13:17:31 iteration: 80400 loss: 0.0033 lr: 0.02
2023-03-01 13:17:39 iteration: 80500 loss: 0.0033 lr: 0.02
2023-03-01 13:17:47 iteration: 80600 loss: 0.0035 lr: 0.02
2023-03-01 13:17:55 iteration: 80700 loss: 0.0031 lr: 0.02
2023-03-01 13:18:05 iteration: 80800 loss: 0.0034 lr: 0.02
2023-03-01 13:18:14 iteration: 80900 loss: 0.0031 lr: 0.02
2023-03-01 13:18:24 iteration: 81000 loss: 0.0033 lr: 0.02
2023-03-01 13:18:33 iteration: 81100 loss: 0.0036 lr: 0.02
2023-03-01 13:18:41 iteration: 81200 loss: 0.0035 lr: 0.02
2023-03-01 13:18:49 iteration: 81300 loss: 0.0032 lr: 0.02
2023-03-01 13:18:58 iteration: 81400 loss: 0.0037 lr: 0.02
2023-03-01 13:19:06 iteration: 81500 loss: 0.0033 lr: 0.02
2023-03-01 13:19:16 iteration: 81600 loss: 0.0034 lr: 0.02
2023-03-01 13:19:25 iteration: 81700 loss: 0.0032 lr: 0.02
2023-03-01 13:19:34 iteration: 81800 loss: 0.0033 lr: 0.02
2023-03-01 13:19:42 iteration: 81900 loss: 0.0029 lr: 0.02
2023-03-01 13:19:50 iteration: 82000 loss: 0.0033 lr: 0.02
2023-03-01 13:19:59 iteration: 82100 loss: 0.0033 lr: 0.02
2023-03-01 13:20:08 iteration: 82200 loss: 0.0030 lr: 0.02
2023-03-01 13:20:17 iteration: 82300 loss: 0.0031 lr: 0.02
2023-03-01 13:20:25 iteration: 82400 loss: 0.0029 lr: 0.02
2023-03-01 13:20:34 iteration: 82500 loss: 0.0034 lr: 0.02
2023-03-01 13:20:42 iteration: 82600 loss: 0.0030 lr: 0.02
2023-03-01 13:20:52 iteration: 82700 loss: 0.0030 lr: 0.02
2023-03-01 13:21:00 iteration: 82800 loss: 0.0031 lr: 0.02
2023-03-01 13:21:09 iteration: 82900 loss: 0.0033 lr: 0.02
2023-03-01 13:21:18 iteration: 83000 loss: 0.0033 lr: 0.02
2023-03-01 13:21:26 iteration: 83100 loss: 0.0034 lr: 0.02
2023-03-01 13:21:34 iteration: 83200 loss: 0.0033 lr: 0.02
2023-03-01 13:21:42 iteration: 83300 loss: 0.0031 lr: 0.02
2023-03-01 13:21:51 iteration: 83400 loss: 0.0036 lr: 0.02
2023-03-01 13:22:00 iteration: 83500 loss: 0.0033 lr: 0.02
2023-03-01 13:22:09 iteration: 83600 loss: 0.0033 lr: 0.02
2023-03-01 13:22:17 iteration: 83700 loss: 0.0031 lr: 0.02
2023-03-01 13:22:26 iteration: 83800 loss: 0.0032 lr: 0.02
2023-03-01 13:22:34 iteration: 83900 loss: 0.0034 lr: 0.02
2023-03-01 13:22:43 iteration: 84000 loss: 0.0033 lr: 0.02
2023-03-01 13:22:51 iteration: 84100 loss: 0.0032 lr: 0.02
2023-03-01 13:23:00 iteration: 84200 loss: 0.0033 lr: 0.02
2023-03-01 13:23:08 iteration: 84300 loss: 0.0032 lr: 0.02
2023-03-01 13:23:16 iteration: 84400 loss: 0.0033 lr: 0.02
2023-03-01 13:23:25 iteration: 84500 loss: 0.0034 lr: 0.02
2023-03-01 13:23:34 iteration: 84600 loss: 0.0031 lr: 0.02
2023-03-01 13:23:43 iteration: 84700 loss: 0.0029 lr: 0.02
2023-03-01 13:23:52 iteration: 84800 loss: 0.0031 lr: 0.02
2023-03-01 13:24:01 iteration: 84900 loss: 0.0033 lr: 0.02
2023-03-01 13:24:09 iteration: 85000 loss: 0.0030 lr: 0.02
2023-03-01 13:24:18 iteration: 85100 loss: 0.0034 lr: 0.02
2023-03-01 13:24:27 iteration: 85200 loss: 0.0032 lr: 0.02
2023-03-01 13:24:36 iteration: 85300 loss: 0.0033 lr: 0.02
2023-03-01 13:24:44 iteration: 85400 loss: 0.0029 lr: 0.02
2023-03-01 13:24:53 iteration: 85500 loss: 0.0035 lr: 0.02
2023-03-01 13:25:01 iteration: 85600 loss: 0.0036 lr: 0.02
2023-03-01 13:25:10 iteration: 85700 loss: 0.0033 lr: 0.02
2023-03-01 13:25:19 iteration: 85800 loss: 0.0033 lr: 0.02
2023-03-01 13:25:29 iteration: 85900 loss: 0.0033 lr: 0.02
2023-03-01 13:25:37 iteration: 86000 loss: 0.0032 lr: 0.02
2023-03-01 13:25:46 iteration: 86100 loss: 0.0032 lr: 0.02
2023-03-01 13:25:54 iteration: 86200 loss: 0.0031 lr: 0.02
2023-03-01 13:26:03 iteration: 86300 loss: 0.0035 lr: 0.02
2023-03-01 13:26:12 iteration: 86400 loss: 0.0037 lr: 0.02
2023-03-01 13:26:21 iteration: 86500 loss: 0.0034 lr: 0.02
2023-03-01 13:26:29 iteration: 86600 loss: 0.0032 lr: 0.02
2023-03-01 13:26:38 iteration: 86700 loss: 0.0031 lr: 0.02
2023-03-01 13:26:46 iteration: 86800 loss: 0.0031 lr: 0.02
2023-03-01 13:26:55 iteration: 86900 loss: 0.0031 lr: 0.02
2023-03-01 13:27:04 iteration: 87000 loss: 0.0034 lr: 0.02
2023-03-01 13:27:13 iteration: 87100 loss: 0.0031 lr: 0.02
2023-03-01 13:27:21 iteration: 87200 loss: 0.0032 lr: 0.02
2023-03-01 13:27:31 iteration: 87300 loss: 0.0032 lr: 0.02
2023-03-01 13:27:39 iteration: 87400 loss: 0.0031 lr: 0.02
2023-03-01 13:27:48 iteration: 87500 loss: 0.0033 lr: 0.02
2023-03-01 13:27:57 iteration: 87600 loss: 0.0031 lr: 0.02
2023-03-01 13:28:05 iteration: 87700 loss: 0.0028 lr: 0.02
2023-03-01 13:28:14 iteration: 87800 loss: 0.0033 lr: 0.02
2023-03-01 13:28:22 iteration: 87900 loss: 0.0033 lr: 0.02
2023-03-01 13:28:32 iteration: 88000 loss: 0.0033 lr: 0.02
2023-03-01 13:28:40 iteration: 88100 loss: 0.0031 lr: 0.02
2023-03-01 13:28:50 iteration: 88200 loss: 0.0031 lr: 0.02
2023-03-01 13:28:59 iteration: 88300 loss: 0.0034 lr: 0.02
2023-03-01 13:29:08 iteration: 88400 loss: 0.0031 lr: 0.02
2023-03-01 13:29:16 iteration: 88500 loss: 0.0033 lr: 0.02
2023-03-01 13:29:25 iteration: 88600 loss: 0.0035 lr: 0.02
2023-03-01 13:29:33 iteration: 88700 loss: 0.0030 lr: 0.02
2023-03-01 13:29:42 iteration: 88800 loss: 0.0032 lr: 0.02
2023-03-01 13:29:50 iteration: 88900 loss: 0.0033 lr: 0.02
2023-03-01 13:29:59 iteration: 89000 loss: 0.0030 lr: 0.02
2023-03-01 13:30:07 iteration: 89100 loss: 0.0031 lr: 0.02
2023-03-01 13:30:16 iteration: 89200 loss: 0.0030 lr: 0.02
2023-03-01 13:30:24 iteration: 89300 loss: 0.0030 lr: 0.02
2023-03-01 13:30:33 iteration: 89400 loss: 0.0031 lr: 0.02
2023-03-01 13:30:41 iteration: 89500 loss: 0.0031 lr: 0.02
2023-03-01 13:30:50 iteration: 89600 loss: 0.0030 lr: 0.02
2023-03-01 13:30:58 iteration: 89700 loss: 0.0033 lr: 0.02
2023-03-01 13:31:06 iteration: 89800 loss: 0.0033 lr: 0.02
2023-03-01 13:31:15 iteration: 89900 loss: 0.0030 lr: 0.02
2023-03-01 13:31:24 iteration: 90000 loss: 0.0035 lr: 0.02
2023-03-01 13:31:32 iteration: 90100 loss: 0.0029 lr: 0.02
2023-03-01 13:31:40 iteration: 90200 loss: 0.0028 lr: 0.02
2023-03-01 13:31:49 iteration: 90300 loss: 0.0032 lr: 0.02
2023-03-01 13:31:57 iteration: 90400 loss: 0.0031 lr: 0.02
2023-03-01 13:32:07 iteration: 90500 loss: 0.0030 lr: 0.02
2023-03-01 13:32:15 iteration: 90600 loss: 0.0030 lr: 0.02
2023-03-01 13:32:24 iteration: 90700 loss: 0.0031 lr: 0.02
2023-03-01 13:32:33 iteration: 90800 loss: 0.0030 lr: 0.02
2023-03-01 13:32:42 iteration: 90900 loss: 0.0032 lr: 0.02
2023-03-01 13:32:51 iteration: 91000 loss: 0.0030 lr: 0.02
2023-03-01 13:33:00 iteration: 91100 loss: 0.0031 lr: 0.02
2023-03-01 13:33:09 iteration: 91200 loss: 0.0031 lr: 0.02
2023-03-01 13:33:17 iteration: 91300 loss: 0.0029 lr: 0.02
2023-03-01 13:33:26 iteration: 91400 loss: 0.0033 lr: 0.02
2023-03-01 13:33:34 iteration: 91500 loss: 0.0031 lr: 0.02
2023-03-01 13:33:43 iteration: 91600 loss: 0.0029 lr: 0.02
2023-03-01 13:33:51 iteration: 91700 loss: 0.0030 lr: 0.02
2023-03-01 13:34:00 iteration: 91800 loss: 0.0029 lr: 0.02
2023-03-01 13:34:08 iteration: 91900 loss: 0.0029 lr: 0.02
2023-03-01 13:34:18 iteration: 92000 loss: 0.0031 lr: 0.02
2023-03-01 13:34:28 iteration: 92100 loss: 0.0030 lr: 0.02
2023-03-01 13:34:36 iteration: 92200 loss: 0.0029 lr: 0.02
2023-03-01 13:34:45 iteration: 92300 loss: 0.0029 lr: 0.02
2023-03-01 13:34:53 iteration: 92400 loss: 0.0029 lr: 0.02
2023-03-01 13:35:02 iteration: 92500 loss: 0.0029 lr: 0.02
2023-03-01 13:35:11 iteration: 92600 loss: 0.0029 lr: 0.02
2023-03-01 13:35:19 iteration: 92700 loss: 0.0033 lr: 0.02
2023-03-01 13:35:28 iteration: 92800 loss: 0.0032 lr: 0.02
2023-03-01 13:35:37 iteration: 92900 loss: 0.0029 lr: 0.02
2023-03-01 13:35:45 iteration: 93000 loss: 0.0033 lr: 0.02
2023-03-01 13:35:54 iteration: 93100 loss: 0.0028 lr: 0.02
2023-03-01 13:36:03 iteration: 93200 loss: 0.0029 lr: 0.02
2023-03-01 13:36:12 iteration: 93300 loss: 0.0032 lr: 0.02
2023-03-01 13:36:20 iteration: 93400 loss: 0.0028 lr: 0.02
2023-03-01 13:36:29 iteration: 93500 loss: 0.0028 lr: 0.02
2023-03-01 13:36:37 iteration: 93600 loss: 0.0030 lr: 0.02
2023-03-01 13:36:45 iteration: 93700 loss: 0.0030 lr: 0.02
2023-03-01 13:36:54 iteration: 93800 loss: 0.0030 lr: 0.02
2023-03-01 13:37:03 iteration: 93900 loss: 0.0031 lr: 0.02
2023-03-01 13:37:12 iteration: 94000 loss: 0.0030 lr: 0.02
2023-03-01 13:37:21 iteration: 94100 loss: 0.0032 lr: 0.02
2023-03-01 13:37:30 iteration: 94200 loss: 0.0031 lr: 0.02
2023-03-01 13:37:39 iteration: 94300 loss: 0.0033 lr: 0.02
2023-03-01 13:37:47 iteration: 94400 loss: 0.0031 lr: 0.02
2023-03-01 13:37:57 iteration: 94500 loss: 0.0030 lr: 0.02
2023-03-01 13:38:05 iteration: 94600 loss: 0.0030 lr: 0.02
2023-03-01 13:38:14 iteration: 94700 loss: 0.0031 lr: 0.02
2023-03-01 13:38:22 iteration: 94800 loss: 0.0033 lr: 0.02
2023-03-01 13:38:31 iteration: 94900 loss: 0.0032 lr: 0.02
2023-03-01 13:38:40 iteration: 95000 loss: 0.0033 lr: 0.02
2023-03-01 13:38:49 iteration: 95100 loss: 0.0029 lr: 0.02
2023-03-01 13:38:58 iteration: 95200 loss: 0.0031 lr: 0.02
2023-03-01 13:39:06 iteration: 95300 loss: 0.0029 lr: 0.02
2023-03-01 13:39:15 iteration: 95400 loss: 0.0030 lr: 0.02
2023-03-01 13:39:25 iteration: 95500 loss: 0.0034 lr: 0.02
2023-03-01 13:39:34 iteration: 95600 loss: 0.0029 lr: 0.02
2023-03-01 13:39:47 iteration: 95700 loss: 0.0033 lr: 0.02
2023-03-01 13:39:56 iteration: 95800 loss: 0.0029 lr: 0.02
2023-03-01 13:40:05 iteration: 95900 loss: 0.0032 lr: 0.02
2023-03-01 13:40:14 iteration: 96000 loss: 0.0028 lr: 0.02
2023-03-01 13:40:22 iteration: 96100 loss: 0.0030 lr: 0.02
2023-03-01 13:40:31 iteration: 96200 loss: 0.0029 lr: 0.02
2023-03-01 13:40:39 iteration: 96300 loss: 0.0031 lr: 0.02
2023-03-01 13:40:49 iteration: 96400 loss: 0.0031 lr: 0.02
2023-03-01 13:40:57 iteration: 96500 loss: 0.0031 lr: 0.02
2023-03-01 13:41:06 iteration: 96600 loss: 0.0030 lr: 0.02
2023-03-01 13:41:15 iteration: 96700 loss: 0.0030 lr: 0.02
2023-03-01 13:41:23 iteration: 96800 loss: 0.0029 lr: 0.02
2023-03-01 13:41:33 iteration: 96900 loss: 0.0031 lr: 0.02
2023-03-01 13:41:43 iteration: 97000 loss: 0.0028 lr: 0.02
2023-03-01 13:41:51 iteration: 97100 loss: 0.0028 lr: 0.02
2023-03-01 13:42:00 iteration: 97200 loss: 0.0030 lr: 0.02
2023-03-01 13:42:08 iteration: 97300 loss: 0.0029 lr: 0.02
2023-03-01 13:42:17 iteration: 97400 loss: 0.0032 lr: 0.02
2023-03-01 13:42:25 iteration: 97500 loss: 0.0027 lr: 0.02
2023-03-01 13:42:34 iteration: 97600 loss: 0.0028 lr: 0.02
2023-03-01 13:42:44 iteration: 97700 loss: 0.0030 lr: 0.02
2023-03-01 13:42:52 iteration: 97800 loss: 0.0031 lr: 0.02
2023-03-01 13:43:01 iteration: 97900 loss: 0.0030 lr: 0.02
2023-03-01 13:43:09 iteration: 98000 loss: 0.0030 lr: 0.02
2023-03-01 13:43:17 iteration: 98100 loss: 0.0030 lr: 0.02
2023-03-01 13:43:26 iteration: 98200 loss: 0.0028 lr: 0.02
2023-03-01 13:43:34 iteration: 98300 loss: 0.0032 lr: 0.02
2023-03-01 13:43:43 iteration: 98400 loss: 0.0031 lr: 0.02
2023-03-01 13:43:51 iteration: 98500 loss: 0.0031 lr: 0.02
2023-03-01 13:44:00 iteration: 98600 loss: 0.0030 lr: 0.02
2023-03-01 13:44:08 iteration: 98700 loss: 0.0029 lr: 0.02
2023-03-01 13:44:17 iteration: 98800 loss: 0.0030 lr: 0.02
2023-03-01 13:44:26 iteration: 98900 loss: 0.0030 lr: 0.02
2023-03-01 13:44:35 iteration: 99000 loss: 0.0030 lr: 0.02
2023-03-01 13:44:43 iteration: 99100 loss: 0.0033 lr: 0.02
2023-03-01 13:44:52 iteration: 99200 loss: 0.0028 lr: 0.02
2023-03-01 13:45:01 iteration: 99300 loss: 0.0032 lr: 0.02
2023-03-01 13:45:09 iteration: 99400 loss: 0.0031 lr: 0.02
2023-03-01 13:45:18 iteration: 99500 loss: 0.0030 lr: 0.02
2023-03-01 13:45:27 iteration: 99600 loss: 0.0031 lr: 0.02
2023-03-01 13:45:37 iteration: 99700 loss: 0.0031 lr: 0.02
2023-03-01 13:45:46 iteration: 99800 loss: 0.0029 lr: 0.02
2023-03-01 13:45:54 iteration: 99900 loss: 0.0030 lr: 0.02
2023-03-01 13:46:03 iteration: 100000 loss: 0.0033 lr: 0.02
2023-03-01 13:46:12 iteration: 100100 loss: 0.0031 lr: 0.02
2023-03-01 13:46:21 iteration: 100200 loss: 0.0029 lr: 0.02
2023-03-01 13:46:29 iteration: 100300 loss: 0.0029 lr: 0.02
2023-03-01 13:46:37 iteration: 100400 loss: 0.0031 lr: 0.02
2023-03-01 13:46:46 iteration: 100500 loss: 0.0032 lr: 0.02
2023-03-01 13:46:54 iteration: 100600 loss: 0.0026 lr: 0.02
2023-03-01 13:47:02 iteration: 100700 loss: 0.0029 lr: 0.02
2023-03-01 13:47:11 iteration: 100800 loss: 0.0033 lr: 0.02
2023-03-01 13:47:20 iteration: 100900 loss: 0.0029 lr: 0.02
2023-03-01 13:47:30 iteration: 101000 loss: 0.0028 lr: 0.02
2023-03-01 13:47:38 iteration: 101100 loss: 0.0028 lr: 0.02
2023-03-01 13:47:50 iteration: 101200 loss: 0.0033 lr: 0.02
2023-03-01 13:47:59 iteration: 101300 loss: 0.0030 lr: 0.02
2023-03-01 13:48:07 iteration: 101400 loss: 0.0033 lr: 0.02
2023-03-01 13:48:15 iteration: 101500 loss: 0.0029 lr: 0.02
2023-03-01 13:48:23 iteration: 101600 loss: 0.0028 lr: 0.02
2023-03-01 13:48:31 iteration: 101700 loss: 0.0033 lr: 0.02
2023-03-01 13:48:40 iteration: 101800 loss: 0.0029 lr: 0.02
2023-03-01 13:48:48 iteration: 101900 loss: 0.0031 lr: 0.02
2023-03-01 13:48:57 iteration: 102000 loss: 0.0029 lr: 0.02
2023-03-01 13:49:05 iteration: 102100 loss: 0.0029 lr: 0.02
2023-03-01 13:49:14 iteration: 102200 loss: 0.0030 lr: 0.02
2023-03-01 13:49:22 iteration: 102300 loss: 0.0029 lr: 0.02
2023-03-01 13:49:31 iteration: 102400 loss: 0.0031 lr: 0.02
2023-03-01 13:49:39 iteration: 102500 loss: 0.0028 lr: 0.02
2023-03-01 13:49:48 iteration: 102600 loss: 0.0030 lr: 0.02
2023-03-01 13:49:55 iteration: 102700 loss: 0.0027 lr: 0.02
2023-03-01 13:50:04 iteration: 102800 loss: 0.0033 lr: 0.02
2023-03-01 13:50:12 iteration: 102900 loss: 0.0028 lr: 0.02
2023-03-01 13:50:21 iteration: 103000 loss: 0.0028 lr: 0.02
2023-03-01 13:50:30 iteration: 103100 loss: 0.0029 lr: 0.02
2023-03-01 13:50:38 iteration: 103200 loss: 0.0028 lr: 0.02
2023-03-01 13:50:46 iteration: 103300 loss: 0.0029 lr: 0.02
2023-03-01 13:50:55 iteration: 103400 loss: 0.0031 lr: 0.02
2023-03-01 13:51:05 iteration: 103500 loss: 0.0029 lr: 0.02
2023-03-01 13:51:15 iteration: 103600 loss: 0.0027 lr: 0.02
2023-03-01 13:51:23 iteration: 103700 loss: 0.0030 lr: 0.02
2023-03-01 13:51:31 iteration: 103800 loss: 0.0029 lr: 0.02
2023-03-01 13:51:40 iteration: 103900 loss: 0.0032 lr: 0.02
2023-03-01 13:51:48 iteration: 104000 loss: 0.0028 lr: 0.02
2023-03-01 13:51:57 iteration: 104100 loss: 0.0029 lr: 0.02
2023-03-01 13:52:05 iteration: 104200 loss: 0.0030 lr: 0.02
2023-03-01 13:52:14 iteration: 104300 loss: 0.0029 lr: 0.02
2023-03-01 13:52:23 iteration: 104400 loss: 0.0029 lr: 0.02
2023-03-01 13:52:31 iteration: 104500 loss: 0.0030 lr: 0.02
2023-03-01 13:52:40 iteration: 104600 loss: 0.0028 lr: 0.02
2023-03-01 13:52:48 iteration: 104700 loss: 0.0030 lr: 0.02
2023-03-01 13:52:56 iteration: 104800 loss: 0.0031 lr: 0.02
2023-03-01 13:53:05 iteration: 104900 loss: 0.0029 lr: 0.02
2023-03-01 13:53:13 iteration: 105000 loss: 0.0027 lr: 0.02
2023-03-01 13:53:21 iteration: 105100 loss: 0.0030 lr: 0.02
2023-03-01 13:53:29 iteration: 105200 loss: 0.0029 lr: 0.02
2023-03-01 13:53:38 iteration: 105300 loss: 0.0029 lr: 0.02
2023-03-01 13:53:47 iteration: 105400 loss: 0.0027 lr: 0.02
2023-03-01 13:53:55 iteration: 105500 loss: 0.0029 lr: 0.02
2023-03-01 13:54:05 iteration: 105600 loss: 0.0028 lr: 0.02
2023-03-01 13:54:13 iteration: 105700 loss: 0.0029 lr: 0.02
2023-03-01 13:54:22 iteration: 105800 loss: 0.0032 lr: 0.02
2023-03-01 13:54:32 iteration: 105900 loss: 0.0029 lr: 0.02
2023-03-01 13:54:41 iteration: 106000 loss: 0.0027 lr: 0.02
2023-03-01 13:54:49 iteration: 106100 loss: 0.0029 lr: 0.02
2023-03-01 13:54:57 iteration: 106200 loss: 0.0027 lr: 0.02
2023-03-01 13:55:06 iteration: 106300 loss: 0.0027 lr: 0.02
2023-03-01 13:55:14 iteration: 106400 loss: 0.0028 lr: 0.02
2023-03-01 13:55:24 iteration: 106500 loss: 0.0030 lr: 0.02
2023-03-01 13:55:32 iteration: 106600 loss: 0.0028 lr: 0.02
2023-03-01 13:55:41 iteration: 106700 loss: 0.0029 lr: 0.02
2023-03-01 13:55:51 iteration: 106800 loss: 0.0032 lr: 0.02
2023-03-01 13:56:01 iteration: 106900 loss: 0.0029 lr: 0.02
2023-03-01 13:56:09 iteration: 107000 loss: 0.0030 lr: 0.02
2023-03-01 13:56:18 iteration: 107100 loss: 0.0028 lr: 0.02
2023-03-01 13:56:26 iteration: 107200 loss: 0.0028 lr: 0.02
2023-03-01 13:56:35 iteration: 107300 loss: 0.0030 lr: 0.02
2023-03-01 13:56:43 iteration: 107400 loss: 0.0032 lr: 0.02
2023-03-01 13:56:53 iteration: 107500 loss: 0.0030 lr: 0.02
2023-03-01 13:57:02 iteration: 107600 loss: 0.0027 lr: 0.02
2023-03-01 13:57:11 iteration: 107700 loss: 0.0028 lr: 0.02
2023-03-01 13:57:20 iteration: 107800 loss: 0.0031 lr: 0.02
2023-03-01 13:57:28 iteration: 107900 loss: 0.0029 lr: 0.02
2023-03-01 13:57:38 iteration: 108000 loss: 0.0030 lr: 0.02
2023-03-01 13:57:47 iteration: 108100 loss: 0.0029 lr: 0.02
2023-03-01 13:57:56 iteration: 108200 loss: 0.0028 lr: 0.02
2023-03-01 13:58:04 iteration: 108300 loss: 0.0028 lr: 0.02
2023-03-01 13:58:13 iteration: 108400 loss: 0.0032 lr: 0.02
2023-03-01 13:58:21 iteration: 108500 loss: 0.0031 lr: 0.02
2023-03-01 13:58:30 iteration: 108600 loss: 0.0029 lr: 0.02
2023-03-01 13:58:38 iteration: 108700 loss: 0.0029 lr: 0.02
2023-03-01 13:58:46 iteration: 108800 loss: 0.0032 lr: 0.02
2023-03-01 13:58:55 iteration: 108900 loss: 0.0029 lr: 0.02
2023-03-01 13:59:04 iteration: 109000 loss: 0.0029 lr: 0.02
2023-03-01 13:59:13 iteration: 109100 loss: 0.0029 lr: 0.02
2023-03-01 13:59:21 iteration: 109200 loss: 0.0029 lr: 0.02
2023-03-01 13:59:29 iteration: 109300 loss: 0.0027 lr: 0.02
2023-03-01 13:59:40 iteration: 109400 loss: 0.0028 lr: 0.02
2023-03-01 13:59:47 iteration: 109500 loss: 0.0028 lr: 0.02
2023-03-01 13:59:57 iteration: 109600 loss: 0.0027 lr: 0.02
2023-03-01 14:00:05 iteration: 109700 loss: 0.0028 lr: 0.02
2023-03-01 14:00:13 iteration: 109800 loss: 0.0028 lr: 0.02
2023-03-01 14:00:21 iteration: 109900 loss: 0.0030 lr: 0.02
2023-03-01 14:00:29 iteration: 110000 loss: 0.0029 lr: 0.02
2023-03-01 14:00:37 iteration: 110100 loss: 0.0027 lr: 0.02
2023-03-01 14:00:45 iteration: 110200 loss: 0.0030 lr: 0.02
2023-03-01 14:00:54 iteration: 110300 loss: 0.0030 lr: 0.02
2023-03-01 14:01:03 iteration: 110400 loss: 0.0033 lr: 0.02
2023-03-01 14:01:11 iteration: 110500 loss: 0.0027 lr: 0.02
2023-03-01 14:01:19 iteration: 110600 loss: 0.0030 lr: 0.02
2023-03-01 14:01:27 iteration: 110700 loss: 0.0029 lr: 0.02
2023-03-01 14:01:36 iteration: 110800 loss: 0.0030 lr: 0.02
2023-03-01 14:01:44 iteration: 110900 loss: 0.0032 lr: 0.02
2023-03-01 14:01:53 iteration: 111000 loss: 0.0028 lr: 0.02
2023-03-01 14:02:01 iteration: 111100 loss: 0.0027 lr: 0.02
2023-03-01 14:02:10 iteration: 111200 loss: 0.0027 lr: 0.02
2023-03-01 14:02:18 iteration: 111300 loss: 0.0028 lr: 0.02
2023-03-01 14:02:27 iteration: 111400 loss: 0.0028 lr: 0.02
2023-03-01 14:02:36 iteration: 111500 loss: 0.0034 lr: 0.02
2023-03-01 14:02:44 iteration: 111600 loss: 0.0029 lr: 0.02
2023-03-01 14:02:53 iteration: 111700 loss: 0.0028 lr: 0.02
2023-03-01 14:03:02 iteration: 111800 loss: 0.0028 lr: 0.02
2023-03-01 14:03:12 iteration: 111900 loss: 0.0028 lr: 0.02
2023-03-01 14:03:21 iteration: 112000 loss: 0.0030 lr: 0.02
2023-03-01 14:03:29 iteration: 112100 loss: 0.0028 lr: 0.02
2023-03-01 14:03:38 iteration: 112200 loss: 0.0027 lr: 0.02
2023-03-01 14:03:46 iteration: 112300 loss: 0.0029 lr: 0.02
2023-03-01 14:03:55 iteration: 112400 loss: 0.0030 lr: 0.02
2023-03-01 14:04:03 iteration: 112500 loss: 0.0031 lr: 0.02
2023-03-01 14:04:12 iteration: 112600 loss: 0.0029 lr: 0.02
2023-03-01 14:04:19 iteration: 112700 loss: 0.0026 lr: 0.02
2023-03-01 14:04:28 iteration: 112800 loss: 0.0027 lr: 0.02
2023-03-01 14:04:35 iteration: 112900 loss: 0.0025 lr: 0.02
2023-03-01 14:04:44 iteration: 113000 loss: 0.0031 lr: 0.02
2023-03-01 14:04:52 iteration: 113100 loss: 0.0030 lr: 0.02
2023-03-01 14:05:00 iteration: 113200 loss: 0.0027 lr: 0.02
2023-03-01 14:05:09 iteration: 113300 loss: 0.0028 lr: 0.02
2023-03-01 14:05:17 iteration: 113400 loss: 0.0028 lr: 0.02
2023-03-01 14:05:25 iteration: 113500 loss: 0.0029 lr: 0.02
2023-03-01 14:05:34 iteration: 113600 loss: 0.0031 lr: 0.02
2023-03-01 14:05:41 iteration: 113700 loss: 0.0029 lr: 0.02
2023-03-01 14:05:49 iteration: 113800 loss: 0.0029 lr: 0.02
2023-03-01 14:05:57 iteration: 113900 loss: 0.0027 lr: 0.02
2023-03-01 14:06:07 iteration: 114000 loss: 0.0028 lr: 0.02
2023-03-01 14:06:16 iteration: 114100 loss: 0.0031 lr: 0.02
2023-03-01 14:06:24 iteration: 114200 loss: 0.0028 lr: 0.02
2023-03-01 14:06:33 iteration: 114300 loss: 0.0027 lr: 0.02
2023-03-01 14:06:41 iteration: 114400 loss: 0.0028 lr: 0.02
2023-03-01 14:06:49 iteration: 114500 loss: 0.0027 lr: 0.02
2023-03-01 14:06:58 iteration: 114600 loss: 0.0031 lr: 0.02
2023-03-01 14:07:06 iteration: 114700 loss: 0.0029 lr: 0.02
2023-03-01 14:07:14 iteration: 114800 loss: 0.0030 lr: 0.02
2023-03-01 14:07:24 iteration: 114900 loss: 0.0027 lr: 0.02
2023-03-01 14:07:33 iteration: 115000 loss: 0.0028 lr: 0.02
2023-03-01 14:07:43 iteration: 115100 loss: 0.0028 lr: 0.02
2023-03-01 14:07:51 iteration: 115200 loss: 0.0028 lr: 0.02
2023-03-01 14:08:00 iteration: 115300 loss: 0.0029 lr: 0.02
2023-03-01 14:08:08 iteration: 115400 loss: 0.0028 lr: 0.02
2023-03-01 14:08:16 iteration: 115500 loss: 0.0029 lr: 0.02
2023-03-01 14:08:25 iteration: 115600 loss: 0.0029 lr: 0.02
2023-03-01 14:08:34 iteration: 115700 loss: 0.0028 lr: 0.02
2023-03-01 14:08:44 iteration: 115800 loss: 0.0028 lr: 0.02
2023-03-01 14:08:52 iteration: 115900 loss: 0.0028 lr: 0.02
2023-03-01 14:09:01 iteration: 116000 loss: 0.0030 lr: 0.02
2023-03-01 14:09:11 iteration: 116100 loss: 0.0028 lr: 0.02
2023-03-01 14:09:20 iteration: 116200 loss: 0.0030 lr: 0.02
2023-03-01 14:09:27 iteration: 116300 loss: 0.0026 lr: 0.02
2023-03-01 14:09:36 iteration: 116400 loss: 0.0031 lr: 0.02
2023-03-01 14:09:45 iteration: 116500 loss: 0.0031 lr: 0.02
2023-03-01 14:09:53 iteration: 116600 loss: 0.0029 lr: 0.02
2023-03-01 14:10:01 iteration: 116700 loss: 0.0027 lr: 0.02
2023-03-01 14:10:10 iteration: 116800 loss: 0.0025 lr: 0.02
2023-03-01 14:10:18 iteration: 116900 loss: 0.0028 lr: 0.02
2023-03-01 14:10:26 iteration: 117000 loss: 0.0027 lr: 0.02
2023-03-01 14:10:34 iteration: 117100 loss: 0.0025 lr: 0.02
2023-03-01 14:10:43 iteration: 117200 loss: 0.0029 lr: 0.02
2023-03-01 14:10:51 iteration: 117300 loss: 0.0026 lr: 0.02
2023-03-01 14:11:00 iteration: 117400 loss: 0.0029 lr: 0.02
2023-03-01 14:11:10 iteration: 117500 loss: 0.0031 lr: 0.02
2023-03-01 14:11:17 iteration: 117600 loss: 0.0027 lr: 0.02
2023-03-01 14:11:26 iteration: 117700 loss: 0.0028 lr: 0.02
2023-03-01 14:11:34 iteration: 117800 loss: 0.0027 lr: 0.02
2023-03-01 14:11:42 iteration: 117900 loss: 0.0028 lr: 0.02
2023-03-01 14:11:50 iteration: 118000 loss: 0.0028 lr: 0.02
2023-03-01 14:11:58 iteration: 118100 loss: 0.0030 lr: 0.02
2023-03-01 14:12:07 iteration: 118200 loss: 0.0027 lr: 0.02
2023-03-01 14:12:15 iteration: 118300 loss: 0.0027 lr: 0.02
2023-03-01 14:12:23 iteration: 118400 loss: 0.0026 lr: 0.02
2023-03-01 14:12:33 iteration: 118500 loss: 0.0028 lr: 0.02
2023-03-01 14:12:43 iteration: 118600 loss: 0.0028 lr: 0.02
2023-03-01 14:12:51 iteration: 118700 loss: 0.0026 lr: 0.02
2023-03-01 14:12:59 iteration: 118800 loss: 0.0027 lr: 0.02
2023-03-01 14:13:07 iteration: 118900 loss: 0.0029 lr: 0.02
2023-03-01 14:13:15 iteration: 119000 loss: 0.0026 lr: 0.02
2023-03-01 14:13:24 iteration: 119100 loss: 0.0027 lr: 0.02
2023-03-01 14:13:33 iteration: 119200 loss: 0.0026 lr: 0.02
2023-03-01 14:13:41 iteration: 119300 loss: 0.0029 lr: 0.02
2023-03-01 14:13:49 iteration: 119400 loss: 0.0027 lr: 0.02
2023-03-01 14:13:58 iteration: 119500 loss: 0.0025 lr: 0.02
2023-03-01 14:14:06 iteration: 119600 loss: 0.0026 lr: 0.02
2023-03-01 14:14:14 iteration: 119700 loss: 0.0024 lr: 0.02
2023-03-01 14:14:23 iteration: 119800 loss: 0.0027 lr: 0.02
2023-03-01 14:14:32 iteration: 119900 loss: 0.0027 lr: 0.02
2023-03-01 14:14:40 iteration: 120000 loss: 0.0027 lr: 0.02
2023-03-01 14:14:49 iteration: 120100 loss: 0.0028 lr: 0.02
2023-03-01 14:14:57 iteration: 120200 loss: 0.0025 lr: 0.02
2023-03-01 14:15:06 iteration: 120300 loss: 0.0027 lr: 0.02
2023-03-01 14:15:14 iteration: 120400 loss: 0.0029 lr: 0.02
2023-03-01 14:15:23 iteration: 120500 loss: 0.0027 lr: 0.02
2023-03-01 14:15:32 iteration: 120600 loss: 0.0027 lr: 0.02
2023-03-01 14:15:41 iteration: 120700 loss: 0.0027 lr: 0.02
2023-03-01 14:15:51 iteration: 120800 loss: 0.0027 lr: 0.02
2023-03-01 14:15:59 iteration: 120900 loss: 0.0029 lr: 0.02
2023-03-01 14:16:07 iteration: 121000 loss: 0.0028 lr: 0.02
2023-03-01 14:16:17 iteration: 121100 loss: 0.0024 lr: 0.02
2023-03-01 14:16:25 iteration: 121200 loss: 0.0026 lr: 0.02
2023-03-01 14:16:34 iteration: 121300 loss: 0.0025 lr: 0.02
2023-03-01 14:16:43 iteration: 121400 loss: 0.0026 lr: 0.02
2023-03-01 14:16:53 iteration: 121500 loss: 0.0028 lr: 0.02
2023-03-01 14:17:01 iteration: 121600 loss: 0.0028 lr: 0.02
2023-03-01 14:17:11 iteration: 121700 loss: 0.0027 lr: 0.02
2023-03-01 14:17:19 iteration: 121800 loss: 0.0028 lr: 0.02
2023-03-01 14:17:27 iteration: 121900 loss: 0.0028 lr: 0.02
2023-03-01 14:17:36 iteration: 122000 loss: 0.0025 lr: 0.02
2023-03-01 14:17:44 iteration: 122100 loss: 0.0026 lr: 0.02
2023-03-01 14:17:52 iteration: 122200 loss: 0.0026 lr: 0.02
2023-03-01 14:18:00 iteration: 122300 loss: 0.0025 lr: 0.02
2023-03-01 14:18:08 iteration: 122400 loss: 0.0027 lr: 0.02
2023-03-01 14:18:17 iteration: 122500 loss: 0.0029 lr: 0.02
2023-03-01 14:18:27 iteration: 122600 loss: 0.0027 lr: 0.02
2023-03-01 14:18:35 iteration: 122700 loss: 0.0027 lr: 0.02
2023-03-01 14:18:43 iteration: 122800 loss: 0.0026 lr: 0.02
2023-03-01 14:18:51 iteration: 122900 loss: 0.0026 lr: 0.02
2023-03-01 14:18:59 iteration: 123000 loss: 0.0029 lr: 0.02
2023-03-01 14:19:08 iteration: 123100 loss: 0.0029 lr: 0.02
2023-03-01 14:19:17 iteration: 123200 loss: 0.0029 lr: 0.02
2023-03-01 14:19:27 iteration: 123300 loss: 0.0030 lr: 0.02
2023-03-01 14:19:36 iteration: 123400 loss: 0.0025 lr: 0.02
2023-03-01 14:19:44 iteration: 123500 loss: 0.0028 lr: 0.02
2023-03-01 14:19:52 iteration: 123600 loss: 0.0025 lr: 0.02
2023-03-01 14:20:00 iteration: 123700 loss: 0.0028 lr: 0.02
2023-03-01 14:20:09 iteration: 123800 loss: 0.0026 lr: 0.02
2023-03-01 14:20:17 iteration: 123900 loss: 0.0026 lr: 0.02
2023-03-01 14:20:25 iteration: 124000 loss: 0.0026 lr: 0.02
2023-03-01 14:20:34 iteration: 124100 loss: 0.0027 lr: 0.02
2023-03-01 14:20:44 iteration: 124200 loss: 0.0027 lr: 0.02
2023-03-01 14:20:52 iteration: 124300 loss: 0.0028 lr: 0.02
2023-03-01 14:21:00 iteration: 124400 loss: 0.0028 lr: 0.02
2023-03-01 14:21:09 iteration: 124500 loss: 0.0028 lr: 0.02
2023-03-01 14:21:17 iteration: 124600 loss: 0.0028 lr: 0.02
2023-03-01 14:21:27 iteration: 124700 loss: 0.0027 lr: 0.02
2023-03-01 14:21:36 iteration: 124800 loss: 0.0025 lr: 0.02
2023-03-01 14:21:44 iteration: 124900 loss: 0.0026 lr: 0.02
2023-03-01 14:21:54 iteration: 125000 loss: 0.0028 lr: 0.02
2023-03-01 14:22:03 iteration: 125100 loss: 0.0031 lr: 0.02
2023-03-01 14:22:11 iteration: 125200 loss: 0.0028 lr: 0.02
2023-03-01 14:22:21 iteration: 125300 loss: 0.0029 lr: 0.02
2023-03-01 14:22:29 iteration: 125400 loss: 0.0029 lr: 0.02
2023-03-01 14:22:38 iteration: 125500 loss: 0.0029 lr: 0.02
2023-03-01 14:22:46 iteration: 125600 loss: 0.0028 lr: 0.02
2023-03-01 14:22:55 iteration: 125700 loss: 0.0027 lr: 0.02
2023-03-01 14:23:03 iteration: 125800 loss: 0.0026 lr: 0.02
2023-03-01 14:23:13 iteration: 125900 loss: 0.0027 lr: 0.02
2023-03-01 14:23:21 iteration: 126000 loss: 0.0030 lr: 0.02
2023-03-01 14:23:30 iteration: 126100 loss: 0.0029 lr: 0.02
2023-03-01 14:23:38 iteration: 126200 loss: 0.0027 lr: 0.02
2023-03-01 14:23:47 iteration: 126300 loss: 0.0026 lr: 0.02
2023-03-01 14:23:56 iteration: 126400 loss: 0.0026 lr: 0.02
2023-03-01 14:24:04 iteration: 126500 loss: 0.0028 lr: 0.02
2023-03-01 14:24:14 iteration: 126600 loss: 0.0026 lr: 0.02
2023-03-01 14:24:22 iteration: 126700 loss: 0.0026 lr: 0.02
2023-03-01 14:24:31 iteration: 126800 loss: 0.0024 lr: 0.02
2023-03-01 14:24:38 iteration: 126900 loss: 0.0025 lr: 0.02
2023-03-01 14:24:46 iteration: 127000 loss: 0.0026 lr: 0.02
2023-03-01 14:24:55 iteration: 127100 loss: 0.0026 lr: 0.02
2023-03-01 14:25:03 iteration: 127200 loss: 0.0026 lr: 0.02
2023-03-01 14:25:11 iteration: 127300 loss: 0.0027 lr: 0.02
2023-03-01 14:25:21 iteration: 127400 loss: 0.0031 lr: 0.02
2023-03-01 14:25:29 iteration: 127500 loss: 0.0028 lr: 0.02
2023-03-01 14:25:38 iteration: 127600 loss: 0.0027 lr: 0.02
2023-03-01 14:25:47 iteration: 127700 loss: 0.0028 lr: 0.02
2023-03-01 14:25:55 iteration: 127800 loss: 0.0026 lr: 0.02
2023-03-01 14:26:03 iteration: 127900 loss: 0.0027 lr: 0.02
2023-03-01 14:26:11 iteration: 128000 loss: 0.0029 lr: 0.02
2023-03-01 14:26:20 iteration: 128100 loss: 0.0026 lr: 0.02
2023-03-01 14:26:28 iteration: 128200 loss: 0.0027 lr: 0.02
2023-03-01 14:26:36 iteration: 128300 loss: 0.0028 lr: 0.02
2023-03-01 14:26:45 iteration: 128400 loss: 0.0025 lr: 0.02
2023-03-01 14:26:53 iteration: 128500 loss: 0.0028 lr: 0.02
2023-03-01 14:27:01 iteration: 128600 loss: 0.0027 lr: 0.02
2023-03-01 14:27:11 iteration: 128700 loss: 0.0027 lr: 0.02
2023-03-01 14:27:20 iteration: 128800 loss: 0.0024 lr: 0.02
2023-03-01 14:27:28 iteration: 128900 loss: 0.0025 lr: 0.02
2023-03-01 14:27:38 iteration: 129000 loss: 0.0026 lr: 0.02
2023-03-01 14:27:46 iteration: 129100 loss: 0.0025 lr: 0.02
2023-03-01 14:27:55 iteration: 129200 loss: 0.0027 lr: 0.02
2023-03-01 14:28:03 iteration: 129300 loss: 0.0028 lr: 0.02
2023-03-01 14:28:11 iteration: 129400 loss: 0.0027 lr: 0.02
2023-03-01 14:28:19 iteration: 129500 loss: 0.0026 lr: 0.02
2023-03-01 14:28:28 iteration: 129600 loss: 0.0024 lr: 0.02
2023-03-01 14:28:36 iteration: 129700 loss: 0.0024 lr: 0.02
2023-03-01 14:28:44 iteration: 129800 loss: 0.0024 lr: 0.02
2023-03-01 14:28:52 iteration: 129900 loss: 0.0026 lr: 0.02
2023-03-01 14:29:01 iteration: 130000 loss: 0.0027 lr: 0.02
2023-03-01 14:29:10 iteration: 130100 loss: 0.0025 lr: 0.02
2023-03-01 14:29:19 iteration: 130200 loss: 0.0026 lr: 0.02
2023-03-01 14:29:27 iteration: 130300 loss: 0.0025 lr: 0.02
2023-03-01 14:29:36 iteration: 130400 loss: 0.0025 lr: 0.02
2023-03-01 14:29:44 iteration: 130500 loss: 0.0026 lr: 0.02
2023-03-01 14:29:53 iteration: 130600 loss: 0.0029 lr: 0.02
2023-03-01 14:30:01 iteration: 130700 loss: 0.0027 lr: 0.02
2023-03-01 14:30:10 iteration: 130800 loss: 0.0028 lr: 0.02
2023-03-01 14:30:19 iteration: 130900 loss: 0.0029 lr: 0.02
2023-03-01 14:30:27 iteration: 131000 loss: 0.0025 lr: 0.02
2023-03-01 14:30:35 iteration: 131100 loss: 0.0028 lr: 0.02
2023-03-01 14:30:43 iteration: 131200 loss: 0.0026 lr: 0.02
2023-03-01 14:30:52 iteration: 131300 loss: 0.0028 lr: 0.02
2023-03-01 14:31:00 iteration: 131400 loss: 0.0028 lr: 0.02
2023-03-01 14:31:08 iteration: 131500 loss: 0.0025 lr: 0.02
2023-03-01 14:31:17 iteration: 131600 loss: 0.0027 lr: 0.02
2023-03-01 14:31:25 iteration: 131700 loss: 0.0027 lr: 0.02
2023-03-01 14:31:33 iteration: 131800 loss: 0.0026 lr: 0.02
2023-03-01 14:31:41 iteration: 131900 loss: 0.0024 lr: 0.02
2023-03-01 14:31:50 iteration: 132000 loss: 0.0026 lr: 0.02
2023-03-01 14:31:59 iteration: 132100 loss: 0.0027 lr: 0.02
2023-03-01 14:32:08 iteration: 132200 loss: 0.0029 lr: 0.02
2023-03-01 14:32:16 iteration: 132300 loss: 0.0026 lr: 0.02
2023-03-01 14:32:24 iteration: 132400 loss: 0.0027 lr: 0.02
2023-03-01 14:32:33 iteration: 132500 loss: 0.0027 lr: 0.02
2023-03-01 14:32:45 iteration: 132600 loss: 0.0026 lr: 0.02
2023-03-01 14:32:53 iteration: 132700 loss: 0.0027 lr: 0.02
2023-03-01 14:33:01 iteration: 132800 loss: 0.0028 lr: 0.02
2023-03-01 14:33:10 iteration: 132900 loss: 0.0026 lr: 0.02
2023-03-01 14:33:17 iteration: 133000 loss: 0.0026 lr: 0.02
2023-03-01 14:33:26 iteration: 133100 loss: 0.0027 lr: 0.02
2023-03-01 14:33:34 iteration: 133200 loss: 0.0026 lr: 0.02
2023-03-01 14:33:42 iteration: 133300 loss: 0.0027 lr: 0.02
2023-03-01 14:33:51 iteration: 133400 loss: 0.0026 lr: 0.02
2023-03-01 14:33:59 iteration: 133500 loss: 0.0026 lr: 0.02
2023-03-01 14:34:07 iteration: 133600 loss: 0.0024 lr: 0.02
2023-03-01 14:34:15 iteration: 133700 loss: 0.0026 lr: 0.02
2023-03-01 14:34:24 iteration: 133800 loss: 0.0026 lr: 0.02
2023-03-01 14:34:32 iteration: 133900 loss: 0.0025 lr: 0.02
2023-03-01 14:34:41 iteration: 134000 loss: 0.0024 lr: 0.02
2023-03-01 14:34:49 iteration: 134100 loss: 0.0025 lr: 0.02
2023-03-01 14:34:57 iteration: 134200 loss: 0.0028 lr: 0.02
2023-03-01 14:35:05 iteration: 134300 loss: 0.0027 lr: 0.02
2023-03-01 14:35:13 iteration: 134400 loss: 0.0027 lr: 0.02
2023-03-01 14:35:22 iteration: 134500 loss: 0.0026 lr: 0.02
2023-03-01 14:35:30 iteration: 134600 loss: 0.0026 lr: 0.02
2023-03-01 14:35:38 iteration: 134700 loss: 0.0028 lr: 0.02
2023-03-01 14:35:47 iteration: 134800 loss: 0.0028 lr: 0.02
2023-03-01 14:35:55 iteration: 134900 loss: 0.0023 lr: 0.02
2023-03-01 14:36:04 iteration: 135000 loss: 0.0025 lr: 0.02
2023-03-01 14:36:12 iteration: 135100 loss: 0.0025 lr: 0.02
2023-03-01 14:36:21 iteration: 135200 loss: 0.0029 lr: 0.02
2023-03-01 14:36:30 iteration: 135300 loss: 0.0025 lr: 0.02
2023-03-01 14:36:38 iteration: 135400 loss: 0.0025 lr: 0.02
2023-03-01 14:36:48 iteration: 135500 loss: 0.0027 lr: 0.02
2023-03-01 14:36:58 iteration: 135600 loss: 0.0027 lr: 0.02
2023-03-01 14:37:07 iteration: 135700 loss: 0.0026 lr: 0.02
2023-03-01 14:37:16 iteration: 135800 loss: 0.0029 lr: 0.02
2023-03-01 14:37:24 iteration: 135900 loss: 0.0030 lr: 0.02
2023-03-01 14:37:32 iteration: 136000 loss: 0.0024 lr: 0.02
2023-03-01 14:37:42 iteration: 136100 loss: 0.0028 lr: 0.02
2023-03-01 14:37:50 iteration: 136200 loss: 0.0027 lr: 0.02
2023-03-01 14:37:59 iteration: 136300 loss: 0.0026 lr: 0.02
2023-03-01 14:38:07 iteration: 136400 loss: 0.0026 lr: 0.02
2023-03-01 14:38:16 iteration: 136500 loss: 0.0025 lr: 0.02
2023-03-01 14:38:24 iteration: 136600 loss: 0.0024 lr: 0.02
2023-03-01 14:38:33 iteration: 136700 loss: 0.0026 lr: 0.02
2023-03-01 14:38:41 iteration: 136800 loss: 0.0025 lr: 0.02
2023-03-01 14:38:49 iteration: 136900 loss: 0.0029 lr: 0.02
2023-03-01 14:38:58 iteration: 137000 loss: 0.0027 lr: 0.02
2023-03-01 14:39:06 iteration: 137100 loss: 0.0026 lr: 0.02
2023-03-01 14:39:14 iteration: 137200 loss: 0.0028 lr: 0.02
2023-03-01 14:39:22 iteration: 137300 loss: 0.0026 lr: 0.02
2023-03-01 14:39:30 iteration: 137400 loss: 0.0027 lr: 0.02
2023-03-01 14:39:38 iteration: 137500 loss: 0.0025 lr: 0.02
2023-03-01 14:39:46 iteration: 137600 loss: 0.0029 lr: 0.02
2023-03-01 14:39:54 iteration: 137700 loss: 0.0024 lr: 0.02
2023-03-01 14:40:02 iteration: 137800 loss: 0.0025 lr: 0.02
2023-03-01 14:40:11 iteration: 137900 loss: 0.0026 lr: 0.02
2023-03-01 14:40:20 iteration: 138000 loss: 0.0026 lr: 0.02
2023-03-01 14:40:28 iteration: 138100 loss: 0.0027 lr: 0.02
2023-03-01 14:40:36 iteration: 138200 loss: 0.0026 lr: 0.02
2023-03-01 14:40:45 iteration: 138300 loss: 0.0027 lr: 0.02
2023-03-01 14:40:53 iteration: 138400 loss: 0.0026 lr: 0.02
2023-03-01 14:41:02 iteration: 138500 loss: 0.0026 lr: 0.02
2023-03-01 14:41:10 iteration: 138600 loss: 0.0028 lr: 0.02
2023-03-01 14:41:19 iteration: 138700 loss: 0.0028 lr: 0.02
2023-03-01 14:41:27 iteration: 138800 loss: 0.0026 lr: 0.02
2023-03-01 14:41:36 iteration: 138900 loss: 0.0027 lr: 0.02
2023-03-01 14:41:44 iteration: 139000 loss: 0.0025 lr: 0.02
2023-03-01 14:41:53 iteration: 139100 loss: 0.0025 lr: 0.02
2023-03-01 14:42:01 iteration: 139200 loss: 0.0026 lr: 0.02
2023-03-01 14:42:10 iteration: 139300 loss: 0.0025 lr: 0.02
2023-03-01 14:42:18 iteration: 139400 loss: 0.0029 lr: 0.02
2023-03-01 14:42:27 iteration: 139500 loss: 0.0027 lr: 0.02
2023-03-01 14:42:37 iteration: 139600 loss: 0.0025 lr: 0.02
2023-03-01 14:42:45 iteration: 139700 loss: 0.0025 lr: 0.02
2023-03-01 14:42:54 iteration: 139800 loss: 0.0024 lr: 0.02
2023-03-01 14:43:03 iteration: 139900 loss: 0.0025 lr: 0.02
2023-03-01 14:43:12 iteration: 140000 loss: 0.0026 lr: 0.02
2023-03-01 14:43:21 iteration: 140100 loss: 0.0025 lr: 0.02
2023-03-01 14:43:29 iteration: 140200 loss: 0.0027 lr: 0.02
2023-03-01 14:43:37 iteration: 140300 loss: 0.0024 lr: 0.02
2023-03-01 14:43:46 iteration: 140400 loss: 0.0029 lr: 0.02
2023-03-01 14:43:55 iteration: 140500 loss: 0.0024 lr: 0.02
2023-03-01 14:44:03 iteration: 140600 loss: 0.0023 lr: 0.02
2023-03-01 14:44:11 iteration: 140700 loss: 0.0024 lr: 0.02
2023-03-01 14:44:18 iteration: 140800 loss: 0.0026 lr: 0.02
2023-03-01 14:44:27 iteration: 140900 loss: 0.0024 lr: 0.02
2023-03-01 14:44:35 iteration: 141000 loss: 0.0023 lr: 0.02
2023-03-01 14:44:44 iteration: 141100 loss: 0.0024 lr: 0.02
2023-03-01 14:44:53 iteration: 141200 loss: 0.0026 lr: 0.02
2023-03-01 14:45:02 iteration: 141300 loss: 0.0026 lr: 0.02
2023-03-01 14:45:11 iteration: 141400 loss: 0.0026 lr: 0.02
2023-03-01 14:45:19 iteration: 141500 loss: 0.0023 lr: 0.02
2023-03-01 14:45:27 iteration: 141600 loss: 0.0024 lr: 0.02
2023-03-01 14:45:36 iteration: 141700 loss: 0.0025 lr: 0.02
2023-03-01 14:45:44 iteration: 141800 loss: 0.0029 lr: 0.02
2023-03-01 14:45:52 iteration: 141900 loss: 0.0025 lr: 0.02
2023-03-01 14:46:02 iteration: 142000 loss: 0.0026 lr: 0.02
2023-03-01 14:46:11 iteration: 142100 loss: 0.0024 lr: 0.02
2023-03-01 14:46:19 iteration: 142200 loss: 0.0028 lr: 0.02
2023-03-01 14:46:28 iteration: 142300 loss: 0.0026 lr: 0.02
2023-03-01 14:46:37 iteration: 142400 loss: 0.0025 lr: 0.02
2023-03-01 14:46:45 iteration: 142500 loss: 0.0024 lr: 0.02
2023-03-01 14:46:53 iteration: 142600 loss: 0.0024 lr: 0.02
2023-03-01 14:47:02 iteration: 142700 loss: 0.0025 lr: 0.02
2023-03-01 14:47:11 iteration: 142800 loss: 0.0028 lr: 0.02
2023-03-01 14:47:20 iteration: 142900 loss: 0.0025 lr: 0.02
2023-03-01 14:47:28 iteration: 143000 loss: 0.0025 lr: 0.02
2023-03-01 14:47:36 iteration: 143100 loss: 0.0023 lr: 0.02
2023-03-01 14:47:45 iteration: 143200 loss: 0.0023 lr: 0.02
2023-03-01 14:47:53 iteration: 143300 loss: 0.0025 lr: 0.02
2023-03-01 14:48:02 iteration: 143400 loss: 0.0025 lr: 0.02
2023-03-01 14:48:10 iteration: 143500 loss: 0.0026 lr: 0.02
2023-03-01 14:48:18 iteration: 143600 loss: 0.0025 lr: 0.02
2023-03-01 14:48:26 iteration: 143700 loss: 0.0024 lr: 0.02
2023-03-01 14:48:34 iteration: 143800 loss: 0.0024 lr: 0.02
2023-03-01 14:48:42 iteration: 143900 loss: 0.0025 lr: 0.02
2023-03-01 14:48:51 iteration: 144000 loss: 0.0024 lr: 0.02
2023-03-01 14:48:59 iteration: 144100 loss: 0.0025 lr: 0.02
2023-03-01 14:49:09 iteration: 144200 loss: 0.0023 lr: 0.02
2023-03-01 14:49:19 iteration: 144300 loss: 0.0023 lr: 0.02
2023-03-01 14:49:29 iteration: 144400 loss: 0.0027 lr: 0.02
2023-03-01 14:49:37 iteration: 144500 loss: 0.0025 lr: 0.02
2023-03-01 14:49:47 iteration: 144600 loss: 0.0024 lr: 0.02
2023-03-01 14:49:56 iteration: 144700 loss: 0.0025 lr: 0.02
2023-03-01 14:50:03 iteration: 144800 loss: 0.0026 lr: 0.02
2023-03-01 14:50:13 iteration: 144900 loss: 0.0026 lr: 0.02
2023-03-01 14:50:22 iteration: 145000 loss: 0.0025 lr: 0.02
2023-03-01 14:50:30 iteration: 145100 loss: 0.0022 lr: 0.02
2023-03-01 14:50:39 iteration: 145200 loss: 0.0022 lr: 0.02
2023-03-01 14:50:48 iteration: 145300 loss: 0.0023 lr: 0.02
2023-03-01 14:50:57 iteration: 145400 loss: 0.0028 lr: 0.02
2023-03-01 14:51:05 iteration: 145500 loss: 0.0024 lr: 0.02
2023-03-01 14:51:14 iteration: 145600 loss: 0.0028 lr: 0.02
2023-03-01 14:51:22 iteration: 145700 loss: 0.0025 lr: 0.02
2023-03-01 14:51:31 iteration: 145800 loss: 0.0024 lr: 0.02
2023-03-01 14:51:39 iteration: 145900 loss: 0.0027 lr: 0.02
2023-03-01 14:51:47 iteration: 146000 loss: 0.0024 lr: 0.02
2023-03-01 14:51:55 iteration: 146100 loss: 0.0028 lr: 0.02
2023-03-01 14:52:04 iteration: 146200 loss: 0.0027 lr: 0.02
2023-03-01 14:52:12 iteration: 146300 loss: 0.0025 lr: 0.02
2023-03-01 14:52:20 iteration: 146400 loss: 0.0026 lr: 0.02
2023-03-01 14:52:29 iteration: 146500 loss: 0.0024 lr: 0.02
2023-03-01 14:52:37 iteration: 146600 loss: 0.0024 lr: 0.02
2023-03-01 14:52:46 iteration: 146700 loss: 0.0025 lr: 0.02
2023-03-01 14:52:54 iteration: 146800 loss: 0.0025 lr: 0.02
2023-03-01 14:53:03 iteration: 146900 loss: 0.0026 lr: 0.02
2023-03-01 14:53:12 iteration: 147000 loss: 0.0024 lr: 0.02
2023-03-01 14:53:20 iteration: 147100 loss: 0.0023 lr: 0.02
2023-03-01 14:53:29 iteration: 147200 loss: 0.0026 lr: 0.02
2023-03-01 14:53:37 iteration: 147300 loss: 0.0024 lr: 0.02
2023-03-01 14:53:46 iteration: 147400 loss: 0.0023 lr: 0.02
2023-03-01 14:53:54 iteration: 147500 loss: 0.0026 lr: 0.02
2023-03-01 14:54:05 iteration: 147600 loss: 0.0024 lr: 0.02
2023-03-01 14:54:13 iteration: 147700 loss: 0.0028 lr: 0.02
2023-03-01 14:54:22 iteration: 147800 loss: 0.0025 lr: 0.02
2023-03-01 14:54:30 iteration: 147900 loss: 0.0025 lr: 0.02
2023-03-01 14:54:38 iteration: 148000 loss: 0.0024 lr: 0.02
2023-03-01 14:54:47 iteration: 148100 loss: 0.0023 lr: 0.02
2023-03-01 14:54:56 iteration: 148200 loss: 0.0023 lr: 0.02
2023-03-01 14:55:04 iteration: 148300 loss: 0.0025 lr: 0.02
2023-03-01 14:55:14 iteration: 148400 loss: 0.0027 lr: 0.02
2023-03-01 14:55:22 iteration: 148500 loss: 0.0022 lr: 0.02
2023-03-01 14:55:30 iteration: 148600 loss: 0.0026 lr: 0.02
2023-03-01 14:55:38 iteration: 148700 loss: 0.0023 lr: 0.02
2023-03-01 14:55:46 iteration: 148800 loss: 0.0023 lr: 0.02
2023-03-01 14:55:55 iteration: 148900 loss: 0.0024 lr: 0.02
2023-03-01 14:56:05 iteration: 149000 loss: 0.0024 lr: 0.02
2023-03-01 14:56:13 iteration: 149100 loss: 0.0022 lr: 0.02
2023-03-01 14:56:21 iteration: 149200 loss: 0.0022 lr: 0.02
2023-03-01 14:56:30 iteration: 149300 loss: 0.0022 lr: 0.02
2023-03-01 14:56:38 iteration: 149400 loss: 0.0023 lr: 0.02
2023-03-01 14:56:46 iteration: 149500 loss: 0.0025 lr: 0.02
2023-03-01 14:56:56 iteration: 149600 loss: 0.0025 lr: 0.02
2023-03-01 14:57:04 iteration: 149700 loss: 0.0026 lr: 0.02
2023-03-01 14:57:12 iteration: 149800 loss: 0.0024 lr: 0.02
2023-03-01 14:57:20 iteration: 149900 loss: 0.0025 lr: 0.02
2023-03-01 14:57:28 iteration: 150000 loss: 0.0025 lr: 0.02
2023-03-01 14:57:37 iteration: 150100 loss: 0.0024 lr: 0.02
2023-03-01 14:57:45 iteration: 150200 loss: 0.0025 lr: 0.02
2023-03-01 14:57:53 iteration: 150300 loss: 0.0025 lr: 0.02
2023-03-01 14:58:02 iteration: 150400 loss: 0.0027 lr: 0.02
2023-03-01 14:58:10 iteration: 150500 loss: 0.0024 lr: 0.02
2023-03-01 14:58:18 iteration: 150600 loss: 0.0026 lr: 0.02
2023-03-01 14:58:27 iteration: 150700 loss: 0.0024 lr: 0.02
2023-03-01 14:58:36 iteration: 150800 loss: 0.0025 lr: 0.02
2023-03-01 14:58:44 iteration: 150900 loss: 0.0024 lr: 0.02
2023-03-01 14:58:53 iteration: 151000 loss: 0.0022 lr: 0.02
2023-03-01 14:59:02 iteration: 151100 loss: 0.0024 lr: 0.02
2023-03-01 14:59:11 iteration: 151200 loss: 0.0023 lr: 0.02
2023-03-01 14:59:19 iteration: 151300 loss: 0.0025 lr: 0.02
2023-03-01 14:59:28 iteration: 151400 loss: 0.0025 lr: 0.02
2023-03-01 14:59:36 iteration: 151500 loss: 0.0024 lr: 0.02
2023-03-01 14:59:44 iteration: 151600 loss: 0.0025 lr: 0.02
2023-03-01 14:59:52 iteration: 151700 loss: 0.0024 lr: 0.02
2023-03-01 15:00:01 iteration: 151800 loss: 0.0023 lr: 0.02
2023-03-01 15:00:09 iteration: 151900 loss: 0.0024 lr: 0.02
2023-03-01 15:00:18 iteration: 152000 loss: 0.0026 lr: 0.02
2023-03-01 15:00:26 iteration: 152100 loss: 0.0025 lr: 0.02
2023-03-01 15:00:36 iteration: 152200 loss: 0.0025 lr: 0.02
2023-03-01 15:00:44 iteration: 152300 loss: 0.0024 lr: 0.02
2023-03-01 15:00:52 iteration: 152400 loss: 0.0026 lr: 0.02
2023-03-01 15:01:00 iteration: 152500 loss: 0.0026 lr: 0.02
2023-03-01 15:01:09 iteration: 152600 loss: 0.0025 lr: 0.02
2023-03-01 15:01:17 iteration: 152700 loss: 0.0025 lr: 0.02
2023-03-01 15:01:26 iteration: 152800 loss: 0.0024 lr: 0.02
2023-03-01 15:01:34 iteration: 152900 loss: 0.0026 lr: 0.02
2023-03-01 15:01:43 iteration: 153000 loss: 0.0026 lr: 0.02
2023-03-01 15:01:52 iteration: 153100 loss: 0.0025 lr: 0.02
2023-03-01 15:02:00 iteration: 153200 loss: 0.0023 lr: 0.02
2023-03-01 15:02:08 iteration: 153300 loss: 0.0026 lr: 0.02
2023-03-01 15:02:18 iteration: 153400 loss: 0.0024 lr: 0.02
2023-03-01 15:02:27 iteration: 153500 loss: 0.0023 lr: 0.02
2023-03-01 15:02:35 iteration: 153600 loss: 0.0024 lr: 0.02
2023-03-01 15:02:43 iteration: 153700 loss: 0.0025 lr: 0.02
2023-03-01 15:02:52 iteration: 153800 loss: 0.0023 lr: 0.02
2023-03-01 15:03:00 iteration: 153900 loss: 0.0025 lr: 0.02
2023-03-01 15:03:10 iteration: 154000 loss: 0.0025 lr: 0.02
2023-03-01 15:03:19 iteration: 154100 loss: 0.0025 lr: 0.02
2023-03-01 15:03:28 iteration: 154200 loss: 0.0025 lr: 0.02
2023-03-01 15:03:37 iteration: 154300 loss: 0.0024 lr: 0.02
2023-03-01 15:03:46 iteration: 154400 loss: 0.0025 lr: 0.02
2023-03-01 15:03:55 iteration: 154500 loss: 0.0024 lr: 0.02
2023-03-01 15:04:03 iteration: 154600 loss: 0.0022 lr: 0.02
2023-03-01 15:04:11 iteration: 154700 loss: 0.0024 lr: 0.02
2023-03-01 15:04:19 iteration: 154800 loss: 0.0025 lr: 0.02
2023-03-01 15:04:27 iteration: 154900 loss: 0.0026 lr: 0.02
2023-03-01 15:04:35 iteration: 155000 loss: 0.0024 lr: 0.02
2023-03-01 15:04:44 iteration: 155100 loss: 0.0024 lr: 0.02
2023-03-01 15:04:53 iteration: 155200 loss: 0.0025 lr: 0.02
2023-03-01 15:05:02 iteration: 155300 loss: 0.0025 lr: 0.02
2023-03-01 15:05:11 iteration: 155400 loss: 0.0024 lr: 0.02
2023-03-01 15:05:19 iteration: 155500 loss: 0.0024 lr: 0.02
2023-03-01 15:05:27 iteration: 155600 loss: 0.0022 lr: 0.02
2023-03-01 15:05:37 iteration: 155700 loss: 0.0026 lr: 0.02
2023-03-01 15:05:45 iteration: 155800 loss: 0.0023 lr: 0.02
2023-03-01 15:05:54 iteration: 155900 loss: 0.0024 lr: 0.02
2023-03-01 15:06:02 iteration: 156000 loss: 0.0025 lr: 0.02
2023-03-01 15:06:10 iteration: 156100 loss: 0.0024 lr: 0.02
2023-03-01 15:06:18 iteration: 156200 loss: 0.0024 lr: 0.02
2023-03-01 15:06:26 iteration: 156300 loss: 0.0024 lr: 0.02
2023-03-01 15:06:36 iteration: 156400 loss: 0.0024 lr: 0.02
2023-03-01 15:06:44 iteration: 156500 loss: 0.0025 lr: 0.02
2023-03-01 15:06:52 iteration: 156600 loss: 0.0023 lr: 0.02
2023-03-01 15:07:01 iteration: 156700 loss: 0.0025 lr: 0.02
2023-03-01 15:07:09 iteration: 156800 loss: 0.0024 lr: 0.02
2023-03-01 15:07:18 iteration: 156900 loss: 0.0025 lr: 0.02
2023-03-01 15:07:27 iteration: 157000 loss: 0.0023 lr: 0.02
2023-03-01 15:07:36 iteration: 157100 loss: 0.0024 lr: 0.02
2023-03-01 15:07:45 iteration: 157200 loss: 0.0026 lr: 0.02
2023-03-01 15:07:53 iteration: 157300 loss: 0.0023 lr: 0.02
2023-03-01 15:08:01 iteration: 157400 loss: 0.0025 lr: 0.02
2023-03-01 15:08:09 iteration: 157500 loss: 0.0022 lr: 0.02
2023-03-01 15:08:17 iteration: 157600 loss: 0.0022 lr: 0.02
2023-03-01 15:08:25 iteration: 157700 loss: 0.0021 lr: 0.02
2023-03-01 15:08:35 iteration: 157800 loss: 0.0026 lr: 0.02
2023-03-01 15:08:43 iteration: 157900 loss: 0.0025 lr: 0.02
2023-03-01 15:08:51 iteration: 158000 loss: 0.0025 lr: 0.02
2023-03-01 15:08:59 iteration: 158100 loss: 0.0024 lr: 0.02
2023-03-01 15:09:08 iteration: 158200 loss: 0.0025 lr: 0.02
2023-03-01 15:09:16 iteration: 158300 loss: 0.0026 lr: 0.02
2023-03-01 15:09:24 iteration: 158400 loss: 0.0025 lr: 0.02
2023-03-01 15:09:33 iteration: 158500 loss: 0.0021 lr: 0.02
2023-03-01 15:09:42 iteration: 158600 loss: 0.0025 lr: 0.02
2023-03-01 15:09:50 iteration: 158700 loss: 0.0023 lr: 0.02
2023-03-01 15:09:58 iteration: 158800 loss: 0.0025 lr: 0.02
2023-03-01 15:10:07 iteration: 158900 loss: 0.0028 lr: 0.02
2023-03-01 15:10:16 iteration: 159000 loss: 0.0026 lr: 0.02
2023-03-01 15:10:24 iteration: 159100 loss: 0.0025 lr: 0.02
2023-03-01 15:10:33 iteration: 159200 loss: 0.0025 lr: 0.02
2023-03-01 15:10:43 iteration: 159300 loss: 0.0026 lr: 0.02
2023-03-01 15:10:52 iteration: 159400 loss: 0.0027 lr: 0.02
2023-03-01 15:11:02 iteration: 159500 loss: 0.0024 lr: 0.02
2023-03-01 15:11:10 iteration: 159600 loss: 0.0026 lr: 0.02
2023-03-01 15:11:18 iteration: 159700 loss: 0.0023 lr: 0.02
2023-03-01 15:11:27 iteration: 159800 loss: 0.0024 lr: 0.02
2023-03-01 15:11:35 iteration: 159900 loss: 0.0026 lr: 0.02
2023-03-01 15:11:43 iteration: 160000 loss: 0.0024 lr: 0.02
2023-03-01 15:11:52 iteration: 160100 loss: 0.0026 lr: 0.02
2023-03-01 15:12:00 iteration: 160200 loss: 0.0024 lr: 0.02
2023-03-01 15:12:09 iteration: 160300 loss: 0.0025 lr: 0.02
2023-03-01 15:12:17 iteration: 160400 loss: 0.0022 lr: 0.02
2023-03-01 15:12:26 iteration: 160500 loss: 0.0024 lr: 0.02
2023-03-01 15:12:34 iteration: 160600 loss: 0.0024 lr: 0.02
2023-03-01 15:12:42 iteration: 160700 loss: 0.0026 lr: 0.02
2023-03-01 15:12:50 iteration: 160800 loss: 0.0023 lr: 0.02
2023-03-01 15:12:59 iteration: 160900 loss: 0.0024 lr: 0.02
2023-03-01 15:13:07 iteration: 161000 loss: 0.0023 lr: 0.02
2023-03-01 15:13:15 iteration: 161100 loss: 0.0023 lr: 0.02
2023-03-01 15:13:23 iteration: 161200 loss: 0.0026 lr: 0.02
2023-03-01 15:13:32 iteration: 161300 loss: 0.0023 lr: 0.02
2023-03-01 15:13:41 iteration: 161400 loss: 0.0024 lr: 0.02
2023-03-01 15:13:49 iteration: 161500 loss: 0.0028 lr: 0.02
2023-03-01 15:13:57 iteration: 161600 loss: 0.0027 lr: 0.02
2023-03-01 15:14:06 iteration: 161700 loss: 0.0025 lr: 0.02
2023-03-01 15:14:14 iteration: 161800 loss: 0.0025 lr: 0.02
2023-03-01 15:14:22 iteration: 161900 loss: 0.0024 lr: 0.02
2023-03-01 15:14:32 iteration: 162000 loss: 0.0026 lr: 0.02
2023-03-01 15:14:42 iteration: 162100 loss: 0.0023 lr: 0.02
2023-03-01 15:14:50 iteration: 162200 loss: 0.0024 lr: 0.02
2023-03-01 15:14:59 iteration: 162300 loss: 0.0023 lr: 0.02
2023-03-01 15:15:08 iteration: 162400 loss: 0.0023 lr: 0.02
2023-03-01 15:15:16 iteration: 162500 loss: 0.0024 lr: 0.02
2023-03-01 15:15:24 iteration: 162600 loss: 0.0024 lr: 0.02
2023-03-01 15:15:33 iteration: 162700 loss: 0.0026 lr: 0.02
2023-03-01 15:15:41 iteration: 162800 loss: 0.0024 lr: 0.02
2023-03-01 15:15:49 iteration: 162900 loss: 0.0024 lr: 0.02
2023-03-01 15:15:57 iteration: 163000 loss: 0.0026 lr: 0.02
2023-03-01 15:16:05 iteration: 163100 loss: 0.0024 lr: 0.02
2023-03-01 15:16:13 iteration: 163200 loss: 0.0024 lr: 0.02
2023-03-01 15:16:22 iteration: 163300 loss: 0.0023 lr: 0.02
2023-03-01 15:16:31 iteration: 163400 loss: 0.0025 lr: 0.02
2023-03-01 15:16:42 iteration: 163500 loss: 0.0025 lr: 0.02
2023-03-01 15:16:50 iteration: 163600 loss: 0.0025 lr: 0.02
2023-03-01 15:16:58 iteration: 163700 loss: 0.0024 lr: 0.02
2023-03-01 15:17:06 iteration: 163800 loss: 0.0023 lr: 0.02
2023-03-01 15:17:15 iteration: 163900 loss: 0.0024 lr: 0.02
2023-03-01 15:17:23 iteration: 164000 loss: 0.0023 lr: 0.02
2023-03-01 15:17:31 iteration: 164100 loss: 0.0022 lr: 0.02
2023-03-01 15:17:40 iteration: 164200 loss: 0.0026 lr: 0.02
2023-03-01 15:17:48 iteration: 164300 loss: 0.0023 lr: 0.02
2023-03-01 15:17:57 iteration: 164400 loss: 0.0024 lr: 0.02
2023-03-01 15:18:05 iteration: 164500 loss: 0.0024 lr: 0.02
2023-03-01 15:18:13 iteration: 164600 loss: 0.0025 lr: 0.02
2023-03-01 15:18:21 iteration: 164700 loss: 0.0025 lr: 0.02
2023-03-01 15:18:29 iteration: 164800 loss: 0.0024 lr: 0.02
2023-03-01 15:18:38 iteration: 164900 loss: 0.0025 lr: 0.02
2023-03-01 15:18:46 iteration: 165000 loss: 0.0023 lr: 0.02
2023-03-01 15:18:56 iteration: 165100 loss: 0.0025 lr: 0.02
2023-03-01 15:19:05 iteration: 165200 loss: 0.0026 lr: 0.02
2023-03-01 15:19:15 iteration: 165300 loss: 0.0025 lr: 0.02
2023-03-01 15:19:24 iteration: 165400 loss: 0.0023 lr: 0.02
2023-03-01 15:19:33 iteration: 165500 loss: 0.0022 lr: 0.02
2023-03-01 15:19:41 iteration: 165600 loss: 0.0023 lr: 0.02
2023-03-01 15:19:50 iteration: 165700 loss: 0.0025 lr: 0.02
2023-03-01 15:19:58 iteration: 165800 loss: 0.0024 lr: 0.02
2023-03-01 15:20:06 iteration: 165900 loss: 0.0023 lr: 0.02
2023-03-01 15:20:14 iteration: 166000 loss: 0.0022 lr: 0.02
2023-03-01 15:20:22 iteration: 166100 loss: 0.0023 lr: 0.02
2023-03-01 15:20:31 iteration: 166200 loss: 0.0023 lr: 0.02
2023-03-01 15:20:39 iteration: 166300 loss: 0.0023 lr: 0.02
2023-03-01 15:20:48 iteration: 166400 loss: 0.0024 lr: 0.02
2023-03-01 15:20:56 iteration: 166500 loss: 0.0025 lr: 0.02
2023-03-01 15:21:04 iteration: 166600 loss: 0.0025 lr: 0.02
2023-03-01 15:21:13 iteration: 166700 loss: 0.0025 lr: 0.02
2023-03-01 15:21:21 iteration: 166800 loss: 0.0024 lr: 0.02
2023-03-01 15:21:29 iteration: 166900 loss: 0.0023 lr: 0.02
2023-03-01 15:21:38 iteration: 167000 loss: 0.0024 lr: 0.02
2023-03-01 15:21:46 iteration: 167100 loss: 0.0024 lr: 0.02
2023-03-01 15:21:54 iteration: 167200 loss: 0.0025 lr: 0.02
2023-03-01 15:22:02 iteration: 167300 loss: 0.0022 lr: 0.02
2023-03-01 15:22:11 iteration: 167400 loss: 0.0023 lr: 0.02
2023-03-01 15:22:19 iteration: 167500 loss: 0.0025 lr: 0.02
2023-03-01 15:22:28 iteration: 167600 loss: 0.0025 lr: 0.02
2023-03-01 15:22:36 iteration: 167700 loss: 0.0024 lr: 0.02
2023-03-01 15:22:44 iteration: 167800 loss: 0.0024 lr: 0.02
2023-03-01 15:22:53 iteration: 167900 loss: 0.0023 lr: 0.02
2023-03-01 15:23:01 iteration: 168000 loss: 0.0024 lr: 0.02
2023-03-01 15:23:10 iteration: 168100 loss: 0.0024 lr: 0.02
2023-03-01 15:23:17 iteration: 168200 loss: 0.0022 lr: 0.02
2023-03-01 15:23:26 iteration: 168300 loss: 0.0023 lr: 0.02
2023-03-01 15:23:34 iteration: 168400 loss: 0.0025 lr: 0.02
2023-03-01 15:23:42 iteration: 168500 loss: 0.0023 lr: 0.02
2023-03-01 15:23:50 iteration: 168600 loss: 0.0024 lr: 0.02
2023-03-01 15:23:59 iteration: 168700 loss: 0.0024 lr: 0.02
2023-03-01 15:24:07 iteration: 168800 loss: 0.0023 lr: 0.02
2023-03-01 15:24:16 iteration: 168900 loss: 0.0024 lr: 0.02
2023-03-01 15:24:25 iteration: 169000 loss: 0.0022 lr: 0.02
2023-03-01 15:24:33 iteration: 169100 loss: 0.0025 lr: 0.02
2023-03-01 15:24:41 iteration: 169200 loss: 0.0023 lr: 0.02
2023-03-01 15:24:50 iteration: 169300 loss: 0.0027 lr: 0.02
2023-03-01 15:24:59 iteration: 169400 loss: 0.0023 lr: 0.02
2023-03-01 15:25:08 iteration: 169500 loss: 0.0024 lr: 0.02
2023-03-01 15:25:17 iteration: 169600 loss: 0.0024 lr: 0.02
2023-03-01 15:25:25 iteration: 169700 loss: 0.0021 lr: 0.02
2023-03-01 15:25:34 iteration: 169800 loss: 0.0022 lr: 0.02
2023-03-01 15:25:42 iteration: 169900 loss: 0.0022 lr: 0.02
2023-03-01 15:25:51 iteration: 170000 loss: 0.0023 lr: 0.02
2023-03-01 15:25:59 iteration: 170100 loss: 0.0021 lr: 0.02
2023-03-01 15:26:07 iteration: 170200 loss: 0.0023 lr: 0.02
2023-03-01 15:26:17 iteration: 170300 loss: 0.0022 lr: 0.02
2023-03-01 15:26:25 iteration: 170400 loss: 0.0023 lr: 0.02
2023-03-01 15:26:33 iteration: 170500 loss: 0.0023 lr: 0.02
2023-03-01 15:26:42 iteration: 170600 loss: 0.0025 lr: 0.02
2023-03-01 15:26:50 iteration: 170700 loss: 0.0022 lr: 0.02
2023-03-01 15:26:59 iteration: 170800 loss: 0.0023 lr: 0.02
2023-03-01 15:27:08 iteration: 170900 loss: 0.0024 lr: 0.02
2023-03-01 15:27:18 iteration: 171000 loss: 0.0023 lr: 0.02
2023-03-01 15:27:27 iteration: 171100 loss: 0.0023 lr: 0.02
2023-03-01 15:27:36 iteration: 171200 loss: 0.0023 lr: 0.02
2023-03-01 15:27:45 iteration: 171300 loss: 0.0024 lr: 0.02
2023-03-01 15:27:52 iteration: 171400 loss: 0.0024 lr: 0.02
2023-03-01 15:28:00 iteration: 171500 loss: 0.0025 lr: 0.02
2023-03-01 15:28:08 iteration: 171600 loss: 0.0025 lr: 0.02
2023-03-01 15:28:17 iteration: 171700 loss: 0.0024 lr: 0.02
2023-03-01 15:28:25 iteration: 171800 loss: 0.0022 lr: 0.02
2023-03-01 15:28:33 iteration: 171900 loss: 0.0022 lr: 0.02
2023-03-01 15:28:43 iteration: 172000 loss: 0.0025 lr: 0.02
2023-03-01 15:28:51 iteration: 172100 loss: 0.0024 lr: 0.02
2023-03-01 15:28:59 iteration: 172200 loss: 0.0023 lr: 0.02
2023-03-01 15:29:08 iteration: 172300 loss: 0.0024 lr: 0.02
2023-03-01 15:29:17 iteration: 172400 loss: 0.0023 lr: 0.02
2023-03-01 15:29:25 iteration: 172500 loss: 0.0022 lr: 0.02
2023-03-01 15:29:33 iteration: 172600 loss: 0.0025 lr: 0.02
2023-03-01 15:29:41 iteration: 172700 loss: 0.0025 lr: 0.02
2023-03-01 15:29:49 iteration: 172800 loss: 0.0022 lr: 0.02
2023-03-01 15:29:58 iteration: 172900 loss: 0.0026 lr: 0.02
2023-03-01 15:30:07 iteration: 173000 loss: 0.0021 lr: 0.02
2023-03-01 15:30:15 iteration: 173100 loss: 0.0021 lr: 0.02
2023-03-01 15:30:24 iteration: 173200 loss: 0.0024 lr: 0.02
2023-03-01 15:30:32 iteration: 173300 loss: 0.0023 lr: 0.02
2023-03-01 15:30:41 iteration: 173400 loss: 0.0022 lr: 0.02
2023-03-01 15:30:50 iteration: 173500 loss: 0.0022 lr: 0.02
2023-03-01 15:30:59 iteration: 173600 loss: 0.0024 lr: 0.02
2023-03-01 15:31:08 iteration: 173700 loss: 0.0024 lr: 0.02
2023-03-01 15:31:16 iteration: 173800 loss: 0.0021 lr: 0.02
2023-03-01 15:31:25 iteration: 173900 loss: 0.0022 lr: 0.02
2023-03-01 15:31:33 iteration: 174000 loss: 0.0022 lr: 0.02
2023-03-01 15:31:41 iteration: 174100 loss: 0.0026 lr: 0.02
2023-03-01 15:31:49 iteration: 174200 loss: 0.0022 lr: 0.02
2023-03-01 15:31:57 iteration: 174300 loss: 0.0026 lr: 0.02
2023-03-01 15:32:06 iteration: 174400 loss: 0.0024 lr: 0.02
2023-03-01 15:32:15 iteration: 174500 loss: 0.0024 lr: 0.02
2023-03-01 15:32:23 iteration: 174600 loss: 0.0024 lr: 0.02
2023-03-01 15:32:31 iteration: 174700 loss: 0.0024 lr: 0.02
2023-03-01 15:32:39 iteration: 174800 loss: 0.0024 lr: 0.02
2023-03-01 15:32:48 iteration: 174900 loss: 0.0024 lr: 0.02
2023-03-01 15:32:57 iteration: 175000 loss: 0.0026 lr: 0.02
2023-03-01 15:33:06 iteration: 175100 loss: 0.0025 lr: 0.02
2023-03-01 15:33:15 iteration: 175200 loss: 0.0022 lr: 0.02
2023-03-01 15:33:25 iteration: 175300 loss: 0.0024 lr: 0.02
2023-03-01 15:33:33 iteration: 175400 loss: 0.0022 lr: 0.02
2023-03-01 15:33:42 iteration: 175500 loss: 0.0023 lr: 0.02
2023-03-01 15:33:50 iteration: 175600 loss: 0.0025 lr: 0.02
2023-03-01 15:33:59 iteration: 175700 loss: 0.0022 lr: 0.02
2023-03-01 15:34:07 iteration: 175800 loss: 0.0026 lr: 0.02
2023-03-01 15:34:15 iteration: 175900 loss: 0.0025 lr: 0.02
2023-03-01 15:34:24 iteration: 176000 loss: 0.0024 lr: 0.02
2023-03-01 15:34:33 iteration: 176100 loss: 0.0023 lr: 0.02
2023-03-01 15:34:42 iteration: 176200 loss: 0.0020 lr: 0.02
2023-03-01 15:34:50 iteration: 176300 loss: 0.0024 lr: 0.02
2023-03-01 15:34:59 iteration: 176400 loss: 0.0023 lr: 0.02
2023-03-01 15:35:07 iteration: 176500 loss: 0.0022 lr: 0.02
2023-03-01 15:35:15 iteration: 176600 loss: 0.0023 lr: 0.02
2023-03-01 15:35:23 iteration: 176700 loss: 0.0029 lr: 0.02
2023-03-01 15:35:33 iteration: 176800 loss: 0.0025 lr: 0.02
2023-03-01 15:35:41 iteration: 176900 loss: 0.0025 lr: 0.02
2023-03-01 15:35:49 iteration: 177000 loss: 0.0023 lr: 0.02
2023-03-01 15:35:57 iteration: 177100 loss: 0.0023 lr: 0.02
2023-03-01 15:36:06 iteration: 177200 loss: 0.0025 lr: 0.02
2023-03-01 15:36:14 iteration: 177300 loss: 0.0023 lr: 0.02
2023-03-01 15:36:23 iteration: 177400 loss: 0.0023 lr: 0.02
2023-03-01 15:36:31 iteration: 177500 loss: 0.0025 lr: 0.02
2023-03-01 15:36:40 iteration: 177600 loss: 0.0025 lr: 0.02
2023-03-01 15:36:49 iteration: 177700 loss: 0.0022 lr: 0.02
2023-03-01 15:36:57 iteration: 177800 loss: 0.0023 lr: 0.02
2023-03-01 15:37:06 iteration: 177900 loss: 0.0022 lr: 0.02
2023-03-01 15:37:14 iteration: 178000 loss: 0.0022 lr: 0.02
2023-03-01 15:37:22 iteration: 178100 loss: 0.0022 lr: 0.02
2023-03-01 15:37:30 iteration: 178200 loss: 0.0021 lr: 0.02
2023-03-01 15:37:39 iteration: 178300 loss: 0.0024 lr: 0.02
2023-03-01 15:37:48 iteration: 178400 loss: 0.0024 lr: 0.02
2023-03-01 15:37:57 iteration: 178500 loss: 0.0025 lr: 0.02
2023-03-01 15:38:05 iteration: 178600 loss: 0.0021 lr: 0.02
2023-03-01 15:38:15 iteration: 178700 loss: 0.0023 lr: 0.02
2023-03-01 15:38:23 iteration: 178800 loss: 0.0024 lr: 0.02
2023-03-01 15:38:32 iteration: 178900 loss: 0.0027 lr: 0.02
2023-03-01 15:38:40 iteration: 179000 loss: 0.0023 lr: 0.02
2023-03-01 15:38:49 iteration: 179100 loss: 0.0023 lr: 0.02
2023-03-01 15:38:58 iteration: 179200 loss: 0.0021 lr: 0.02
2023-03-01 15:39:07 iteration: 179300 loss: 0.0023 lr: 0.02
2023-03-01 15:39:16 iteration: 179400 loss: 0.0023 lr: 0.02
2023-03-01 15:39:24 iteration: 179500 loss: 0.0023 lr: 0.02
2023-03-01 15:39:33 iteration: 179600 loss: 0.0024 lr: 0.02
2023-03-01 15:39:41 iteration: 179700 loss: 0.0023 lr: 0.02
2023-03-01 15:39:50 iteration: 179800 loss: 0.0025 lr: 0.02
2023-03-01 15:39:58 iteration: 179900 loss: 0.0023 lr: 0.02
2023-03-01 15:40:06 iteration: 180000 loss: 0.0021 lr: 0.02
2023-03-01 15:40:23 iteration: 180100 loss: 0.0026 lr: 0.02
2023-03-01 15:40:32 iteration: 180200 loss: 0.0024 lr: 0.02
2023-03-01 15:40:40 iteration: 180300 loss: 0.0024 lr: 0.02
2023-03-01 15:40:48 iteration: 180400 loss: 0.0024 lr: 0.02
2023-03-01 15:40:58 iteration: 180500 loss: 0.0023 lr: 0.02
2023-03-01 15:41:07 iteration: 180600 loss: 0.0024 lr: 0.02
2023-03-01 15:41:15 iteration: 180700 loss: 0.0021 lr: 0.02
2023-03-01 15:41:23 iteration: 180800 loss: 0.0022 lr: 0.02
2023-03-01 15:41:31 iteration: 180900 loss: 0.0023 lr: 0.02
2023-03-01 15:41:40 iteration: 181000 loss: 0.0024 lr: 0.02
2023-03-01 15:41:48 iteration: 181100 loss: 0.0023 lr: 0.02
2023-03-01 15:41:57 iteration: 181200 loss: 0.0022 lr: 0.02
2023-03-01 15:42:05 iteration: 181300 loss: 0.0025 lr: 0.02
2023-03-01 15:42:14 iteration: 181400 loss: 0.0022 lr: 0.02
2023-03-01 15:42:22 iteration: 181500 loss: 0.0023 lr: 0.02
2023-03-01 15:42:30 iteration: 181600 loss: 0.0024 lr: 0.02
2023-03-01 15:42:39 iteration: 181700 loss: 0.0022 lr: 0.02
2023-03-01 15:42:47 iteration: 181800 loss: 0.0023 lr: 0.02
2023-03-01 15:42:55 iteration: 181900 loss: 0.0024 lr: 0.02
2023-03-01 15:43:04 iteration: 182000 loss: 0.0022 lr: 0.02
2023-03-01 15:43:14 iteration: 182100 loss: 0.0022 lr: 0.02
2023-03-01 15:43:23 iteration: 182200 loss: 0.0023 lr: 0.02
2023-03-01 15:43:31 iteration: 182300 loss: 0.0022 lr: 0.02
2023-03-01 15:43:40 iteration: 182400 loss: 0.0022 lr: 0.02
2023-03-01 15:43:48 iteration: 182500 loss: 0.0022 lr: 0.02
2023-03-01 15:43:57 iteration: 182600 loss: 0.0024 lr: 0.02
2023-03-01 15:44:05 iteration: 182700 loss: 0.0022 lr: 0.02
2023-03-01 15:44:13 iteration: 182800 loss: 0.0023 lr: 0.02
2023-03-01 15:44:21 iteration: 182900 loss: 0.0024 lr: 0.02
2023-03-01 15:44:29 iteration: 183000 loss: 0.0023 lr: 0.02
2023-03-01 15:44:38 iteration: 183100 loss: 0.0023 lr: 0.02
2023-03-01 15:44:46 iteration: 183200 loss: 0.0026 lr: 0.02
2023-03-01 15:44:55 iteration: 183300 loss: 0.0021 lr: 0.02
2023-03-01 15:45:03 iteration: 183400 loss: 0.0022 lr: 0.02
2023-03-01 15:45:11 iteration: 183500 loss: 0.0022 lr: 0.02
2023-03-01 15:45:21 iteration: 183600 loss: 0.0026 lr: 0.02
2023-03-01 15:45:29 iteration: 183700 loss: 0.0022 lr: 0.02
2023-03-01 15:45:38 iteration: 183800 loss: 0.0024 lr: 0.02
2023-03-01 15:45:47 iteration: 183900 loss: 0.0022 lr: 0.02
2023-03-01 15:45:55 iteration: 184000 loss: 0.0023 lr: 0.02
2023-03-01 15:46:04 iteration: 184100 loss: 0.0024 lr: 0.02
2023-03-01 15:46:12 iteration: 184200 loss: 0.0023 lr: 0.02
2023-03-01 15:46:20 iteration: 184300 loss: 0.0021 lr: 0.02
2023-03-01 15:46:28 iteration: 184400 loss: 0.0024 lr: 0.02
2023-03-01 15:46:37 iteration: 184500 loss: 0.0022 lr: 0.02
2023-03-01 15:46:45 iteration: 184600 loss: 0.0022 lr: 0.02
2023-03-01 15:46:53 iteration: 184700 loss: 0.0022 lr: 0.02
2023-03-01 15:47:02 iteration: 184800 loss: 0.0022 lr: 0.02
2023-03-01 15:47:10 iteration: 184900 loss: 0.0023 lr: 0.02
2023-03-01 15:47:18 iteration: 185000 loss: 0.0022 lr: 0.02
2023-03-01 15:47:26 iteration: 185100 loss: 0.0023 lr: 0.02
2023-03-01 15:47:36 iteration: 185200 loss: 0.0025 lr: 0.02
2023-03-01 15:47:44 iteration: 185300 loss: 0.0023 lr: 0.02
2023-03-01 15:47:52 iteration: 185400 loss: 0.0024 lr: 0.02
2023-03-01 15:48:00 iteration: 185500 loss: 0.0023 lr: 0.02
2023-03-01 15:48:08 iteration: 185600 loss: 0.0022 lr: 0.02
2023-03-01 15:48:16 iteration: 185700 loss: 0.0022 lr: 0.02
2023-03-01 15:48:26 iteration: 185800 loss: 0.0023 lr: 0.02
2023-03-01 15:48:34 iteration: 185900 loss: 0.0024 lr: 0.02
2023-03-01 15:48:43 iteration: 186000 loss: 0.0023 lr: 0.02
2023-03-01 15:48:51 iteration: 186100 loss: 0.0024 lr: 0.02
2023-03-01 15:49:00 iteration: 186200 loss: 0.0022 lr: 0.02
2023-03-01 15:49:09 iteration: 186300 loss: 0.0023 lr: 0.02
2023-03-01 15:49:18 iteration: 186400 loss: 0.0024 lr: 0.02
2023-03-01 15:49:26 iteration: 186500 loss: 0.0022 lr: 0.02
2023-03-01 15:49:36 iteration: 186600 loss: 0.0022 lr: 0.02
2023-03-01 15:49:45 iteration: 186700 loss: 0.0023 lr: 0.02
2023-03-01 15:49:53 iteration: 186800 loss: 0.0022 lr: 0.02
2023-03-01 15:50:03 iteration: 186900 loss: 0.0025 lr: 0.02
2023-03-01 15:50:12 iteration: 187000 loss: 0.0024 lr: 0.02
2023-03-01 15:50:21 iteration: 187100 loss: 0.0024 lr: 0.02
2023-03-01 15:50:30 iteration: 187200 loss: 0.0023 lr: 0.02
2023-03-01 15:50:38 iteration: 187300 loss: 0.0023 lr: 0.02
2023-03-01 15:50:46 iteration: 187400 loss: 0.0021 lr: 0.02
2023-03-01 15:50:57 iteration: 187500 loss: 0.0022 lr: 0.02
2023-03-01 15:51:06 iteration: 187600 loss: 0.0023 lr: 0.02
2023-03-01 15:51:14 iteration: 187700 loss: 0.0024 lr: 0.02
2023-03-01 15:51:23 iteration: 187800 loss: 0.0023 lr: 0.02
2023-03-01 15:51:31 iteration: 187900 loss: 0.0022 lr: 0.02
2023-03-01 15:51:39 iteration: 188000 loss: 0.0023 lr: 0.02
2023-03-01 15:51:48 iteration: 188100 loss: 0.0022 lr: 0.02
2023-03-01 15:51:56 iteration: 188200 loss: 0.0022 lr: 0.02
2023-03-01 15:52:05 iteration: 188300 loss: 0.0023 lr: 0.02
2023-03-01 15:52:13 iteration: 188400 loss: 0.0022 lr: 0.02
2023-03-01 15:52:22 iteration: 188500 loss: 0.0024 lr: 0.02
2023-03-01 15:52:30 iteration: 188600 loss: 0.0024 lr: 0.02
2023-03-01 15:52:40 iteration: 188700 loss: 0.0029 lr: 0.02
2023-03-01 15:52:49 iteration: 188800 loss: 0.0023 lr: 0.02
2023-03-01 15:52:57 iteration: 188900 loss: 0.0026 lr: 0.02
2023-03-01 15:53:05 iteration: 189000 loss: 0.0025 lr: 0.02
2023-03-01 15:53:14 iteration: 189100 loss: 0.0023 lr: 0.02
2023-03-01 15:53:23 iteration: 189200 loss: 0.0023 lr: 0.02
2023-03-01 15:53:34 iteration: 189300 loss: 0.0022 lr: 0.02
2023-03-01 15:53:42 iteration: 189400 loss: 0.0022 lr: 0.02
2023-03-01 15:53:50 iteration: 189500 loss: 0.0025 lr: 0.02
2023-03-01 15:53:59 iteration: 189600 loss: 0.0021 lr: 0.02
2023-03-01 15:54:07 iteration: 189700 loss: 0.0022 lr: 0.02
2023-03-01 15:54:16 iteration: 189800 loss: 0.0022 lr: 0.02
2023-03-01 15:54:24 iteration: 189900 loss: 0.0022 lr: 0.02
2023-03-01 15:54:32 iteration: 190000 loss: 0.0022 lr: 0.02
2023-03-01 15:54:41 iteration: 190100 loss: 0.0021 lr: 0.02
2023-03-01 15:54:49 iteration: 190200 loss: 0.0022 lr: 0.02
2023-03-01 15:54:58 iteration: 190300 loss: 0.0022 lr: 0.02
2023-03-01 15:55:06 iteration: 190400 loss: 0.0023 lr: 0.02
2023-03-01 15:55:14 iteration: 190500 loss: 0.0023 lr: 0.02
2023-03-01 15:55:23 iteration: 190600 loss: 0.0023 lr: 0.02
2023-03-01 15:55:31 iteration: 190700 loss: 0.0022 lr: 0.02
2023-03-01 15:55:39 iteration: 190800 loss: 0.0023 lr: 0.02
2023-03-01 15:55:48 iteration: 190900 loss: 0.0023 lr: 0.02
2023-03-01 15:55:56 iteration: 191000 loss: 0.0021 lr: 0.02
2023-03-01 15:56:04 iteration: 191100 loss: 0.0023 lr: 0.02
2023-03-01 15:56:13 iteration: 191200 loss: 0.0023 lr: 0.02
2023-03-01 15:56:21 iteration: 191300 loss: 0.0023 lr: 0.02
2023-03-01 15:56:29 iteration: 191400 loss: 0.0021 lr: 0.02
2023-03-01 15:56:39 iteration: 191500 loss: 0.0022 lr: 0.02
2023-03-01 15:56:47 iteration: 191600 loss: 0.0023 lr: 0.02
2023-03-01 15:56:55 iteration: 191700 loss: 0.0021 lr: 0.02
2023-03-01 15:57:03 iteration: 191800 loss: 0.0024 lr: 0.02
2023-03-01 15:57:11 iteration: 191900 loss: 0.0022 lr: 0.02
2023-03-01 15:57:19 iteration: 192000 loss: 0.0022 lr: 0.02
2023-03-01 15:57:27 iteration: 192100 loss: 0.0022 lr: 0.02
2023-03-01 15:57:35 iteration: 192200 loss: 0.0026 lr: 0.02
2023-03-01 15:57:43 iteration: 192300 loss: 0.0023 lr: 0.02
2023-03-01 15:57:53 iteration: 192400 loss: 0.0021 lr: 0.02
2023-03-01 15:58:01 iteration: 192500 loss: 0.0023 lr: 0.02
2023-03-01 15:58:10 iteration: 192600 loss: 0.0022 lr: 0.02
2023-03-01 15:58:19 iteration: 192700 loss: 0.0022 lr: 0.02
2023-03-01 15:58:28 iteration: 192800 loss: 0.0022 lr: 0.02
2023-03-01 15:58:35 iteration: 192900 loss: 0.0021 lr: 0.02
2023-03-01 15:58:44 iteration: 193000 loss: 0.0022 lr: 0.02
2023-03-01 15:58:52 iteration: 193100 loss: 0.0021 lr: 0.02
2023-03-01 15:59:00 iteration: 193200 loss: 0.0023 lr: 0.02
2023-03-01 15:59:09 iteration: 193300 loss: 0.0024 lr: 0.02
2023-03-01 15:59:18 iteration: 193400 loss: 0.0023 lr: 0.02
2023-03-01 15:59:26 iteration: 193500 loss: 0.0022 lr: 0.02
2023-03-01 15:59:34 iteration: 193600 loss: 0.0022 lr: 0.02
2023-03-01 15:59:43 iteration: 193700 loss: 0.0025 lr: 0.02
2023-03-01 15:59:51 iteration: 193800 loss: 0.0022 lr: 0.02
2023-03-01 15:59:59 iteration: 193900 loss: 0.0024 lr: 0.02
2023-03-01 16:00:08 iteration: 194000 loss: 0.0023 lr: 0.02
2023-03-01 16:00:17 iteration: 194100 loss: 0.0022 lr: 0.02
2023-03-01 16:00:25 iteration: 194200 loss: 0.0023 lr: 0.02
2023-03-01 16:00:33 iteration: 194300 loss: 0.0023 lr: 0.02
2023-03-01 16:00:42 iteration: 194400 loss: 0.0022 lr: 0.02
2023-03-01 16:00:50 iteration: 194500 loss: 0.0022 lr: 0.02
2023-03-01 16:00:58 iteration: 194600 loss: 0.0023 lr: 0.02
2023-03-01 16:01:06 iteration: 194700 loss: 0.0023 lr: 0.02
2023-03-01 16:01:15 iteration: 194800 loss: 0.0023 lr: 0.02
2023-03-01 16:01:23 iteration: 194900 loss: 0.0022 lr: 0.02
2023-03-01 16:01:32 iteration: 195000 loss: 0.0024 lr: 0.02
2023-03-01 16:01:40 iteration: 195100 loss: 0.0022 lr: 0.02
2023-03-01 16:01:48 iteration: 195200 loss: 0.0021 lr: 0.02
2023-03-01 16:01:56 iteration: 195300 loss: 0.0023 lr: 0.02
2023-03-01 16:02:05 iteration: 195400 loss: 0.0026 lr: 0.02
2023-03-01 16:02:13 iteration: 195500 loss: 0.0022 lr: 0.02
2023-03-01 16:02:22 iteration: 195600 loss: 0.0023 lr: 0.02
2023-03-01 16:02:30 iteration: 195700 loss: 0.0022 lr: 0.02
2023-03-01 16:02:38 iteration: 195800 loss: 0.0023 lr: 0.02
2023-03-01 16:02:47 iteration: 195900 loss: 0.0022 lr: 0.02
2023-03-01 16:02:56 iteration: 196000 loss: 0.0024 lr: 0.02
2023-03-01 16:03:04 iteration: 196100 loss: 0.0023 lr: 0.02
2023-03-01 16:03:13 iteration: 196200 loss: 0.0024 lr: 0.02
2023-03-01 16:03:21 iteration: 196300 loss: 0.0021 lr: 0.02
2023-03-01 16:03:31 iteration: 196400 loss: 0.0023 lr: 0.02
2023-03-01 16:03:39 iteration: 196500 loss: 0.0021 lr: 0.02
2023-03-01 16:03:47 iteration: 196600 loss: 0.0023 lr: 0.02
2023-03-01 16:03:55 iteration: 196700 loss: 0.0025 lr: 0.02
2023-03-01 16:04:05 iteration: 196800 loss: 0.0024 lr: 0.02
2023-03-01 16:04:14 iteration: 196900 loss: 0.0023 lr: 0.02
2023-03-01 16:04:22 iteration: 197000 loss: 0.0024 lr: 0.02
2023-03-01 16:04:30 iteration: 197100 loss: 0.0022 lr: 0.02
2023-03-01 16:04:38 iteration: 197200 loss: 0.0023 lr: 0.02
2023-03-01 16:04:46 iteration: 197300 loss: 0.0021 lr: 0.02
2023-03-01 16:04:54 iteration: 197400 loss: 0.0024 lr: 0.02
2023-03-01 16:05:02 iteration: 197500 loss: 0.0024 lr: 0.02
2023-03-01 16:05:11 iteration: 197600 loss: 0.0022 lr: 0.02
2023-03-01 16:05:19 iteration: 197700 loss: 0.0023 lr: 0.02
2023-03-01 16:05:27 iteration: 197800 loss: 0.0022 lr: 0.02
2023-03-01 16:05:36 iteration: 197900 loss: 0.0022 lr: 0.02
2023-03-01 16:05:43 iteration: 198000 loss: 0.0023 lr: 0.02
2023-03-01 16:05:52 iteration: 198100 loss: 0.0024 lr: 0.02
2023-03-01 16:06:01 iteration: 198200 loss: 0.0023 lr: 0.02
2023-03-01 16:06:09 iteration: 198300 loss: 0.0022 lr: 0.02
2023-03-01 16:06:17 iteration: 198400 loss: 0.0023 lr: 0.02
2023-03-01 16:06:26 iteration: 198500 loss: 0.0022 lr: 0.02
2023-03-01 16:06:35 iteration: 198600 loss: 0.0020 lr: 0.02
2023-03-01 16:06:43 iteration: 198700 loss: 0.0022 lr: 0.02
2023-03-01 16:06:51 iteration: 198800 loss: 0.0021 lr: 0.02
2023-03-01 16:06:59 iteration: 198900 loss: 0.0024 lr: 0.02
2023-03-01 16:07:08 iteration: 199000 loss: 0.0022 lr: 0.02
2023-03-01 16:07:16 iteration: 199100 loss: 0.0022 lr: 0.02
2023-03-01 16:07:24 iteration: 199200 loss: 0.0021 lr: 0.02
2023-03-01 16:07:33 iteration: 199300 loss: 0.0023 lr: 0.02
2023-03-01 16:07:42 iteration: 199400 loss: 0.0021 lr: 0.02
2023-03-01 16:07:50 iteration: 199500 loss: 0.0020 lr: 0.02
2023-03-01 16:07:58 iteration: 199600 loss: 0.0024 lr: 0.02
2023-03-01 16:08:07 iteration: 199700 loss: 0.0022 lr: 0.02
2023-03-01 16:08:15 iteration: 199800 loss: 0.0022 lr: 0.02
2023-03-01 16:08:22 iteration: 199900 loss: 0.0022 lr: 0.02
2023-03-01 16:08:31 iteration: 200000 loss: 0.0023 lr: 0.02
2023-03-01 16:08:39 iteration: 200100 loss: 0.0023 lr: 0.02
2023-03-01 16:08:48 iteration: 200200 loss: 0.0022 lr: 0.02
2023-03-01 16:08:56 iteration: 200300 loss: 0.0021 lr: 0.02
2023-03-01 16:09:05 iteration: 200400 loss: 0.0022 lr: 0.02
2023-03-01 16:09:13 iteration: 200500 loss: 0.0022 lr: 0.02
2023-03-01 16:09:21 iteration: 200600 loss: 0.0022 lr: 0.02
2023-03-01 16:09:30 iteration: 200700 loss: 0.0021 lr: 0.02
2023-03-01 16:09:38 iteration: 200800 loss: 0.0023 lr: 0.02
2023-03-01 16:09:48 iteration: 200900 loss: 0.0022 lr: 0.02
2023-03-01 16:09:56 iteration: 201000 loss: 0.0022 lr: 0.02
2023-03-01 16:10:04 iteration: 201100 loss: 0.0023 lr: 0.02
2023-03-01 16:10:14 iteration: 201200 loss: 0.0023 lr: 0.02
2023-03-01 16:10:22 iteration: 201300 loss: 0.0022 lr: 0.02
2023-03-01 16:10:30 iteration: 201400 loss: 0.0020 lr: 0.02
2023-03-01 16:10:38 iteration: 201500 loss: 0.0022 lr: 0.02
2023-03-01 16:10:46 iteration: 201600 loss: 0.0022 lr: 0.02
2023-03-01 16:10:54 iteration: 201700 loss: 0.0021 lr: 0.02
2023-03-01 16:11:03 iteration: 201800 loss: 0.0021 lr: 0.02
2023-03-01 16:11:12 iteration: 201900 loss: 0.0022 lr: 0.02
2023-03-01 16:11:21 iteration: 202000 loss: 0.0021 lr: 0.02
2023-03-01 16:11:29 iteration: 202100 loss: 0.0019 lr: 0.02
2023-03-01 16:11:37 iteration: 202200 loss: 0.0022 lr: 0.02
2023-03-01 16:11:46 iteration: 202300 loss: 0.0023 lr: 0.02
2023-03-01 16:11:53 iteration: 202400 loss: 0.0023 lr: 0.02
2023-03-01 16:12:02 iteration: 202500 loss: 0.0023 lr: 0.02
2023-03-01 16:12:10 iteration: 202600 loss: 0.0022 lr: 0.02
2023-03-01 16:12:18 iteration: 202700 loss: 0.0020 lr: 0.02
2023-03-01 16:12:26 iteration: 202800 loss: 0.0021 lr: 0.02
2023-03-01 16:12:35 iteration: 202900 loss: 0.0022 lr: 0.02
2023-03-01 16:12:43 iteration: 203000 loss: 0.0022 lr: 0.02
2023-03-01 16:12:51 iteration: 203100 loss: 0.0023 lr: 0.02
2023-03-01 16:13:00 iteration: 203200 loss: 0.0024 lr: 0.02
2023-03-01 16:13:08 iteration: 203300 loss: 0.0022 lr: 0.02
2023-03-01 16:13:16 iteration: 203400 loss: 0.0021 lr: 0.02
2023-03-01 16:13:24 iteration: 203500 loss: 0.0022 lr: 0.02
2023-03-01 16:13:32 iteration: 203600 loss: 0.0021 lr: 0.02
2023-03-01 16:13:41 iteration: 203700 loss: 0.0021 lr: 0.02
2023-03-01 16:13:49 iteration: 203800 loss: 0.0023 lr: 0.02
2023-03-01 16:13:59 iteration: 203900 loss: 0.0023 lr: 0.02
2023-03-01 16:14:07 iteration: 204000 loss: 0.0020 lr: 0.02
2023-03-01 16:14:16 iteration: 204100 loss: 0.0023 lr: 0.02
2023-03-01 16:14:24 iteration: 204200 loss: 0.0021 lr: 0.02
2023-03-01 16:14:33 iteration: 204300 loss: 0.0022 lr: 0.02
2023-03-01 16:14:42 iteration: 204400 loss: 0.0022 lr: 0.02
2023-03-01 16:14:50 iteration: 204500 loss: 0.0020 lr: 0.02
2023-03-01 16:14:59 iteration: 204600 loss: 0.0021 lr: 0.02
2023-03-01 16:15:07 iteration: 204700 loss: 0.0021 lr: 0.02
2023-03-01 16:15:15 iteration: 204800 loss: 0.0020 lr: 0.02
2023-03-01 16:15:23 iteration: 204900 loss: 0.0021 lr: 0.02
2023-03-01 16:15:33 iteration: 205000 loss: 0.0022 lr: 0.02
2023-03-01 16:15:42 iteration: 205100 loss: 0.0022 lr: 0.02
2023-03-01 16:15:50 iteration: 205200 loss: 0.0020 lr: 0.02
2023-03-01 16:15:58 iteration: 205300 loss: 0.0022 lr: 0.02
2023-03-01 16:16:08 iteration: 205400 loss: 0.0022 lr: 0.02
2023-03-01 16:16:16 iteration: 205500 loss: 0.0020 lr: 0.02
2023-03-01 16:16:24 iteration: 205600 loss: 0.0022 lr: 0.02
2023-03-01 16:16:32 iteration: 205700 loss: 0.0023 lr: 0.02
2023-03-01 16:16:40 iteration: 205800 loss: 0.0021 lr: 0.02
2023-03-01 16:16:50 iteration: 205900 loss: 0.0023 lr: 0.02
2023-03-01 16:16:58 iteration: 206000 loss: 0.0021 lr: 0.02
2023-03-01 16:17:07 iteration: 206100 loss: 0.0022 lr: 0.02
2023-03-01 16:17:15 iteration: 206200 loss: 0.0020 lr: 0.02
2023-03-01 16:17:24 iteration: 206300 loss: 0.0023 lr: 0.02
2023-03-01 16:17:32 iteration: 206400 loss: 0.0021 lr: 0.02
2023-03-01 16:17:40 iteration: 206500 loss: 0.0023 lr: 0.02
2023-03-01 16:17:48 iteration: 206600 loss: 0.0026 lr: 0.02
2023-03-01 16:17:57 iteration: 206700 loss: 0.0020 lr: 0.02
2023-03-01 16:18:05 iteration: 206800 loss: 0.0021 lr: 0.02
2023-03-01 16:18:14 iteration: 206900 loss: 0.0023 lr: 0.02
2023-03-01 16:18:22 iteration: 207000 loss: 0.0023 lr: 0.02
2023-03-01 16:18:31 iteration: 207100 loss: 0.0023 lr: 0.02
2023-03-01 16:18:39 iteration: 207200 loss: 0.0023 lr: 0.02
2023-03-01 16:18:48 iteration: 207300 loss: 0.0022 lr: 0.02
2023-03-01 16:18:57 iteration: 207400 loss: 0.0023 lr: 0.02
2023-03-01 16:19:05 iteration: 207500 loss: 0.0023 lr: 0.02
2023-03-01 16:19:14 iteration: 207600 loss: 0.0023 lr: 0.02
2023-03-01 16:19:23 iteration: 207700 loss: 0.0023 lr: 0.02
2023-03-01 16:19:32 iteration: 207800 loss: 0.0023 lr: 0.02
2023-03-01 16:19:40 iteration: 207900 loss: 0.0020 lr: 0.02
2023-03-01 16:19:49 iteration: 208000 loss: 0.0020 lr: 0.02
2023-03-01 16:19:57 iteration: 208100 loss: 0.0022 lr: 0.02
2023-03-01 16:20:05 iteration: 208200 loss: 0.0021 lr: 0.02
2023-03-01 16:20:13 iteration: 208300 loss: 0.0021 lr: 0.02
2023-03-01 16:20:22 iteration: 208400 loss: 0.0024 lr: 0.02
2023-03-01 16:20:30 iteration: 208500 loss: 0.0021 lr: 0.02
2023-03-01 16:20:39 iteration: 208600 loss: 0.0023 lr: 0.02
2023-03-01 16:20:47 iteration: 208700 loss: 0.0024 lr: 0.02
2023-03-01 16:20:55 iteration: 208800 loss: 0.0021 lr: 0.02
2023-03-01 16:21:03 iteration: 208900 loss: 0.0021 lr: 0.02
2023-03-01 16:21:11 iteration: 209000 loss: 0.0020 lr: 0.02
2023-03-01 16:21:20 iteration: 209100 loss: 0.0021 lr: 0.02
2023-03-01 16:21:28 iteration: 209200 loss: 0.0021 lr: 0.02
2023-03-01 16:21:37 iteration: 209300 loss: 0.0021 lr: 0.02
2023-03-01 16:21:45 iteration: 209400 loss: 0.0021 lr: 0.02
2023-03-01 16:21:53 iteration: 209500 loss: 0.0021 lr: 0.02
2023-03-01 16:22:02 iteration: 209600 loss: 0.0022 lr: 0.02
2023-03-01 16:22:10 iteration: 209700 loss: 0.0023 lr: 0.02
2023-03-01 16:22:19 iteration: 209800 loss: 0.0024 lr: 0.02
2023-03-01 16:22:27 iteration: 209900 loss: 0.0021 lr: 0.02
2023-03-01 16:22:36 iteration: 210000 loss: 0.0022 lr: 0.02
2023-03-01 16:22:45 iteration: 210100 loss: 0.0022 lr: 0.02
2023-03-01 16:22:54 iteration: 210200 loss: 0.0021 lr: 0.02
2023-03-01 16:23:03 iteration: 210300 loss: 0.0022 lr: 0.02
2023-03-01 16:23:11 iteration: 210400 loss: 0.0021 lr: 0.02
2023-03-01 16:23:21 iteration: 210500 loss: 0.0022 lr: 0.02
2023-03-01 16:23:29 iteration: 210600 loss: 0.0023 lr: 0.02
2023-03-01 16:23:38 iteration: 210700 loss: 0.0022 lr: 0.02
2023-03-01 16:23:46 iteration: 210800 loss: 0.0022 lr: 0.02
2023-03-01 16:23:54 iteration: 210900 loss: 0.0021 lr: 0.02
2023-03-01 16:24:03 iteration: 211000 loss: 0.0021 lr: 0.02
2023-03-01 16:24:12 iteration: 211100 loss: 0.0022 lr: 0.02
2023-03-01 16:24:20 iteration: 211200 loss: 0.0022 lr: 0.02
2023-03-01 16:24:29 iteration: 211300 loss: 0.0021 lr: 0.02
2023-03-01 16:24:37 iteration: 211400 loss: 0.0022 lr: 0.02
2023-03-01 16:24:45 iteration: 211500 loss: 0.0021 lr: 0.02
2023-03-01 16:24:53 iteration: 211600 loss: 0.0024 lr: 0.02
2023-03-01 16:25:01 iteration: 211700 loss: 0.0021 lr: 0.02
2023-03-01 16:25:10 iteration: 211800 loss: 0.0021 lr: 0.02
2023-03-01 16:25:19 iteration: 211900 loss: 0.0022 lr: 0.02
2023-03-01 16:25:27 iteration: 212000 loss: 0.0023 lr: 0.02
2023-03-01 16:25:36 iteration: 212100 loss: 0.0022 lr: 0.02
2023-03-01 16:25:45 iteration: 212200 loss: 0.0020 lr: 0.02
2023-03-01 16:25:53 iteration: 212300 loss: 0.0022 lr: 0.02
2023-03-01 16:26:02 iteration: 212400 loss: 0.0024 lr: 0.02
2023-03-01 16:26:10 iteration: 212500 loss: 0.0022 lr: 0.02
2023-03-01 16:26:18 iteration: 212600 loss: 0.0023 lr: 0.02
2023-03-01 16:26:26 iteration: 212700 loss: 0.0022 lr: 0.02
2023-03-01 16:26:34 iteration: 212800 loss: 0.0021 lr: 0.02
2023-03-01 16:26:43 iteration: 212900 loss: 0.0023 lr: 0.02
2023-03-01 16:26:51 iteration: 213000 loss: 0.0023 lr: 0.02
2023-03-01 16:26:59 iteration: 213100 loss: 0.0022 lr: 0.02
2023-03-01 16:27:07 iteration: 213200 loss: 0.0021 lr: 0.02
2023-03-01 16:27:16 iteration: 213300 loss: 0.0021 lr: 0.02
2023-03-01 16:27:24 iteration: 213400 loss: 0.0023 lr: 0.02
2023-03-01 16:27:33 iteration: 213500 loss: 0.0019 lr: 0.02
2023-03-01 16:27:43 iteration: 213600 loss: 0.0022 lr: 0.02
2023-03-01 16:27:52 iteration: 213700 loss: 0.0021 lr: 0.02
2023-03-01 16:28:00 iteration: 213800 loss: 0.0023 lr: 0.02
2023-03-01 16:28:09 iteration: 213900 loss: 0.0021 lr: 0.02
2023-03-01 16:28:17 iteration: 214000 loss: 0.0021 lr: 0.02
2023-03-01 16:28:26 iteration: 214100 loss: 0.0021 lr: 0.02
2023-03-01 16:28:35 iteration: 214200 loss: 0.0022 lr: 0.02
2023-03-01 16:28:44 iteration: 214300 loss: 0.0025 lr: 0.02
2023-03-01 16:28:53 iteration: 214400 loss: 0.0026 lr: 0.02
2023-03-01 16:29:01 iteration: 214500 loss: 0.0021 lr: 0.02
2023-03-01 16:29:09 iteration: 214600 loss: 0.0021 lr: 0.02
2023-03-01 16:29:18 iteration: 214700 loss: 0.0023 lr: 0.02
2023-03-01 16:29:26 iteration: 214800 loss: 0.0023 lr: 0.02
2023-03-01 16:29:34 iteration: 214900 loss: 0.0021 lr: 0.02
2023-03-01 16:29:42 iteration: 215000 loss: 0.0022 lr: 0.02
2023-03-01 16:29:50 iteration: 215100 loss: 0.0022 lr: 0.02
2023-03-01 16:29:58 iteration: 215200 loss: 0.0021 lr: 0.02
2023-03-01 16:30:08 iteration: 215300 loss: 0.0021 lr: 0.02
2023-03-01 16:30:18 iteration: 215400 loss: 0.0021 lr: 0.02
2023-03-01 16:30:26 iteration: 215500 loss: 0.0024 lr: 0.02
2023-03-01 16:30:35 iteration: 215600 loss: 0.0022 lr: 0.02
2023-03-01 16:30:43 iteration: 215700 loss: 0.0023 lr: 0.02
2023-03-01 16:30:52 iteration: 215800 loss: 0.0023 lr: 0.02
2023-03-01 16:31:01 iteration: 215900 loss: 0.0021 lr: 0.02
2023-03-01 16:31:10 iteration: 216000 loss: 0.0022 lr: 0.02
2023-03-01 16:31:19 iteration: 216100 loss: 0.0021 lr: 0.02
2023-03-01 16:31:27 iteration: 216200 loss: 0.0023 lr: 0.02
2023-03-01 16:31:35 iteration: 216300 loss: 0.0023 lr: 0.02
2023-03-01 16:31:44 iteration: 216400 loss: 0.0022 lr: 0.02
2023-03-01 16:31:52 iteration: 216500 loss: 0.0021 lr: 0.02
2023-03-01 16:32:01 iteration: 216600 loss: 0.0021 lr: 0.02
2023-03-01 16:32:09 iteration: 216700 loss: 0.0022 lr: 0.02
2023-03-01 16:32:17 iteration: 216800 loss: 0.0021 lr: 0.02
2023-03-01 16:32:25 iteration: 216900 loss: 0.0021 lr: 0.02
2023-03-01 16:32:34 iteration: 217000 loss: 0.0019 lr: 0.02
2023-03-01 16:32:42 iteration: 217100 loss: 0.0020 lr: 0.02
2023-03-01 16:32:50 iteration: 217200 loss: 0.0024 lr: 0.02
2023-03-01 16:32:59 iteration: 217300 loss: 0.0022 lr: 0.02
2023-03-01 16:33:09 iteration: 217400 loss: 0.0023 lr: 0.02
2023-03-01 16:33:18 iteration: 217500 loss: 0.0021 lr: 0.02
2023-03-01 16:33:27 iteration: 217600 loss: 0.0020 lr: 0.02
2023-03-01 16:33:36 iteration: 217700 loss: 0.0020 lr: 0.02
2023-03-01 16:33:44 iteration: 217800 loss: 0.0020 lr: 0.02
2023-03-01 16:33:52 iteration: 217900 loss: 0.0022 lr: 0.02
2023-03-01 16:34:01 iteration: 218000 loss: 0.0023 lr: 0.02
2023-03-01 16:34:09 iteration: 218100 loss: 0.0020 lr: 0.02
2023-03-01 16:34:18 iteration: 218200 loss: 0.0021 lr: 0.02
2023-03-01 16:34:26 iteration: 218300 loss: 0.0022 lr: 0.02
2023-03-01 16:34:35 iteration: 218400 loss: 0.0022 lr: 0.02
2023-03-01 16:34:42 iteration: 218500 loss: 0.0022 lr: 0.02
2023-03-01 16:34:51 iteration: 218600 loss: 0.0022 lr: 0.02
2023-03-01 16:34:59 iteration: 218700 loss: 0.0023 lr: 0.02
2023-03-01 16:35:08 iteration: 218800 loss: 0.0023 lr: 0.02
2023-03-01 16:35:17 iteration: 218900 loss: 0.0023 lr: 0.02
2023-03-01 16:35:26 iteration: 219000 loss: 0.0020 lr: 0.02
2023-03-01 16:35:35 iteration: 219100 loss: 0.0023 lr: 0.02
2023-03-01 16:35:43 iteration: 219200 loss: 0.0023 lr: 0.02
2023-03-01 16:35:51 iteration: 219300 loss: 0.0021 lr: 0.02
2023-03-01 16:35:59 iteration: 219400 loss: 0.0024 lr: 0.02
2023-03-01 16:36:08 iteration: 219500 loss: 0.0022 lr: 0.02
2023-03-01 16:36:17 iteration: 219600 loss: 0.0021 lr: 0.02
2023-03-01 16:36:25 iteration: 219700 loss: 0.0023 lr: 0.02
2023-03-01 16:36:33 iteration: 219800 loss: 0.0022 lr: 0.02
2023-03-01 16:36:41 iteration: 219900 loss: 0.0020 lr: 0.02
2023-03-01 16:36:49 iteration: 220000 loss: 0.0020 lr: 0.02
2023-03-01 16:36:58 iteration: 220100 loss: 0.0022 lr: 0.02
2023-03-01 16:37:07 iteration: 220200 loss: 0.0021 lr: 0.02
2023-03-01 16:37:16 iteration: 220300 loss: 0.0021 lr: 0.02
2023-03-01 16:37:25 iteration: 220400 loss: 0.0021 lr: 0.02
2023-03-01 16:37:33 iteration: 220500 loss: 0.0021 lr: 0.02
2023-03-01 16:37:41 iteration: 220600 loss: 0.0023 lr: 0.02
2023-03-01 16:37:49 iteration: 220700 loss: 0.0019 lr: 0.02
2023-03-01 16:37:57 iteration: 220800 loss: 0.0020 lr: 0.02
2023-03-01 16:38:05 iteration: 220900 loss: 0.0020 lr: 0.02
2023-03-01 16:38:13 iteration: 221000 loss: 0.0021 lr: 0.02
2023-03-01 16:38:23 iteration: 221100 loss: 0.0022 lr: 0.02
2023-03-01 16:38:32 iteration: 221200 loss: 0.0026 lr: 0.02
2023-03-01 16:38:40 iteration: 221300 loss: 0.0020 lr: 0.02
2023-03-01 16:38:48 iteration: 221400 loss: 0.0021 lr: 0.02
2023-03-01 16:38:57 iteration: 221500 loss: 0.0023 lr: 0.02
2023-03-01 16:39:06 iteration: 221600 loss: 0.0024 lr: 0.02
2023-03-01 16:39:16 iteration: 221700 loss: 0.0020 lr: 0.02
2023-03-01 16:39:24 iteration: 221800 loss: 0.0022 lr: 0.02
2023-03-01 16:39:33 iteration: 221900 loss: 0.0021 lr: 0.02
2023-03-01 16:39:42 iteration: 222000 loss: 0.0020 lr: 0.02
2023-03-01 16:39:50 iteration: 222100 loss: 0.0022 lr: 0.02
2023-03-01 16:39:58 iteration: 222200 loss: 0.0023 lr: 0.02
2023-03-01 16:40:07 iteration: 222300 loss: 0.0022 lr: 0.02
2023-03-01 16:40:15 iteration: 222400 loss: 0.0021 lr: 0.02
2023-03-01 16:40:23 iteration: 222500 loss: 0.0020 lr: 0.02
2023-03-01 16:40:31 iteration: 222600 loss: 0.0022 lr: 0.02
2023-03-01 16:40:40 iteration: 222700 loss: 0.0020 lr: 0.02
2023-03-01 16:40:49 iteration: 222800 loss: 0.0020 lr: 0.02
2023-03-01 16:40:57 iteration: 222900 loss: 0.0021 lr: 0.02
2023-03-01 16:41:04 iteration: 223000 loss: 0.0021 lr: 0.02
2023-03-01 16:41:14 iteration: 223100 loss: 0.0021 lr: 0.02
2023-03-01 16:41:22 iteration: 223200 loss: 0.0020 lr: 0.02
2023-03-01 16:41:31 iteration: 223300 loss: 0.0022 lr: 0.02
2023-03-01 16:41:40 iteration: 223400 loss: 0.0021 lr: 0.02
2023-03-01 16:41:50 iteration: 223500 loss: 0.0023 lr: 0.02
2023-03-01 16:41:58 iteration: 223600 loss: 0.0021 lr: 0.02
2023-03-01 16:42:06 iteration: 223700 loss: 0.0022 lr: 0.02
2023-03-01 16:42:16 iteration: 223800 loss: 0.0021 lr: 0.02
2023-03-01 16:42:24 iteration: 223900 loss: 0.0020 lr: 0.02
2023-03-01 16:42:32 iteration: 224000 loss: 0.0021 lr: 0.02
2023-03-01 16:42:40 iteration: 224100 loss: 0.0021 lr: 0.02
2023-03-01 16:42:50 iteration: 224200 loss: 0.0021 lr: 0.02
2023-03-01 16:42:58 iteration: 224300 loss: 0.0022 lr: 0.02
2023-03-01 16:43:06 iteration: 224400 loss: 0.0020 lr: 0.02
2023-03-01 16:43:15 iteration: 224500 loss: 0.0021 lr: 0.02
2023-03-01 16:43:23 iteration: 224600 loss: 0.0022 lr: 0.02
2023-03-01 16:43:31 iteration: 224700 loss: 0.0022 lr: 0.02
2023-03-01 16:43:39 iteration: 224800 loss: 0.0021 lr: 0.02
2023-03-01 16:43:47 iteration: 224900 loss: 0.0020 lr: 0.02
2023-03-01 16:43:55 iteration: 225000 loss: 0.0021 lr: 0.02
2023-03-01 16:44:03 iteration: 225100 loss: 0.0022 lr: 0.02
2023-03-01 16:44:12 iteration: 225200 loss: 0.0019 lr: 0.02
2023-03-01 16:44:21 iteration: 225300 loss: 0.0022 lr: 0.02
2023-03-01 16:44:29 iteration: 225400 loss: 0.0019 lr: 0.02
2023-03-01 16:44:37 iteration: 225500 loss: 0.0022 lr: 0.02
2023-03-01 16:44:46 iteration: 225600 loss: 0.0024 lr: 0.02
2023-03-01 16:44:55 iteration: 225700 loss: 0.0021 lr: 0.02
2023-03-01 16:45:03 iteration: 225800 loss: 0.0023 lr: 0.02
2023-03-01 16:45:12 iteration: 225900 loss: 0.0021 lr: 0.02
2023-03-01 16:45:20 iteration: 226000 loss: 0.0020 lr: 0.02
2023-03-01 16:45:28 iteration: 226100 loss: 0.0020 lr: 0.02
2023-03-01 16:45:37 iteration: 226200 loss: 0.0022 lr: 0.02
2023-03-01 16:45:45 iteration: 226300 loss: 0.0021 lr: 0.02
2023-03-01 16:45:55 iteration: 226400 loss: 0.0021 lr: 0.02
2023-03-01 16:46:03 iteration: 226500 loss: 0.0022 lr: 0.02
2023-03-01 16:46:12 iteration: 226600 loss: 0.0020 lr: 0.02
2023-03-01 16:46:20 iteration: 226700 loss: 0.0019 lr: 0.02
2023-03-01 16:46:29 iteration: 226800 loss: 0.0023 lr: 0.02
2023-03-01 16:46:38 iteration: 226900 loss: 0.0019 lr: 0.02
2023-03-01 16:46:46 iteration: 227000 loss: 0.0020 lr: 0.02
2023-03-01 16:46:55 iteration: 227100 loss: 0.0022 lr: 0.02
2023-03-01 16:47:03 iteration: 227200 loss: 0.0019 lr: 0.02
2023-03-01 16:47:12 iteration: 227300 loss: 0.0023 lr: 0.02
2023-03-01 16:47:22 iteration: 227400 loss: 0.0021 lr: 0.02
2023-03-01 16:47:30 iteration: 227500 loss: 0.0023 lr: 0.02
2023-03-01 16:47:38 iteration: 227600 loss: 0.0021 lr: 0.02
2023-03-01 16:47:47 iteration: 227700 loss: 0.0021 lr: 0.02
2023-03-01 16:47:55 iteration: 227800 loss: 0.0021 lr: 0.02
2023-03-01 16:48:03 iteration: 227900 loss: 0.0020 lr: 0.02
2023-03-01 16:48:11 iteration: 228000 loss: 0.0020 lr: 0.02
2023-03-01 16:48:19 iteration: 228100 loss: 0.0023 lr: 0.02
2023-03-01 16:48:28 iteration: 228200 loss: 0.0020 lr: 0.02
2023-03-01 16:48:36 iteration: 228300 loss: 0.0021 lr: 0.02
2023-03-01 16:48:46 iteration: 228400 loss: 0.0021 lr: 0.02
2023-03-01 16:48:54 iteration: 228500 loss: 0.0020 lr: 0.02
2023-03-01 16:49:02 iteration: 228600 loss: 0.0018 lr: 0.02
2023-03-01 16:49:10 iteration: 228700 loss: 0.0019 lr: 0.02
2023-03-01 16:49:19 iteration: 228800 loss: 0.0021 lr: 0.02
2023-03-01 16:49:27 iteration: 228900 loss: 0.0021 lr: 0.02
2023-03-01 16:49:35 iteration: 229000 loss: 0.0021 lr: 0.02
2023-03-01 16:49:43 iteration: 229100 loss: 0.0021 lr: 0.02
2023-03-01 16:49:51 iteration: 229200 loss: 0.0020 lr: 0.02
2023-03-01 16:50:00 iteration: 229300 loss: 0.0020 lr: 0.02
2023-03-01 16:50:08 iteration: 229400 loss: 0.0020 lr: 0.02
2023-03-01 16:50:17 iteration: 229500 loss: 0.0020 lr: 0.02
2023-03-01 16:50:24 iteration: 229600 loss: 0.0020 lr: 0.02
2023-03-01 16:50:33 iteration: 229700 loss: 0.0022 lr: 0.02
2023-03-01 16:50:41 iteration: 229800 loss: 0.0023 lr: 0.02
2023-03-01 16:50:49 iteration: 229900 loss: 0.0021 lr: 0.02
2023-03-01 16:50:57 iteration: 230000 loss: 0.0023 lr: 0.02
2023-03-01 16:51:07 iteration: 230100 loss: 0.0023 lr: 0.02
2023-03-01 16:51:15 iteration: 230200 loss: 0.0022 lr: 0.02
2023-03-01 16:51:23 iteration: 230300 loss: 0.0023 lr: 0.02
2023-03-01 16:51:32 iteration: 230400 loss: 0.0022 lr: 0.02
2023-03-01 16:51:40 iteration: 230500 loss: 0.0019 lr: 0.02
2023-03-01 16:51:48 iteration: 230600 loss: 0.0021 lr: 0.02
2023-03-01 16:51:57 iteration: 230700 loss: 0.0020 lr: 0.02
2023-03-01 16:52:05 iteration: 230800 loss: 0.0022 lr: 0.02
2023-03-01 16:52:14 iteration: 230900 loss: 0.0020 lr: 0.02
2023-03-01 16:52:23 iteration: 231000 loss: 0.0024 lr: 0.02
2023-03-01 16:52:31 iteration: 231100 loss: 0.0021 lr: 0.02
2023-03-01 16:52:40 iteration: 231200 loss: 0.0021 lr: 0.02
2023-03-01 16:52:48 iteration: 231300 loss: 0.0022 lr: 0.02
2023-03-01 16:52:56 iteration: 231400 loss: 0.0021 lr: 0.02
2023-03-01 16:53:06 iteration: 231500 loss: 0.0020 lr: 0.02
2023-03-01 16:53:14 iteration: 231600 loss: 0.0022 lr: 0.02
2023-03-01 16:53:23 iteration: 231700 loss: 0.0021 lr: 0.02
2023-03-01 16:53:31 iteration: 231800 loss: 0.0021 lr: 0.02
2023-03-01 16:53:40 iteration: 231900 loss: 0.0020 lr: 0.02
2023-03-01 16:53:48 iteration: 232000 loss: 0.0021 lr: 0.02
2023-03-01 16:53:56 iteration: 232100 loss: 0.0019 lr: 0.02
2023-03-01 16:54:05 iteration: 232200 loss: 0.0022 lr: 0.02
2023-03-01 16:54:14 iteration: 232300 loss: 0.0023 lr: 0.02
2023-03-01 16:54:23 iteration: 232400 loss: 0.0020 lr: 0.02
2023-03-01 16:54:31 iteration: 232500 loss: 0.0020 lr: 0.02
2023-03-01 16:54:39 iteration: 232600 loss: 0.0020 lr: 0.02
